{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28abc6f1",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733eb70",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d93dae",
   "metadata": {},
   "source": [
    "**Definition:**  \n",
    "Ensemble learning combines multiple hypotheses (models) to form a (hopefully) better-performing hypothesis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dc018",
   "metadata": {},
   "source": [
    "**Ensemble vs. Multiple Classifier**\n",
    "- **Ensemble:**  \n",
    "    Uses the **same base learner** to generate hypotheses.\n",
    "\n",
    "- **Multiple Classifier (Broader term):**  \n",
    "    Combines hypotheses from **different base learners**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b24d56",
   "metadata": {},
   "source": [
    "**Strong Learner vs. Weak (Simple) Learner**\n",
    "- **Strong Learner:**\n",
    "    - A classifier that achieves **low classification error** (high accuracy).\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Weak Learner:**  \n",
    "    - A classifier that performs slightly better than random guessing.\n",
    "    - Two common Types:\n",
    "        - **Low Variance, High Bias**\n",
    "        - **High Variance, Low Bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28089f1",
   "metadata": {},
   "source": [
    "**Ensemble Methods**\n",
    "\n",
    "- **Parallel Methods**  \n",
    "    Weak learners are trained **independently** and in **parallel**.\n",
    "    - Requires **low-bias** weak learners\n",
    "    - **Reduces variance** in predictions\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Sequential Methods**  \n",
    "    Weak learners are trained **consecutively**, with each new learner correcting the errors of the previous ones.\n",
    "    - Requires **low-variance** weak learners\n",
    "    - **Reduces bias** and improves model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4f8ca",
   "metadata": {},
   "source": [
    "**Ensemble Learning Methods**\n",
    "\n",
    "- **Bagging**\n",
    "    - Parallel\n",
    "    - To decrease variance\n",
    "    - Random Forest\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Boosting**\n",
    "    - Sequential\n",
    "    - To decrease the bias (enhance capabilities)\n",
    "    - AdaBoost, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9bb26",
   "metadata": {},
   "source": [
    "### **Bagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d400e",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58982f7",
   "metadata": {},
   "source": [
    "Bagging: **B**ootstrap **agg**regat**ing**\n",
    "\n",
    "**Definition:**  \n",
    "A parallel ensemble method that reduces variance by combining multiple weak learners trained on bootstrapped datasets.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Bootstrap Resampling:**\n",
    "    - Creates multiple datasets by sampling **with replacement** from the original training data.\n",
    "    - Each dataset has the same size as the original but may contain duplicates.\n",
    "\n",
    "- **Aggregation:**\n",
    "    - **Classification:** Majority voting.\n",
    "    - **Regression:** Averaging predictions.\n",
    "\n",
    "- **Works Best When:**\n",
    "    - Base learners are **unstable** (high variance, low bias).\n",
    "    - **Low error correlation** between models (diverse predictions).\n",
    "    - Base learners are **nearly unbiased**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b161cb9",
   "metadata": {},
   "source": [
    "**Bagging Algorithm**\n",
    "\n",
    "**Input:**\n",
    "- $M$: Required ensemble size\n",
    "- $D = \\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(N)}, y^{(N)}) \\}$\n",
    "\n",
    "**Steps:**\n",
    "> for $t = 1$ to $M$ do:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Build a dataset $D_t$ by sampling $N$ items, randomly with replacement from $D$ (Bootstrap Resampling)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Train model $h_t$ using $D_t$ and add it to ensemble  \n",
    "> end for  \n",
    "> $H(x) = \\text{sign}(\\sum_{t=1}^M h_t(x))$  \n",
    "\n",
    "*Aggregate models by voting for classification or by averaging for regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12a2ad",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6873238",
   "metadata": {},
   "source": [
    "**Bagging on Decision Trees**\n",
    "\n",
    "Why Decision Trees for Bagging?\n",
    "- **Simple** and **Interpretable**\n",
    "- Handles both **numerical & categorical** data\n",
    "- **Robust** to outliers\n",
    "- **Low Bias**\n",
    "- Requires **little data preparation**\n",
    "\n",
    "How ever they are **high variance**\n",
    "\n",
    "Why are DTs good candidates for ensembles?\n",
    "- Averaging many **unbiased but high-variance** trees reduces overall variance.\n",
    "- Bias remains low; ensemble accuracy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b382a1",
   "metadata": {},
   "source": [
    "**Random Forest**\n",
    "- **Random Feature Subsets:**  \n",
    "    - At each split, only $m$ out of $d$ features are considered ($m \\leq \\sqrt{d}$).\n",
    "    - **Goal**: Further decorrelate trees to reduce error correlation.\n",
    "\n",
    "- **Forest**\n",
    "    - Use many decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136543c1",
   "metadata": {},
   "source": [
    "**Random Forest Algorithm**\n",
    "\n",
    "**Input:**\n",
    "- $T$ number of trees\n",
    "- $m$ number of variables used to split each node\n",
    "\n",
    "> for $t = 1$ to $T$ do  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Draw a bootstrap dataset  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Select $m$ features randomly out of $d$ features as candidates for splitting  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Learn a tree on this dataset  \n",
    "> end for  \n",
    "> Output:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Regression: average of the outputs  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Classification: majority voting  \n",
    "\n",
    "Where usually $m \\le \\sqrt{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73ecbf",
   "metadata": {},
   "source": [
    "### **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de6d16",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b82ba",
   "metadata": {},
   "source": [
    "Sequentially combine **multiple weak learners** (each slightly better than random guessing) to form a **strong learner**.\n",
    "\n",
    "**Key Principles:**\n",
    "- **Complementary Weak Learners:**\n",
    "    - Each learner specializes in different subsets of the data.\n",
    "    - Later learners focus on **correcting errors of previous** ones.\n",
    "\n",
    "- **Weighted Voting:**\n",
    "    - Strong learner = weighted sum of weak learners.\n",
    "    - More reliable learners get higher weights ($\\alpha_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31e127",
   "metadata": {},
   "source": [
    "**Ensemble Model:**\n",
    "$$H_m(x) = \\alpha_1 h(x; \\theta_1) + \\ldots + \\alpha_m h(x; \\theta_m), \\quad \\alpha_t \\ge 0$$\n",
    "$$H_m(x) = \\alpha_1 h_1(x) + \\ldots + \\alpha_m h_m(x)$$\n",
    "Where:\n",
    "- $h_t(x)$: Weak learner at step $t$.\n",
    "- $\\alpha_t$: Weight (higher for more accurate learners).\n",
    "\n",
    "**Prediction Rule:**\n",
    "$$\\hat{y} = \\text{sign}(H_m(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe463",
   "metadata": {},
   "source": [
    "**Decision stumps**\n",
    "\n",
    "**Definition:**  \n",
    "A **decision stump** is a single-level decision tree (one feature + threshold).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Model Form:**\n",
    "$$h(x;\\theta) = \\text{sign}(w_1 x_k - w_0), \\quad \\theta = \\{k, w_1, w_0\\}$$\n",
    "Where\n",
    "- $k$: Feature index.\n",
    "- $w_1, w_0$: Weight and threshold parameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why Use Stumps?**\n",
    "- Simple (low computational cost).\n",
    "- High bias, low variance (ideal for boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0dcac",
   "metadata": {},
   "source": [
    "#### **AdaBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797bff4",
   "metadata": {},
   "source": [
    "##### **Core Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736b6ae",
   "metadata": {},
   "source": [
    "**AdaBoost = Adaptive Boosting**\n",
    "\n",
    "- Sequential production of classifiers\n",
    "    - Iteratively add the classifier whose addition will be most helpful.\n",
    "\n",
    "- Each classifier is **dependent** on the previous ones.\n",
    "    - Focuses on the previous onesâ€™ error.\n",
    "\n",
    "- Represent the important of each sample by assigning **weights** to them.\n",
    "    - **Correct** classification $\\rightarrow$ **smaller** weights\n",
    "    - **Misclassified** samples $\\rightarrow$ **larger** weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2baede5",
   "metadata": {},
   "source": [
    "##### **AdaBoost Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f064f",
   "metadata": {},
   "source": [
    "**Definitions**:\n",
    "- $w_m^{(i)}$: Weighting coefficient of data point $i$ in iteration $m$\n",
    "- $\\alpha_m$: Weighting coefficient of $m$-th in the final ensemble\n",
    "- $\\epsilon_m$: Weighted error rate of $m$-th base classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b62b14",
   "metadata": {},
   "source": [
    "**Steps**:\n",
    "> Initialize data weight $w_1(i) = \\frac{1}{N}$ for all N samples  \n",
    "> for $m=1$ to M do:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Find a classifier $h_m(x)$ by minimizing th weighted error function:  \n",
    "> $$J_m = \\sum_{i = 1}^N w_m^{(i)} I \\left( y^{(i)} \\neq h_m( x^{(i)} ) \\right)$$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Find the weighted error of $h_m(x)$:\n",
    "> $$\\epsilon_m = \\frac{\\sum_{i = 1}^N w_m^{(i)} I \\left( y^{(i)} \\neq h_m( x^{(i)} ) \\right)}{\\sum_{i = 1}^N w_m^{(i)}}$$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;New component is assigned votes based on its error:\n",
    "> $$\\alpha_m = \\ln(\\frac{1 - \\epsilon_m}{\\epsilon_m})$$\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Update the normalized weights:\n",
    "> $$w_{m+1}^{(i)} = w_m^{(i)} e^{\\alpha_m I(y^{(i)} \\neq h_m( x^{(i)}))}$$\n",
    "> end for  \n",
    "> Combined classifier:  \n",
    "> $$\\hat{y} = sign(H_M(x)), \\quad$$  \n",
    "> Where:  \n",
    "> $$H_M(x) = \\frac{1}{2} \\sum_{m=1}^M \\alpha_m h_m(x)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c8ce1",
   "metadata": {},
   "source": [
    "**Ensemble Model:**\n",
    "$$H_M(x) = \\frac{1}{2} [\\alpha_1 h_1(x) + \\ldots + \\alpha_M h_M(x)]$$\n",
    "where:\n",
    "- $h_m(x)$ is the $m^{th}$ weak learner (typically a decision stump)\n",
    "- $\\alpha_m$ is the learner's weight (confidence)\n",
    "- The $\\frac{1}{2}$ factor simplifies the relationship between $\\alpha_m$ and the loss function\n",
    "\n",
    "<br>\n",
    "\n",
    "**Optimization Objective:**  \n",
    "AdaBoost implicitly minimizes the exponential loss:\n",
    "$$Loss(y, \\hat{y}) = e^{-yH_m(x)}$$\n",
    "$$\\hat{y} = sign(H_M(x)), \\quad$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Key Properties of Exponential Loss:**\n",
    "- **Differentiability:**\n",
    "    - Smooth and convex, enabling efficient optimization\n",
    "\n",
    "- **Margin Maximization:**\n",
    "    - Heavily penalizes misclassifications ($\\mathcal{L} \\to \\infty$ when $yH(x) \\to -\\infty$)\n",
    "    - Still assigns non-zero loss to correctly classified examples near the decision boundary ($yH(x)$ slightly positive)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/exponential_loss_function.png\" alt=\"Exponential Loss Function\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40783d",
   "metadata": {},
   "source": [
    "##### **Finding the Optimal Weak Learner $h_m$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f6d3e",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "At iteration $m$, minimize:\n",
    "$$E = \\sum_{i=1}^N \\exp\\left(-y^{(i)}H_m(x^{(i)})\\right)$$\n",
    "\n",
    "**Current Ensemble:**\n",
    "$$H_m(x) = H_{m-1}(x) + \\tfrac{1}{2}\\alpha_m h_m(x)$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: Rewrite Loss Function**\n",
    "\n",
    "Substitute $H_m(x)$ into $E$:\n",
    "$$\n",
    "E = \\sum_{i=1}^N \\exp\\left(-y^{(i)}\\left[H_{m-1}(x^{(i)}) + \\tfrac{1}{2}\\alpha_m h_m(x^{(i)})\\right]\\right) \\\\\n",
    "= \\sum_{i=1}^N \\underbrace{\\exp\\left(-y^{(i)}H_{m-1}(x^{(i)})\\right)}_{w_m^{(i)}} \\cdot \\exp\\left(-\\tfrac{1}{2}\\alpha_m y^{(i)}h_m(x^{(i)})\\right)\n",
    "$$\n",
    "\n",
    "where $w_m^{(i)}$ are the sample weights from previous iterations.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Analyze Cases**\n",
    "\n",
    "For each sample:\n",
    "\n",
    "1. **Correct Classification** ($y^{(i)} = h_m(x^{(i)})$):\n",
    "   $$\\exp\\left(-\\tfrac{1}{2}\\alpha_m\\right)$$\n",
    "\n",
    "2. **Incorrect Classification** ($y^{(i)} \\neq h_m(x^{(i)})$):\n",
    "   $$\\exp\\left(\\tfrac{1}{2}\\alpha_m\\right)$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "E = \\exp\\left(-\\tfrac{\\alpha_m}{2}\\right)\\sum_{\\text{correct}} w_m^{(i)} + \\exp\\left(\\tfrac{\\alpha_m}{2}\\right)\\sum_{\\text{incorrect}} w_m^{(i)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Simplify Expression**\n",
    "\n",
    "Add and subtract $\\exp(\\frac{-\\alpha_m}{2})\\sum_{\\text{incorrect}} w_m^{(i)}$:\n",
    "\n",
    "$$\n",
    "E = \\exp\\left(-\\tfrac{\\alpha_m}{2}\\right)\\left[\\sum_{\\text{correct}} w_m^{(i)} + \\sum_{\\text{incorrect}} w_m^{(i)}\\right] + \\left[\\exp\\left(\\tfrac{\\alpha_m}{2}\\right) - \\exp\\left(-\\tfrac{\\alpha_m}{2}\\right)\\right]\\sum_{\\text{incorrect}} w_m^{(i)}\n",
    "$$\n",
    "\n",
    "Which simplifies to:\n",
    "$$\n",
    "E = \\left(\\exp(\\frac{\\alpha_m}{2}) - \\exp(-\\frac{\\alpha_m}{2})\\right) W_{\\text{incorrect}} + \\exp(-\\frac{\\alpha_m}{2}) W_{\\text{total}}\n",
    "$$\n",
    "where:\n",
    "- $W_{\\text{incorrect}} = \\sum_{y^{(i)} \\neq h_m(x^{(i)})} w_m^{(i)}$\n",
    "- $W_{\\text{total}} = \\sum_{i=1}^N w_m^{(i)}$\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "Since $W_{\\text{total}}$ is fixed, minimizing $E$ reduces to minimizing:\n",
    "$$\n",
    "J_m = W_{\\text{incorrect}} = \\sum_{i=1}^N w_m^{(i)} I(y^{(i)} \\neq h_m(x^{(i)}))\n",
    "$$\n",
    "\n",
    "**Find** $h_m(x)$ **that minimizes** $J_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128897b",
   "metadata": {},
   "source": [
    "##### **Finding the Optimal Classifier Weight $\\alpha_m$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e475bb1",
   "metadata": {},
   "source": [
    "We need to find the weight $\\alpha_m$ that minimizes the exponential loss:\n",
    "$$E = (e^{\\frac{\\alpha_m}{2}} - e^{\\frac{-\\alpha_m}{2}}) \\sum_{y^{(i)} \\neq h_m(x^{(i)})} w_m^{(i)} +  e^{\\frac{-\\alpha_m}{2}} \\sum_{i=1}^N w_m^{(i)}$$\n",
    "\n",
    "**Differentiate the Loss Function:**\n",
    "$$\\frac{\\partial E}{\\partial \\alpha_m} = \\frac{1}{2}(\\frac{e^{\\alpha_m}}{2} + \\frac{-e^{\\alpha_m}}{2}) \\sum_{y^{(i)} \\neq h_m(x^{(i)})} w_m^{(i)} - \\frac{1}{2} e^{\\frac{-\\alpha_m}{2}} \\sum_{i=1}^N w_m^{(i)} = 0$$\n",
    "\n",
    "**Simplify the Equation**:\n",
    "$$\\frac{e^{\\frac{-\\alpha_m}{2}}}{\\frac{\\alpha_m}{2} + \\frac{-\\alpha_m}{2}} = \\frac{\\sum_{y^{(i)} \\neq h_m(x^{(i)})} w_m^{(i)}}{\\sum_{i=1}^N w_m^{(i)}}$$\n",
    "\n",
    "Divide both sides by $e^{\\frac{-\\alpha_m}{2}}$ and introduced weighted error rate $\\epsilon_m$:\n",
    "$$\\frac{1}{1 + e^{\\alpha_m}} = \\epsilon_m$$\n",
    "\n",
    "**Solve for $\\alpha_m$:**\n",
    "$$1 = \\epsilon_m + \\epsilon_m e^{-\\alpha_m} \\implies e^{\\alpha_m} = \\frac{1 - \\epsilon_m}{\\epsilon_m}$$\n",
    "As result:\n",
    "$$\\alpha_m = \\ln \\frac{1 - \\epsilon_m}{\\epsilon_m}$$\n",
    "\n",
    "**Final Result**  \n",
    "The optimal weight for classifier $h_m$ is:\n",
    "\n",
    "$$\n",
    "\\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right)\n",
    "$$\n",
    "\n",
    "where the weighted error rate is:\n",
    "\n",
    "$$\n",
    "\\epsilon_m = \\frac{\\sum_{i=1}^N w_m^{(i)} I(y^{(i)} \\neq h_m(x^{(i)}))}{\\sum_{i=1}^N w_m^{(i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4abd53",
   "metadata": {},
   "source": [
    "##### **Deriving the Sample Weight Update Rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf92d0",
   "metadata": {},
   "source": [
    "**Objective:**  \n",
    "We need to derive how sample weights evolve:\n",
    "$$w_{m+1}^{(i)} = e^{-y^{(i)}H_m(x^{(i)})}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Express Current Ensemble:**\n",
    "$$w_{m+1}^{(i)} = w_m^{(i)} e^{-\\frac{1}{2}\\alpha_m y^{(i)} h_m(x^{(i)})}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Transform the Indicator:**  \n",
    "We prove a useful identity:\n",
    "$$y^{(i)}h_m(x^{(i)}) = 1 - 2I(y^{(i)} \\neq h_m(x^{(i)}))$$\n",
    "**Verification:**\n",
    "- When **correct** ($y^{(i)} = h_m(x^{(i)})$):  \n",
    "    $1 = 1 - 2(0)$\n",
    "\n",
    "- When **incorrect** ($y^{(i)} \\neq h_m(x^{(i)})$):  \n",
    "    $-1 = 1 - 2(1)$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Substitute into Weight Update**\n",
    "$$w_{m+1}^{(i)} = w_m^{(i)} e^{-\\frac{1}{2} \\alpha_m \\left(1 - 2I ( y^{(i)} \\neq h_m(x^{(i)}) ) \\right)}$$\n",
    "$$ = w_m^{(i)} e^{-\\frac{1}{2} \\alpha_m} e^{ \\alpha_m \\left(I ( y^{(i)} \\neq h_m(x^{(i)}) ) \\right)}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Final Update Rule**  \n",
    "Since $e^{-\\frac{\\alpha_m}{2}}$ is constant across all samples, we can normalize it away, leaving:\n",
    "$$w_{m+1}^{(i)} = w_m^{(i)} e^{ \\alpha_m \\left(I ( y^{(i)} \\neq h_m(x^{(i)}) ) \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb23cbe",
   "metadata": {},
   "source": [
    "##### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bdc85d",
   "metadata": {},
   "source": [
    "**AdaBoost Algorithm Summary**\n",
    "\n",
    "**Initialization:**\n",
    "1. For $ i = 1 $ to $ N $:\n",
    "   - Initialize data weights: $ w_1^{(i)} = \\frac{1}{N} $\n",
    "\n",
    "**Main Loop (for $ m = 1 $ to $ M $):**\n",
    "- **Find classifier** $ h_m(x) $:  \n",
    "   Minimize the weighted error function\n",
    "\n",
    "- **Compute normalized error**:  \n",
    "   $ \\epsilon_m = $ weighted error of $ h_m(x) $\n",
    "\n",
    "- **Calculate component weight**:  \n",
    "   $ \\alpha_m = \\ln\\left(\\frac{1-\\epsilon_m}{\\epsilon_m}\\right) $\n",
    "\n",
    "- **Update example weights**:  \n",
    "   $ w_{m+1}^{(i)} = w_m^{(i)} \\cdot e^{\\alpha_m \\cdot \\mathbb{I}(y^{(i)} \\neq h_m(x^{(i)}))} $  \n",
    "   (then normalize)\n",
    "\n",
    "**Final Classifier:**\n",
    "$ \\hat{y} = \\text{sign}(H_M(x)) $  \n",
    "where:  \n",
    "$ H_M(x) = \\sum_{m=1}^M \\alpha_m h_m(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9d6a7",
   "metadata": {},
   "source": [
    "**Behavior**\n",
    "\n",
    "- Exponential loss goes down\n",
    "- Training error $H$ goes down\n",
    "- Weighted error $\\epsilon_m$ goes up $\\rightarrow$ votes $\\alpha_m$ goes down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54247c0d",
   "metadata": {},
   "source": [
    "**Base Learners**\n",
    "\n",
    "- Decision Stumps\n",
    "- Decision Trees\n",
    "- Multi-Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d14c5",
   "metadata": {},
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108c2bb",
   "metadata": {},
   "source": [
    "**Objective Function: Training Loss + Regularization**\n",
    "\n",
    "The objective function in machine learning typically consists of two components:\n",
    "$$obj(\\theta) = L(\\theta) + \\Omega(\\theta)$$\n",
    "Where:\n",
    "- $L(\\theta)$: Training Loss Function  \n",
    "    Measures how well the model fits the training data (predictive performance)\n",
    "\n",
    "- $\\Omega(\\theta)$: Regularization Term  \n",
    "    Controls model complexity to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3a3b7",
   "metadata": {},
   "source": [
    "**CART: Classification And Regression Tree**\n",
    "\n",
    "Key characteristics of CART models:\n",
    "- Members are classified into different leaves\n",
    "- Each leaf is assigned a real-valued score (unlike standard decision trees which only provide class decisions)\n",
    "- Provides richer interpretation beyond simple classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104b3a9",
   "metadata": {},
   "source": [
    "**Tree Boosting Objective:**\n",
    "\n",
    "The objective function for tree boosting:\n",
    "$$J(w) = \\sum_{i=1}^n l(y_i, {\\hat{y}_i}^{(t)}) + \\sum_{k=1}^t w_{(f_k)}$$\n",
    "Where:\n",
    "- $t$: Number of trees\n",
    "- $f_k$: $k$-th tree in the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146e83f",
   "metadata": {},
   "source": [
    "**Additive Training:**\n",
    "\n",
    "The boosting approach:\n",
    "- Keep existing trees fixed\n",
    "- Add one new tree at each iteration\n",
    "\n",
    "Prediction at step $t$:\n",
    "$${\\hat{y}_i}^{(t)} = \\sum_{k=1}^t f_k(x_i) = {y_i}^{(t-1)} + f_t(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf094c51",
   "metadata": {},
   "source": [
    "**Optimize the objective**\n",
    "\n",
    "**Mean Squared Error (MSE) Case**\n",
    "\n",
    "If we consider using mean squared error (MSE) as our loss function, the objective at step $t$ becomes:\n",
    "$$obj^{(t)} = \\sum_{i=1}^n \\left( y_i - ( {\\hat{y}_i}^{(t-1)} + f_t(x_i) ) \\right)^2 + \\sum_{k=1}^t w(f_k)$$\n",
    "\n",
    "Expansion of Training Loss:\n",
    "$$\\left( y_i - ( {\\hat{y}_i}^{(t-1)} + f_t(x_i) ) \\right)^2 = 2 \\left( {\\hat{y}_i}^{t-1} - y_i \\right) f_t(x_i) + f_t(x_i)^2 + \\text{constant}$$\n",
    "\n",
    "Resulting objective\n",
    "$$obj^{(t)} = \\sum_{i=1}^n \\left[ 2 \\left( {\\hat{y}_i}^{t-1} - y_i \\right) f_t(x_i) + f_t(x_i)^2 \\right] + w(f_t) + \\text{constant}$$\n",
    "\n",
    "**Key Components:**\n",
    "- **First-order term:** $2(\\hat{y}_i^{(t-1)} - y_i)f_t(x_i)$ (**residuals**)\n",
    "- **Second-order term:** $f_t(x_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633226d0",
   "metadata": {},
   "source": [
    "**General Loss Functions**\n",
    "\n",
    "For non-MSE losses (e.g., logistic loss), we donâ€™t have a nice quadratic form naturally. So, XGBoost uses a *second-order Taylor expansion of the loss function*:\n",
    "$$obj^{(t)} \\approx \\sum_{i=1}^n  \\left[ l( y_i, {\\hat{y}_i}^{(t-1)} ) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + w(f_t) + \\text{constant}$$\n",
    "\n",
    "where:\n",
    "$$g_i = \\partial_{ {\\hat{y}_i}^{(t-1)} } l(y_i, {\\hat{y}_i}^{(t-1)}) \\quad \\text{gradient}$$\n",
    "$$h_i = \\partial^2_{ {\\hat{y}_i}^{(t-1)} } l(y_i, {\\hat{y}_i}^{(t-1)}) \\quad \\text{Hessian}$$\n",
    "\n",
    "Simplified objective\n",
    "$$\\sum_{i=1}^n \\left[g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + w(f_t)$$\n",
    "\n",
    "**Advantage:** This formulation enables XGBoost to support arbitrary loss functions through gradient ($g_i$) and Hessian ($h_i$) computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d53062",
   "metadata": {},
   "source": [
    "**Regularization Term**\n",
    "\n",
    "Definition of the tree $f(x)$:\n",
    "$$f_t(x) = w_{q(x)}, \\quad w \\in R^T, \\quad q: R^d \\rightarrow \\{1, 2, \\ldots, T \\}$$\n",
    "Where:\n",
    "- $w$: Vector of scores on leaves\n",
    "- $q$: Function assigning each data point to the corresponding leaf\n",
    "- $T$: Number of leaves\n",
    "\n",
    "Complexity Definition:\n",
    "$$w(f) = \\eta T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2$$\n",
    "\n",
    "*There is more than one way to define complexity but this one works well in practice.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfc1e3",
   "metadata": {},
   "source": [
    "**The Structure Score**\n",
    "\n",
    "Objective value with the $t$-th tree as:\n",
    "$$obj^{(t)} \\approx \\sum_{i=1}^n \\left[g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\eta T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2$$\n",
    "\n",
    "Change the index of the summation because all the data points on the same leaf get the same score.  \n",
    "Let $I_j = {i | q(x_i) = j}$ be the set of indices assigned to leaf $j$:\n",
    "$$obj^{(t)} = \\sum_{j=1}^T \\left[(\\sum_{i \\in I_j} g_i)w_j + \\frac{1}{2}(\\sum_{i \\in I_j} h_i + \\lambda) w_j^2 \\right] + \\eta T$$\n",
    "\n",
    "Notation:\n",
    "$$G_j = \\sum_{i \\in I_j} g_i, \\quad H_j = \\sum_{i \\in I_j} h_i$$\n",
    "\n",
    "Final Form:\n",
    "$$obj^{(t)} = \\sum_{j=1}^T \\left[G_j w_j + \\frac{1}{2}(H_j + \\lambda) w_j^2 \\right] + \\eta T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b620062",
   "metadata": {},
   "source": [
    "**Optimal Solution**\n",
    "\n",
    "Objective function:\n",
    "$$obj^{(t)} = \\sum_{j=1}^T \\left[G_j w_j + \\frac{1}{2}(H_j + \\lambda) w_j^2 \\right] + \\eta T$$\n",
    "\n",
    "**Optimal Leaf Weights**\n",
    "$$w^*_j = - \\frac{G_j}{H_j + \\lambda}$$\n",
    "\n",
    "**Optimal Objective Value:**\n",
    "$$\\text{obj}^* = -\\frac{1}{2} \\sum_{j=1}^T \\frac{G_j}{H_j + \\lambda} + \\eta T$$\n",
    "\n",
    "**Interpretation:** $obj^*$ serves as a quality measure for tree structure $q(x)$, combining both:\n",
    "- Predictive power (through $G_j$ and $H_j$)\n",
    "- Model complexity (through $\\eta$ and $\\lambda$)\n",
    "\n",
    "This score functions similarly to impurity measures in decision trees, but with built-in complexity control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5208c8",
   "metadata": {},
   "source": [
    "**Learning the Tree Structure**\n",
    "\n",
    "While enumerating all possible trees is computationally infeasible, we can efficiently build trees by:\n",
    "- Optimizing one level at a time\n",
    "- Using the **Gain** metric to evaluate split quality\n",
    "\n",
    "**Gain Formula:**\n",
    "$$Gain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_L + \\lambda} - \\frac{(G_R + G_L)^2}{H_L + H_R + \\lambda} \\right] - \\eta$$\n",
    "\n",
    "**Key Terms:**\n",
    "- $G_L$ and $G_R$: Sum of gradients for the left child and the right child nodes after the split\n",
    "- $H_L$ and $H_R$: Sum of Hessians for the left child and the right child nodes\n",
    "- $\\lambda$: Regularization Term\n",
    "- $\\eta$: Complexity control\n",
    "\n",
    "**Intuition Behind the Gain Formula:**\n",
    "- Score after splitting:\n",
    "    $$\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_L + \\lambda}$$\n",
    "- Score before splitting:\n",
    "    $$\\frac{(G_R + G_L)^2}{H_L + H_R + \\lambda}$$\n",
    "\n",
    "**Role of Regularization:**\n",
    "- $\\lambda$: Smooths leaf weights\n",
    "    - Prevents overfitting by shrinking leaf weights\n",
    "- $\\eta$: Minimum gain threshold\n",
    "    - Acts as early stopping criterion\n",
    "    - if $\\text{Gain} \\lt \\eta$, XGBoost stops splitting (prunes the branch)\n",
    "\n",
    "**Implementation Note:** XGBoost evaluates all possible splits at each level and selects the one with maximum Gain, continuing recursively until stopping conditions are met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9428a8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}