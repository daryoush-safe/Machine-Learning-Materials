{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0c646e",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095b015",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d44db",
   "metadata": {},
   "source": [
    "One of the most **intuitive** classifiers that is **easy** to understand and construct\n",
    "\n",
    "**Main application:** Database Mining  \n",
    "**Key Limitation:** Prone to overfitting (without proper regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841e341",
   "metadata": {},
   "source": [
    "**Structure**\n",
    "\n",
    "- **Leaves (Terminal Nodes):**\n",
    "    - Represent target variable\n",
    "    - Each leaf represents a class label\n",
    "\n",
    "- **Internal Nodes:**\n",
    "    - Test an attribute's value, with branches for possible outcomes.\n",
    "\n",
    "- **Edges**\n",
    "    - Connect nodes to children based on possible attribute test results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fcbc58",
   "metadata": {},
   "source": [
    "**Decision Tree Learning**\n",
    "\n",
    "**Goal:** Construct a decision tree from training data to classify new instances.\n",
    "\n",
    "Common Algorithms:\n",
    "- **ID3:**\n",
    "- **C4.5:** \n",
    "\n",
    "**Key Characteristics:**\n",
    "- Primarily used for **classification** (though regression variants exist).\n",
    "- Usually operates on **discrete domain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda68168",
   "metadata": {},
   "source": [
    "**Computational Complexity**\n",
    "- Difference in **Feature Space**\n",
    "    - Algorithms such as Linear Regression, SWM, etc:\n",
    "        - Operate in **continuous feature** spaces where optimization is often convex\n",
    "    - Decision Tree:\n",
    "        - Work with **discrete structures** (tree configurations), making the search space combinatorial.\n",
    "\n",
    "- Complexity:\n",
    "    - **NP-Completeness**:\n",
    "        - Finding the globally optimal decision tree is NP-complete.\n",
    "        - Reason: Discrete feature space \n",
    "    - Practical Solution:\n",
    "        - Use **greedy top-down recursion** with **heuristic** splitting criteria (e.g., information gain).."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3847e",
   "metadata": {},
   "source": [
    "**Learning Strategy:**\n",
    "\n",
    "- **Top-Down** Induction:\n",
    "    - Start at the root, choose the attribute that **best splits the data** (e.g., maximizes information gain).\n",
    "    - **Recursively** split subsets until:\n",
    "        - All samples in a node belong to one class (**pure**).\n",
    "        - **No further informative** splits exist.\n",
    "\n",
    "- Attribute Usage:\n",
    "    - Each **discrete** attribute is tested **only once** per path from root to leaf.\n",
    "    - **Continuous** attributes may be **reused with different thresholds** (e.g., \"Age > 30\" vs. \"Age > 50\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84659c",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "- **Sub-optimality:** Greedy search may miss better trees deeper in the search space.\n",
    "- **Overfitting:** Deep trees memorize training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d382f28",
   "metadata": {},
   "source": [
    "### **ID3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae524c",
   "metadata": {},
   "source": [
    "**Constructing a Basic Decision Tree**\n",
    "\n",
    "**Objective:** Build a **simple**, **compact** tree with **few nodes**.\n",
    "\n",
    "**Key Principles:**\n",
    "- **Simplicity:** Prefer shorter trees to avoid overfitting.\n",
    "- **Homogeneity-Driven:** Splits aim to create pure subsets (all samples in a node share the same class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07593077",
   "metadata": {},
   "source": [
    "**Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb90a95",
   "metadata": {},
   "source": [
    "- **Root Attribute Selection:**\n",
    "    - Criterion: Choose the attribute that best separates the data into homogeneous groups.\n",
    "    - Use some metrics such as *Information Gain* or *Gini Impurity* *(which we wil discuss them later)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70be22c",
   "metadata": {},
   "source": [
    "- **Forming Descendant:**\n",
    "    - **Rule:** For each value of the selected attribute $A$:\n",
    "        - **Create a branch:** One per unique value of $A$\n",
    "        - **Partition Data:** $S_v = \\{x \\in S | A(x) = v\\}$\n",
    "        - **Recursion:** Apply ID3 to $S_v$ with remaining attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d327d4",
   "metadata": {},
   "source": [
    "- **Stop Condition:**\n",
    "    - All samples in a node belong to the same class.\n",
    "    - No attributes left to split.\n",
    "    - Minimum samples per node reached (hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83ef2b",
   "metadata": {},
   "source": [
    "- **Guaranteed Consistency:**\n",
    "    - ID3 will always find a decision tree that **perfectly fits** any **conflict-free** training set.\n",
    "    - **No training** error in **conflict-free** training set.\n",
    "    - *Conflict-free:* Identical feature vectors must have identical class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115d1a4",
   "metadata": {},
   "source": [
    "- **Limitations**\n",
    "    - Not necessarily find the simplest tree (containing minimum number of nodes)\n",
    "    - **Locally-optimal** decisions at each node\n",
    "    - No backtracking\n",
    "    - Globally minimal tree is **NP-complete** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38083d",
   "metadata": {},
   "source": [
    "> Function FindTree(S,A)\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;If empty(A) or all labels of the samples in $S$ are the same  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;status $\\leftarrow$ leaf  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class $\\leftarrow$ most common class in the labels of $S$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;else  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;status $\\leftarrow$ internal  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a $\\leftarrow$ bestAttribute(S,A)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LeftNode $\\leftarrow$ FindTree(S($a \\gt t$), $A - \\{a\\}$)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RightNode $\\leftarrow$ FindTree(S($a \\le t$), $A - \\{a\\}$)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;end if  \n",
    "> end function\n",
    "\n",
    "Where:\n",
    "- Function input: S (Samples), A (Attributes)\n",
    "- *bestAttribute* function returns the attribute test\n",
    "- t (threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7c380",
   "metadata": {},
   "source": [
    "More Advance Pseudo Code: (CHECK IT LATER AGAIN)\n",
    "\n",
    ">ID3 (Examples,Target_Attribute,Attributes)  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Create a root node for the tree???  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;If all examples are positive:  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return the single-node, with label = +  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;If all examples are negative:  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return the single-node, with label = -  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;If number of predicting attributes is empty then:  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return the single-node, with label = most common value of the target attribute in the examples  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;else:  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A = The Attribute that best classifies examples  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Testing attribute for Root = A  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for each possible value, $v_i$ of A:  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a new tree branch below A, corresponding to the testA =$v_i$  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Let Examples($v_i$) be the subset of examples that have the value for A  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if Examples($v_i$) is empty then:  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;below this new branch add a leaf node with label = most common target value in the examples  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;below this new branch add subtree ID3(Examples($v_i$), Target_Attribute, Attributes â€“ {A})  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;return Root  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f6fc0",
   "metadata": {},
   "source": [
    "### **Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14504b29",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e93a1",
   "metadata": {},
   "source": [
    "To determine the optimal attribute for splitting, we evaluate candidates using **splitting heuristics**.\n",
    "\n",
    "The two most common metrics are:\n",
    "- **Information Gain**\n",
    "- **Gini Impurity**\n",
    "\n",
    "These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09794bb",
   "metadata": {},
   "source": [
    "#### **Information Gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655f1ac",
   "metadata": {},
   "source": [
    "##### **Entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0e6a6",
   "metadata": {},
   "source": [
    "Entropy measures the uncertainty or unpredictability in a specific distribution.\n",
    "\n",
    "**Formula:**\n",
    "$$H(X) = -\\sum_{x_i \\in X} p(x_i) \\log{p(x_i)}$$\n",
    "\n",
    "**Information theory:**\n",
    "- **Expected Code Length:**\n",
    "    - $H(X)$ is the average number of bits needed to encode one randomly drawn value of $X$.\n",
    "    - *How many yes/no questions (bits) do we need, on average, to guess the value of $X$?*\n",
    "\n",
    "- **Optimal Encoding:**\n",
    "    - The most efficient code assigns $âˆ’\\log_2{P(x_iâ€‹)}$ bits to encode outcome $x_i$\n",
    "    - Rare events should be assigned longer codes (because they happen less often).\n",
    "    - Common events get shorter codes.\n",
    "\n",
    "- **Uncertainty Relationship:**\n",
    "    - Higher entropy $\\rightarrow$ More uncertainty $\\rightarrow$ Harder to predict outcomes.\n",
    "    - Lower entropy $\\rightarrow$ Less uncertainty $\\rightarrow$ Easier to predict outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93bc9f",
   "metadata": {},
   "source": [
    "**Example: Entropy for a Boolean variable**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/entropyOnaGrapah.png\" alt=\"Entropy for a Boolean\">\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "- $$H(X = 0) = 0 \\log_2{0} - 1 \\log_2{1} = 0$$\n",
    "- $$H(X = 0) = - 1 \\log_2{1} - 0 \\log_2{0} = 0$$\n",
    "- $$H(X = 0.5) = -0.5 \\log_2{\\frac{1}{2}} - 0.5 \\log_2{\\frac{1}{2}} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6ca36",
   "metadata": {},
   "source": [
    "**Conditional Entropy**\n",
    "\n",
    "Conditional entropy $$H(Y | X)$ measures the average uncertainty in $Y$ given knowledge of $X$:\n",
    "$$H(Y | X) = E_{x \\sim X} [H(Y | X = x)]$$\n",
    "Where:\n",
    "$H(Y | X = x)$ is the entropy of $Y$ for a fixed value $X = x$\n",
    "\n",
    "Entropy for a $X = x$:\n",
    "$$H(Y | X = x) = -\\sum_{y \\in Y} p(y | x) \\log{p(y | x)}$$\n",
    "\n",
    "Take Expectation Over $X$:\n",
    "$$H(Y | X) = -\\sum_{x \\in X} p(x) H(Y | X = x)$$\n",
    "\n",
    "Substitute $H(Y | X = x)$:\n",
    "$$- \\sum_{x \\in X} p(x) \\sum_{y \\in Y} p(y | x) \\log{p(y | x)}$$\n",
    "\n",
    "Since $p(x, y) = p(x) p(y|x)$:\n",
    "$$- \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log{\\frac{p(x, y)}{p(x)}}$$\n",
    "$$= \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log{\\frac{p(x)}{p(x, y)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e8ca7",
   "metadata": {},
   "source": [
    "##### **Information Gain (IG)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd074af4",
   "metadata": {},
   "source": [
    "- **Definition:** Information Gain (IG) **quantifies the reduction in uncertainty** (entropy) of the target variable $Y$ after splitting samples $S$ by attribute $A$.\n",
    "\n",
    "- **Input:** Samples S, Attribute A.\n",
    "- **Output:** Numerical value representing the utility of splitting on A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77c050",
   "metadata": {},
   "source": [
    "**Formula:**\n",
    "$$\\text{Gain} (S, A) = H_S(Y) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H_{S_v}(Y)$$\n",
    "\n",
    "Where:\n",
    "- $A$: Variable used to split samples\n",
    "- $Y$: Target variable\n",
    "- $S$: Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da268ac0",
   "metadata": {},
   "source": [
    "**Formula Breakdown:**\n",
    "\n",
    "- $H_S(Y)$: Entropy of $Y$ before splitting (baseline uncertainty).\n",
    "\n",
    "- $H_{S_v}(Y)$: Measures purity of subsets ($S_v$) created by splitting.\n",
    "\n",
    "- $\\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H_{S_v}(Y)$: Weighted average uncertainty of subsets.\n",
    "\n",
    "**Goal:**\n",
    "- **Maximize IG:** Choose splits that most reduce uncertainty in $Y$.\n",
    "    - **High IG:** Attribute A effectively separates classes.\n",
    "    - **Low IG:** Attribute A provides little predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949ebf4",
   "metadata": {},
   "source": [
    "##### **Mutual Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2966e",
   "metadata": {},
   "source": [
    "The expected reduction in entropy of $Y$ caused by knowing $X$:\n",
    "$$I(X, Y) = H(Y) - H(Y | X)$$\n",
    "\n",
    "Marginal Entropy:\n",
    "$$H(Y) = -\\sum_{j} p(y_j) \\log{p(y_j)}$$\n",
    "\n",
    "Conditional Entropy:\n",
    "$$H(X | Y) = -\\sum_{i} \\sum_{j} p(x_i, y_j) \\log{p(y_j) | p(x_i)}$$\n",
    "$$= \\sum_{i} \\sum_{j} p(x_i, y_j) \\log{\\frac{p(x_i)}{p(x_i, y_j)}}$$\n",
    "\n",
    "Subtract Them:\n",
    "$$H(Y) - H(X | Y) = -\\sum_{j} p(y_j) \\log{p(y_j)} + \\sum_{i} \\sum_{j} p(x_i, y_j) \\log{p(y_j) | p(x_i)}$$\n",
    "\n",
    "Since (Law of Total Probability):\n",
    "$$p(y_j) = \\sum_{i}p(x_i, y_j)$$\n",
    "\n",
    "Substitute into the Equation:\n",
    "$$H(Y) - H(X | Y) = \\sum_{j} \\left(\\sum_{i} p(x_i, y_j) \\right) \\log{\\frac{1}{p(y_j)}} + \\sum_{i} \\sum_{j} p(x_i, y_j) \\log{\\frac{p(x_i)}{p(x_i, y_j)}}$$\n",
    "\n",
    "Combine the Sums:\n",
    "$$I(X, Y) = H(Y) - H(X | Y) = \\sum_{i} \\sum_{j} p(x_i, y_j) \\left(\\log{\\frac{p(x_i)p(y_j)}{p(x_i, y_j)}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558900e7",
   "metadata": {},
   "source": [
    "**Key Properties:**\n",
    "- Marginal Entropy\n",
    "- Conditional Entropy\n",
    "- Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5d0e2",
   "metadata": {},
   "source": [
    "**Marginal Entropy $H(Y)$:**  \n",
    "- **Definition:**  \n",
    "Measures the uncertainty (or disorder) in the target variable $Y$\n",
    "\n",
    "- **Formula:**\n",
    "$$H(Y) = -\\sum_{j} p(y_j) \\log{p(y_j)}$$\n",
    "\n",
    "- **Interpretation:**  \n",
    "High $H(Y)$: Labels are evenly distributed (harder to predict).  \n",
    "Low $H(Y)$: One label dominates (easier to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedcfd5",
   "metadata": {},
   "source": [
    "**Conditional Entropy $H(Y|X)$**  \n",
    "- **Definition:**  \n",
    "Expected entropy of $Y$ after splitting the data based on attribute $X$.\n",
    "\n",
    "- **Formula:**\n",
    "    $$H(Y | X) = -\\sum_{x \\in X} p(x) H(Y | X = x)$$\n",
    "    Where:\n",
    "    $$H(Y | X = x) = -\\sum_{y \\in Y} p(y | x) \\log{p(y | x)}$$\n",
    "\n",
    "- **Interpretation:**  \n",
    "Averages the entropy of $Y$ across all subsets created by splitting on $X$.  \n",
    "Low $H(Yâˆ£X)$: $X$ creates homogeneous subsets (good split)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f01596",
   "metadata": {},
   "source": [
    "**Mutual Information $I(X, Y)$**  \n",
    "- **Formula:**\n",
    "    $$I(X, Y) = H(Y) - H(Y | X)$$\n",
    "    Quantifies how much $X$ reduces uncertainty about $Y$.\n",
    "\n",
    "- **Key Implications:**\n",
    "    - **Independence Case $I(X, Y) = 0$:**   \n",
    "        - Condition:\n",
    "            $$I(X, Y) = 0 \\iff p(X, Y) = p(X) p(Y)$$\n",
    "            Occurs when the logarithm term equals zero (since $\\log{1}=0$).\n",
    "        - Interpretation:\n",
    "            - $X$ and $Y$ are **independent**.\n",
    "            - Knowing $X$ provides **no information** about $Y$\n",
    "\n",
    "    - **Dependence Case $I(X, Y) \\gt 0$**  \n",
    "        - Interpretation:\n",
    "            - $X$ **reduces uncertainty** about $Y$\n",
    "            - Higher $I(X, Y)$ $\\rightarrow$ Stronger dependence\n",
    "            - In **Feature Selection**: Prefer higher mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae5982",
   "metadata": {},
   "source": [
    "**Selecting the Best Attribute for Splitting**\n",
    "\n",
    "Identify the attribute that maximizes Information Gain (IG) or Mutual Information (MI) when splitting a set of samples $S$.  \n",
    "This ensures the most significant reduction in uncertainty (entropy) about the target variable $Y$\n",
    "\n",
    "**Recall: Mutual Information**\n",
    "$$\\text{Gain}(S, X_i) = H_S(Y) - H_S(Y | X_i)$$\n",
    "\n",
    "**Optimal Attribute Selection:**  \n",
    "Choose $j$-th attribute for test in this node where:\n",
    "$$j = \\argmax_{i \\in attributes} \\text{Gain}(S, X_i)$$\n",
    "Equivalently:\n",
    "$$j = \\argmax_{i \\in attributes} H_S(Y) - H_S(Y | X_i)$$\n",
    "\n",
    "- **Single-Node Context:**\n",
    "    $$j = \\argmin_{i \\in attributes} H_S(Y | X_i)$$\n",
    "\n",
    "- **Cross-Node Comparison:**\n",
    "    - If comparing splits across different nodes (with different sample sets $S$) $H_S(Y)$ varies.\n",
    "    - It is invalid to directly compare $H_S(Y | X_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b846a4",
   "metadata": {},
   "source": [
    "#### **Gini Impurity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc9121",
   "metadata": {},
   "source": [
    "- Gini impurity is a **measure of impurity** or randomness used in decision tree algorithms.\n",
    "\n",
    "- It quantifies how well a node in a decision tree splits the data.\n",
    "\n",
    "- **Lower Gini** impurity $\\rightarrow$ a more **homogeneous** node $\\rightarrow$ most data points belong to the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead6637",
   "metadata": {},
   "source": [
    "**Formula:**\n",
    "$$G(X) = 1 - \\sum_{x_i \\in X} p(x_i)^2$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/gini.png\" alt=\"Gini\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746cef7",
   "metadata": {},
   "source": [
    "### **Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeba023",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35ebb1",
   "metadata": {},
   "source": [
    "**Optimal Decision Trees: Balancing Size and Generalization**\n",
    "\n",
    "**Smaller trees are preferred**\n",
    "- Fewer short hypothesis than long ones\n",
    "- Lower variance of the smaller trees\n",
    "- Lower risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf58530",
   "metadata": {},
   "source": [
    "**Overfitting Problem In ID3**\n",
    "\n",
    "ID3 algorithm perfectly classifies training data (non-conflict data)\n",
    "\n",
    "**Causes**:\n",
    "- **Small Data:**  \n",
    "    Limited samples lead to poor decisions\n",
    "\n",
    "- **Noisy Data:**  \n",
    "    Trees fit irrelevant outliers or mislabeled examples.\n",
    "\n",
    "- **Irrelevant Attributes:**  \n",
    "    ID3 may split on non-predictive features, creating useless branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87845b6",
   "metadata": {},
   "source": [
    "Given:\n",
    "- Hypothesis space $H$: decision trees\n",
    "- Training (empirical) error of $h \\in H: error_{train}(h)$\n",
    "- Expected error of $h \\in H: error_{true}(h)$\n",
    "\n",
    "$h$ overfits training data if there is a $h' \\in H$ such that:\n",
    "- $error_{train}(h) \\lt error_{train}(h')$\n",
    "- $error_{true}(h) \\gt error_{true}(h')$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/overfittingDecisionTree.png\" alt=\"Overfitting\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699bf71",
   "metadata": {},
   "source": [
    "**Avoiding Overfit**\n",
    "\n",
    "- **Early Stopping**\n",
    "- **Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869822e",
   "metadata": {},
   "source": [
    "#### **Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fe40e",
   "metadata": {},
   "source": [
    "**Stop growing** when the data split is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661081a0",
   "metadata": {},
   "source": [
    "#### **Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1580d51",
   "metadata": {},
   "source": [
    "**Post-pruning** is a technique to refine decision trees by removing branches that overfit training data while preserving predictive accuracy.\n",
    "\n",
    "Unlike pre-pruning (early stopping), it **first grows** the full tree and then **prunes it backward**.\n",
    "\n",
    "Yielding **better results** in practice than pre-pruning (early stopping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbc289",
   "metadata": {},
   "source": [
    "**Training Process:**\n",
    "\n",
    "- **Split Data**  \n",
    "    Divide the dataset into:\n",
    "    - **Training Set:** Used to build the full tree\n",
    "    - **Validation Set:** Used to evaluate pruning decisions\n",
    "\n",
    "- **Construct Full Tree**  \n",
    "    Grow the tree until:\n",
    "    - All leaves are **pure**\n",
    "    - **No attributes** left\n",
    "\n",
    "- **Iterative Pruning**  \n",
    "    Repeat until further pruning is harmful:\n",
    "    - **Evaluate impact on validation set** when pruning sub-tree rooted at each node\n",
    "        - **Temporarily remove** sub-tree rooted at node\n",
    "        - Replace it with a **leaf labeled** with the current majority class at that node\n",
    "        - **Measure** and record error on validation set\n",
    "    - **Greedily remove** the one that **most improves** validation set accuracy (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da8a80",
   "metadata": {},
   "source": [
    "### **C4.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd41d5",
   "metadata": {},
   "source": [
    "*(Extension of ID3 with rule-based pruning and handling of continuous attributes)*\n",
    "\n",
    "Decision tree = a set of rules\n",
    "\n",
    "- Each path from root to leaf represents a conjunctive rule (AND conditions).\n",
    "- All leaves with class $Y = i$ form a disjunctive rule (OR of paths)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83275066",
   "metadata": {},
   "source": [
    "**Training Process:**\n",
    "- Learns a tree from data (like ID3, prone to overfitting)  \n",
    "- Convert the tree into the equivalent set of rules\n",
    "- Prune (generalize) each rule by removing any precondition that results in improving estimated accuracy  \n",
    "- Sort the pruned rules by their estimated accuracy  \n",
    "- consider them in sequence when classifying new instances  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e4cd7",
   "metadata": {},
   "source": [
    "**C4.5 vs. ID3**\n",
    "\n",
    "**In ID3 (Inflexible Pruning):**\n",
    "- **Tree-Structured Pruning:**  \n",
    "    If an attribute (e.g., \"Outlook\") is pruned, it is **removed from all descendant** nodes in the subtree.\n",
    "    - **Limitation:** Overly rigid.\n",
    "\n",
    "**In C4.5 (Flexible Rule-Based Pruning):**\n",
    "- **Per-Rule Condition Removal:**  \n",
    "    Each rule (derived from a root-to-leaf path) is pruned **independently**.\n",
    "    - **Advantage:** Preserves informative attributes where they matter, improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d9985",
   "metadata": {},
   "source": [
    "### **Continuous Attributes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26653fc6",
   "metadata": {},
   "source": [
    "For **continuous attributes**, we must:\n",
    "- **Define candidate thresholds** to discretize the feature.\n",
    "    - A candidate threshold is a value where the **class label changes** in sorted data.\n",
    "\n",
    "- **Evaluate each threshold** using Information Gain (or Gini Impurity).\n",
    "\n",
    "- **Select the optimal threshold** that maximizes separation between classes *(highest IG)*.\n",
    "    - *Unlike discrete* attributes, continuous features can be reused with different thresholds in other branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7129bed",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972a992",
   "metadata": {},
   "source": [
    "- **Simple**\n",
    "\n",
    "- **Interpretable**\n",
    "\n",
    "- Requires **little data preparation**\n",
    "\n",
    "- Handles both **numerical & categorical** data\n",
    "\n",
    "- **Time efficient** $\\rightarrow$ Can be used on **large datasets**\n",
    "\n",
    "- **Robust:** Performs well even if its assumptions are somewhat violated\n",
    "\n",
    "- **Issue: Bias Towards High-Cardinality Attributes**\n",
    "    - Information Gain tends to favor attributes with many unique values, even if theyâ€™re non-predictive.\n",
    "\n",
    "    - Attributes with many values can artificially split data into tiny, pure subsets.\n",
    "\n",
    "    - Example: *Visit Date* Attribute\n",
    "        - 100 patients, each with a unique visit date.\n",
    "        - IG will be maximized ($H(S)$) because each date perfectly \"predicts\" its patientâ€™s outcome.\n",
    "\n",
    "- **High Variance** tends to overfit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
