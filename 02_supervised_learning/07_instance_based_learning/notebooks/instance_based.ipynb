{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b625670",
   "metadata": {},
   "source": [
    "# Instance-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7613c",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719697fe",
   "metadata": {},
   "source": [
    "**Recall:**\n",
    "\n",
    "**Parametric methods:**\n",
    "- Assuming a parameterized model for density function.\n",
    "- A number of parameters are optimized by fitting the model to the data set\n",
    "\n",
    "**Non-parametric methods:**\n",
    "- No specific parametric model is assumed.  \n",
    "- The form of density function is determined entirely by the data\n",
    "- Training phase often just stores the data.\n",
    "\n",
    "Both *supervised* and *unsupervised* learning methods can be categorized into parametric and non-parametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488f832",
   "metadata": {},
   "source": [
    "**Instance-Based (Memory-Based) Learning**\n",
    "\n",
    "These methods store the entire training dataset and make predictions by retrieving and analyzing similar instances from memory.\n",
    "\n",
    "**Key Characteristics**\n",
    "- **No Explicit Training:**  \n",
    "    Unlike parametric models, no parameters are learned; the training phase simply stores data.\n",
    "\n",
    "- **Lazy Learning:**  \n",
    "    Postpones the building of the model.  \n",
    "    Shorter time to train and a longer time to predict.  \n",
    "    (Almost) All the work at the test time\n",
    "\n",
    "- **Instance-Based Decision Making:**  \n",
    "    Predictions are based on local similarity to stored training examples (e.g., nearest neighbors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf893e",
   "metadata": {},
   "source": [
    "**Histogram Approximation**\n",
    "\n",
    "Divide the data into $L$ bins and count the samples in each bin $b_l$:\n",
    "$$p(b_l) \\approx \\frac{k_n(b_l)}{n}, \\quad l = 1, ..., L$$\n",
    "Where:\n",
    "- $k_n(b_l)$ number of samples (out of $n$) in bin $b_l$\n",
    "\n",
    "**Estimated Probability Density Function (PDF)**\n",
    "\n",
    "For a fixed point $x$ within $b_l$:\n",
    "$$\\hat{p}(x) = \\frac{p(b_l)}{h}$$\n",
    "Where:\n",
    "- $h$: bin width\n",
    "- $|x - \\bar{x}_{bl}| \\le \\frac{h}{2}$\n",
    "- $\\bar{x}_{bl}$: Mid-point of the bin $b_l$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/histogram.png\" alt=\"Histogram\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5cf9b",
   "metadata": {},
   "source": [
    "### **Density Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764fd83",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd7b8a",
   "metadata": {},
   "source": [
    "**Given:**\n",
    "- $D = \\{x^{(i)}\\}_{i=1}^n$: A set of samples drawn i.i.d. according to $p(x)$\n",
    "\n",
    "Probability of Falling in a Region $R$:\n",
    "$$P = \\int_R p(x') dx'$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Binomial Distribution for Sample Counts**  \n",
    "Probability of $k$ of the $n$ samples falling in $R$ follows a binomial distribution:\n",
    "$$P_k = \\binom{n}{k}P^k(1 - P)^{n-k}$$\n",
    "Where:\n",
    "- **Mean:** $E[k] = nP$\n",
    "- **Variance:** $Var(k) = nP(1-P)$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Key Insight:**  \n",
    "For large $n$ the distribution of $k$ peaks sharply around the mean ($nP$), Thus\n",
    "$$k \\approx nP \\implies \\frac{k}{n} \\approx P$$\n",
    "- This makes $\\frac{k}{n}$ a consistent estimator for $P$\n",
    "- More accurate for large $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cfb2f",
   "metadata": {},
   "source": [
    "**Estimating $p(x)$ from $P$**\n",
    "\n",
    "**Assumptions:**\n",
    "- $p(x)$ is **continuous** (smooth)\n",
    "- Region $R$ is small enough that $p(x)$ is approximately constant within it.\n",
    "\n",
    "**Approximation**\n",
    "$$P = \\int_R p(x')dx' = p(x) V$$\n",
    "Where:\n",
    "$$V = Vol(R)$$\n",
    "\n",
    "Result:\n",
    "$$x \\in R \\implies p(x) = \\frac{P}{V} \\approx \\frac{\\frac{k}{n}}{V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc4cd9",
   "metadata": {},
   "source": [
    "Let:\n",
    "- $p_n(x)$: Estimate of $p(x)$ using $n$ samples\n",
    "- $V_n(x)$: Volume of region around $x$\n",
    "- $k_n$: Number of samples falling in the region\n",
    "\n",
    "Then:\n",
    "$$p_n(x) = \\frac{k_n}{n V_n}$$\n",
    "\n",
    "Conditions for converge of $p_n(x)$ to $p(x)$:\n",
    "$$\\lim_{n \\rightarrow \\infty} V_n = 0$$\n",
    "$$\\lim_{n \\rightarrow \\infty} k_n = \\infty$$\n",
    "$$\\lim_{n \\rightarrow \\infty} \\frac{k_n}{n} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c06d9",
   "metadata": {},
   "source": [
    "**Main Approaches of Satisfying Conditions**\n",
    "\n",
    "- **K-Nearest Neighbor Density Estimator:**\n",
    "    - Fix $K$ and determine the value of $V$ from the data\n",
    "    - Volume grows until it contains $K$ neighbors of $x$\n",
    "\n",
    "- **Kernel Density Estimator (Parzen Window):**\n",
    "    - Fix V and determine K from the data\n",
    "    - Number of points falling inside the volume can vary from point to point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59436913",
   "metadata": {},
   "source": [
    "#### **Parzen Window**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b5f78",
   "metadata": {},
   "source": [
    "##### **Window Function Conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afca105",
   "metadata": {},
   "source": [
    "Let $\\varphi(x)$ be a **window function** (also called a **kernel** or **potential function**)  \n",
    "It must satisfy following conditions:\n",
    "- **Non-negativity:**\n",
    "    $$\\varphi(x) \\ge 0$$\n",
    "- **Normalization**\n",
    "    $$\\int \\varphi(x) dx = 1$$\n",
    "\n",
    "Following conditions ensure that $\\varphi(x)$ behaves like a probability density function (PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e7092",
   "metadata": {},
   "source": [
    "##### **Hypercube Window Function for Density Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88878ea6",
   "metadata": {},
   "source": [
    "**Definition**  \n",
    "The hypercube window function is defined as:\n",
    "\n",
    "$$\n",
    "\\varphi(u) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } \\left( |u_1| \\leq \\frac{1}{2} \\land \\dots \\land |u_d| \\leq \\frac{1}{2} \\right) \\\\ \n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Meaning**: Returns 1 when point $u$ is inside a unit hypercube centered at origin, else 0.\n",
    "\n",
    "**Density Estimation Formula**  \n",
    "The density estimate at point $x$:\n",
    "\n",
    "$$\n",
    "p_n(x) = \\frac{k_n}{n V_n} = \\frac{1}{n V_n} \\sum_{i=1}^n \\varphi \\left( \\frac{x - x^{(i)}}{h_n} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $k_n = \\sum_{i=1}^n \\varphi \\left( \\frac{x - x^{(i)}}{h_n} \\right)$ → sample count in hypercube\n",
    "- $V_n = (h_n)^d$ → Volume of $d$-dimensional hypercube\n",
    "- $h_n$: The edge length of the hypercube (bandwidth).\n",
    "- $\\frac{x - x^{(i)}}{h_n}$: Normalized distance between $x$ and $x^{(i)}$\n",
    "\n",
    "**Key Result:**\n",
    "- final density estimate formula does not depend on $k_n$ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198500c",
   "metadata": {},
   "source": [
    "##### **Gaussian Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5246e",
   "metadata": {},
   "source": [
    "- Hypercube window function simply counts samples in a neighborhood of $x$\n",
    "- Gaussian kernel weight samples based on their distance to $x$\n",
    "\n",
    "**Formula:**  \n",
    "The density estimate at point $x$:\n",
    "$$\\hat{p(x)} = \\frac{1}{n} \\sum_{i=1}^n N(x | x^{(i)}, \\sigma^2)$$\n",
    "$$ = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - x^{(i)})^2}{2 \\sigma^2}}$$\n",
    "\n",
    "**Key Parameters:**\n",
    "- $\\sigma$ (bandwidth): Controls the smoothing scale (analogous to $h_n$ hypercube methods)\n",
    "- **Larger** $\\sigma$ $\\rightarrow$ Smoother estimates, Low resolution, **High bias**\n",
    "- **Smaller** $\\sigma$ $\\rightarrow$ More resolution but potentially noisy estimates, **High variance**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/gaussianDensityFunction.png\" alt=\"Gaussian Window\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c269cb",
   "metadata": {},
   "source": [
    "##### **Key Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f3d0b",
   "metadata": {},
   "source": [
    "**General Density Estimator Formula**\n",
    "$$p_n(x) = \\frac{k_n}{n V_n} = \\frac{1}{n h_n^d} \\sum_{i=1}^n \\varphi \\left( \\frac{x - x^{(i)}}{h_n} \\right)$$\n",
    "\n",
    "Where:\n",
    "- $k_n$: Number of samples within the window\n",
    "- $V_n = h_n^d$: Volume of the $d$-dimensional window\n",
    "- $\\varphi$: Kernel function (e.g., hypercube, Gaussian)\n",
    "- $h_n$: Bandwidth controlling window volume\n",
    "\n",
    "**Bandwidth Trade-offs**   \n",
    "For a fixed sample size $n$, bandwidth $h_n$:\n",
    "- **Too large:**\n",
    "    - Low resolution\n",
    "    - **High bias**\n",
    "    - **Low Variance**\n",
    "- **Too small:**\n",
    "    - High variability\n",
    "    - **Low Bias**\n",
    "    - **High Variance**\n",
    "\n",
    "For a fixed $h$, the **variance decreases** as the number of samples $n$ tends to $\\infty$\n",
    "\n",
    "**Practical Considerations**\n",
    "- **Finite samples:** must balance between $h_n$ and $n$\n",
    "- **Bandwidth Selection:**\n",
    "    - Techniques like cross-validation where the density estimation used for learning tasks such as classification\n",
    "    - smaller $h_n$ improves accuracy only when $n$ is sufficiently large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11216b",
   "metadata": {},
   "source": [
    "**Issue: Curse of Dimensionality**\n",
    "\n",
    "$n$ must **grow exponentially** with the dimensionality $d$  \n",
    "This phenomenon fundamentally limits non-parametric methods in high-dimensional spaces.\n",
    "\n",
    "For example in **hypercube density function**:\n",
    "- **One-Dimensional:** $n$ points are required to densely fill an interval\n",
    "- **$d$-dimensional:** $n^d$ points are required to fill the corresponding hypercube\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/curseOfDim.png\" alt=\"Curse of Dim Example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede1a39",
   "metadata": {},
   "source": [
    "#### **K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08585032",
   "metadata": {},
   "source": [
    "Cell volume is a function of the point location:  \n",
    "To estimate $p(x)$, let the cell around $x$ grow until it captures $k_n$ samples called $k_n$ nearest neighbors of $x$\n",
    "\n",
    "Two possibility:\n",
    "- **High density near $x$:**  \n",
    "    The cell volume $V_n(x)$ remains small, preserving fine details.\n",
    "\n",
    "- **Low density near $x$:**  \n",
    "    The cell expands until it captures k_n points, ensuring stable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a5814",
   "metadata": {},
   "source": [
    "**Formulation**\n",
    "\n",
    "$$p_n(x) = \\frac{k_n}{n V_n}$$\n",
    "Where:\n",
    "- $V_n(x)$ is the volume of the cell containing $k_n$ nearest neighbors of $x$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Optimal $k_n$ Selection:**  \n",
    "Scaling with sample size:\n",
    "$$k_n = k_1 \\sqrt{n}$$\n",
    "*(Assume $k_1 = 1$ (baseline) for simplicity)*\n",
    "\n",
    "<br>\n",
    "\n",
    "**Volume Behavior:**\n",
    "$$p_n(x) = \\frac{k_n}{n V_n} \\implies V_n \\approx \\frac{1}{p(x) \\sqrt{n}}$$\n",
    "Properties:\n",
    "- Automatically smaller in dense regions (p(x) large)\n",
    "- Larger in sparse regions (p(x) small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4372f3",
   "metadata": {},
   "source": [
    "**Effect of $K$**\n",
    "\n",
    "**Practical tuning** $K$ via **cross-validation** ensures optimal performance.\n",
    "\n",
    "- Small $K$:\n",
    "    - High variance\n",
    "    - Low bias\n",
    "- Large $K$:\n",
    "    - Low variance\n",
    "    - High bias\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/KNNExample.png\" alt=\"KNN Example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4c745",
   "metadata": {},
   "source": [
    "#### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ef0ed",
   "metadata": {},
   "source": [
    "- **Generality of Distribution**\n",
    "    - Convergence with enough samples\n",
    "    - Makes no assumptions about the underlying distributional form of the data.\n",
    "\n",
    "- **Number of required samples**\n",
    "    - To assure convergence number of samples must be very large\n",
    "    - Grows exponentially with the dimensionality of the feature space\n",
    "\n",
    "- Sensitive to Choice of **Window** or **Number of Nearest Neighbors**\n",
    "\n",
    "- Need to **save all training samples**\n",
    "    - Training phase simply requires storage of the training set\n",
    "    - Computational cost of evaluating $p(x)$ grows linearly with the size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d88387",
   "metadata": {},
   "source": [
    "### **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a04544",
   "metadata": {},
   "source": [
    "#### **Parzen Window**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adff617",
   "metadata": {},
   "source": [
    "**Generative Classification**\n",
    "\n",
    "Recall Bayes' theorem:\n",
    "$$p(y | x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "For each class $C_i$, we estimate:\n",
    "- Density\n",
    "$$\n",
    "p(x | C_i) = \\frac{k}{n_i V} = \\frac{1}{n_i h^d} \\sum_{x^{(i)} \\in {D_i}} \\varphi \\left( \\frac{x - x^{(i)}}{h} \\right)\n",
    "$$\n",
    "- Class Prior\n",
    "$$p(C_i) = \\frac{n_i}{n}$$\n",
    "Where\n",
    "- $D_i$ Training samples in class $i$\n",
    "- $n_i$: Number of training samples in class $C_i$\n",
    "- $h$: bandwidth\n",
    "- $\\varphi$: Kernel window function\n",
    "\n",
    "Decision Rule: Assign $x$ to the class with higher posterior probability:\n",
    "$$\n",
    "\\text{label} = \n",
    "\\begin{cases} \n",
    "C_1 & \\text{if } {p(x|C_1)p(C_1)} \\gt {p(x|C_2)p(C_2)} \\\\ \n",
    "C_2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\\\n",
    "\n",
    "The comparison simplifies to:\n",
    "$${p(x|C_1)p(C_1)} \\gt {p(x|C_2)p(C_2)} \\rightarrow \\frac{p(x|C_1)}{p(x|C_2)} \\gt \\frac{p(C_2)}{p(C_1)}$$\n",
    "\n",
    "Substitute $p(x | C_i)$ and $p(C_i)$:\n",
    "$$\\frac{\\frac{1}{n_1 h^d} \\sum_{x^{(i)} \\in {D_1}} \\varphi \\left( \\frac{x - x^{(i)}}{h} \\right)}{\\frac{1}{n_2 h^d} \\sum_{x^{(i)} \\in {D_2}} \\varphi \\left( \\frac{x - x^{(i)}}{h} \\right)} \\gt \\frac{n_2}{n_1}$$\n",
    "\n",
    "After cancellation of common terms:\n",
    "$$\\sum_{x^{(i)} \\in {D_1}} \\varphi \\left( \\frac{x - x^{(i)}}{h} \\right) \\gt \\sum_{x^{(i)} \\in {D_2}} \\varphi \\left( \\frac{x - x^{(i)}}{h} \\right)$$\n",
    "\n",
    "**Key Insight**\n",
    "- The final decision rule reduces to comparing:\n",
    "    - The number of $C_1$ neighbors vs $C_2$ neighbors within the kernel window\n",
    "    - This shows how a **generative approach** leads to a **discriminative solution**\n",
    "- For large $n$, it needs both high time and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d795e",
   "metadata": {},
   "source": [
    "#### **KNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8762f25",
   "metadata": {},
   "source": [
    "##### **Generative Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733a2a5",
   "metadata": {},
   "source": [
    "Recall Bayes' theorem:\n",
    "$$p(y | x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "For each class $C_i$, we estimate:\n",
    "- Density\n",
    "$$p(x | C_i) = \\frac{k}{n_i V}$$\n",
    "- Class Prior\n",
    "$$p(C_i) = \\frac{n_i}{n}$$\n",
    "Where\n",
    "- $n_i$: Number of training samples in class $C_i$\n",
    "\n",
    "Decision Rule: Assign $x$ to the class with higher posterior probability:\n",
    "$$\n",
    "\\text{label} = \n",
    "\\begin{cases} \n",
    "C_1 & \\text{if } {p(x|C_1)p(C_1)} \\gt {p(x|C_2)p(C_2)} \\\\ \n",
    "C_2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\\\n",
    "\n",
    "The comparison simplifies to:\n",
    "$$\\frac{p(x|C_1)}{p(x|C_2)} \\gt \\frac{p(C_2)}{p(C_1)}$$\n",
    "\n",
    "Substitute $p(x | C_i)$ and $p(C_i)$:\n",
    "$$\\frac{k n_2 v_2}{k n_1 v_1} \\gt \\frac{n_2}{n_1}$$\n",
    "\n",
    "Result is similar to *Parzen Window*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abe835",
   "metadata": {},
   "source": [
    "##### **Discriminative Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82068b",
   "metadata": {},
   "source": [
    "**Given**\n",
    "- Training dataset: $\\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(n)}, y^{(n)})\\}$\n",
    "- Test sample: $x$\n",
    "\n",
    "**Classification Steps**\n",
    "- Find $k$ nearest training samples to $x$\n",
    "- Among these $k$ neighbors, count how many belong to each class:  \n",
    "    $$k_j = \\text{Number of neighbors in class } C_j \\quad (j = 1, \\ldots, C)$$\n",
    "\n",
    "- Assign $x$ to the class $C_{j^*}$; where:\n",
    "    $$\n",
    "    j^* = \\operatorname*{argmax}_{j=1,\\ldots,C} k_j\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387915e",
   "metadata": {},
   "source": [
    "##### **Effect of $K$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61e641",
   "metadata": {},
   "source": [
    "**Practical tuning** $K$ via **cross-validation** ensures optimal performance.\n",
    "\n",
    "- Small $K$:\n",
    "    - High variance\n",
    "    - Low bias\n",
    "- Large $K$:\n",
    "    - Low variance\n",
    "    - High bias\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/effectOfK.png\" alt=\"Effect of k\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2aa55c",
   "metadata": {},
   "source": [
    "#### **Distance Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cdd22",
   "metadata": {},
   "source": [
    "**Euclidean Distance**\n",
    "$$d(x, x') = \\sqrt[2]{||x - x'||_{2}^{2}} = \\sqrt[2]{(x_1 - x'_1)^2 + \\ldots + (x_d - x'_d)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6411f",
   "metadata": {},
   "source": [
    "**Distance Learning Methods**\n",
    "\n",
    "- **Weighted Euclidean Distance**\n",
    "    $$d(x, x') = \\sqrt[2]{w_1(x_1 - x'_1)^2 + \\ldots + w_d(x_d - x'_d)^2}$$\n",
    "\n",
    "- **Mahalanobis Distance**\n",
    "    $$d(x, x') = \\sqrt[2]{(x_1 - x'_1)^T A (x_1 - x'_1)}$$\n",
    "    Where:\n",
    "    - The matrix $A$ acts similarly to the inverse covariance matrix\n",
    "    - $A$: $d \\times d$ matrix\n",
    "    - If $A$ is diagonal (uncorrelated features), it reduces to weighted Euclidean distance\n",
    "    - Off-diagonal elements capture feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580aa90",
   "metadata": {},
   "source": [
    "**Minkowski Distance**\n",
    "$$d(x, x') = \\sqrt[p]{(\\sum_{i=1}^n |x_i - x'_i|^p)}$$\n",
    "\n",
    "- For $p \\ge 1$\n",
    "- Minkowski with $p = 2$ is the same as Euclidean distance\n",
    "- Minkowski distance is the same as $L^p$ norm of $(x - x')$\n",
    "\n",
    "**$L^p$ norm:**\n",
    "$$||x||_p = \\sqrt[p]{|x_1|^p + \\ldots + |x_n|^p}$$\n",
    "Some famous ones:\n",
    "$$||x||_1 = \\sum_{i=1}^n |x_i|$$\n",
    "$$||x||_2 = \\sqrt{x_1^2 + \\ldots + x_n^2}$$\n",
    "$$||x||_{\\infty} = \\max{\\{|x_1|, |x_2|, \\ldots, |x_n|\\}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd47e7",
   "metadata": {},
   "source": [
    "**Cosine distance (angle)**\n",
    "$$d(x, x') = 1 - \\text{cosine similarity}(x, x')$$\n",
    "Where:\n",
    "$$\\text{cosine similarity}(x, x') = \\frac{x.x'}{||x||_2 ||x'||_2} = \\frac{\\sum_{i=1}^d x_i x'_i}{\\sqrt{\\sum_{i=1}^d x_i^2} \\sqrt{\\sum_{i=1}^d {x'_i}^2}}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/cosineSimilarity.png\" alt=\"Cosine Similarity\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383517a9",
   "metadata": {},
   "source": [
    "#### **Weighted (Kernel) KNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb022f",
   "metadata": {},
   "source": [
    "Recall: **Weighted Euclidean distance** measure:\n",
    "$$d(x, x') = \\sqrt[2]{w_1(x_1 - x'_1)^2 + \\ldots + w_d(x_d - x'_d)^2}$$\n",
    "\n",
    "Weight **nearer** neighbors more **heavily**:\n",
    "$$\\hat{y} = f(x)= \\argmax_{c = 1, \\ldots, C} \\sum_{j \\in N_k(x)} w_j(x) I(c = y^{(j)})$$\n",
    "\n",
    "Example of weighting function:\n",
    "$$w_j(x) = \\frac{1}{||x - x^{(j)}||^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6f8f0",
   "metadata": {},
   "source": [
    "**Stepard's Method**\n",
    "\n",
    "In the weighted kNN, we can use all training examples instead of just $k$:\n",
    "$$\\hat{y} = f(x)= \\argmax_{c = 1, \\ldots, C} \\sum_{j \\in n} w_j(x) I(c = y^{(j)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780752a",
   "metadata": {},
   "source": [
    "Weights can be found using a **kernel function**\n",
    "$$w_j(x) = K(x, x^{(j)})$$\n",
    "Example:\n",
    "$$K(x, x^{(j)}) = e^{-\\frac{d(x, x^{(j)})}{\\sigma^2}}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/weightingFuncs.png\" alt=\"Weighing Functions\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d9967",
   "metadata": {},
   "source": [
    "### **Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8c718",
   "metadata": {},
   "source": [
    "#### **KNN Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76726a",
   "metadata": {},
   "source": [
    "Let\n",
    "- $\\{x'^{(1)}, \\ldots, x'^{(k)}\\}$: $k$ nearest neighbors of $x$\n",
    "- $\\{y'^{(1)}, \\ldots, y'^{(k)}\\}$ Corresponding labels.\n",
    "\n",
    "Predict $\\hat{y}$ as the average of the neighbors' labels:\n",
    "$$\\hat{y} = \\frac{1}{k} \\sum_{j = 1}^k y'_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbe6cf",
   "metadata": {},
   "source": [
    "**Challenges of kNN Regression**\n",
    "\n",
    "\n",
    "- **Discontinuities in Estimated Function**\n",
    "    - **Problem:**  \n",
    "    The predicted function $\\hat{y}$ is piecewise constant, leading to abrupt jumps.\n",
    "\n",
    "    - **Solution:**  \n",
    "    **Weighted (Kernel) regression**: Assign weights to neighbors based on distance\n",
    "\n",
    "- **Noise Sensitivity**\n",
    "    - **1NN:**  \n",
    "        - Overfit to noise\n",
    "        - High variance\n",
    "        - Adopting the label of the single nearest neighbor, even if it's an outlier.\n",
    "\n",
    "    - **kNN (k $\\gt$ 1):**\n",
    "        - Smooths away noise, but there are other deficiencies.\n",
    "        - Lower variance\n",
    "        - Disadvantage: **Flats the end**\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/kNNRegression.png\" alt=\"kNN Regression examples\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b93119",
   "metadata": {},
   "source": [
    "#### **Weighted kNN Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea1a4a",
   "metadata": {},
   "source": [
    "**Recall:**  \n",
    "Standard kNN regression estimate:\n",
    "$$\\hat{y} = \\frac{1}{k} \\sum_{j = 1}^k y_j$$\n",
    "Where $\\{y'^{(1)}, \\ldots, y'^{(k)}\\}$ are the labels if the $k$ nearest neighbors.\n",
    "\n",
    "**Weighted kNN Regression**\n",
    "\n",
    "Give higher weights to nearer neighbors:\n",
    "$$\\hat{y} = \\frac{\\sum_{j \\in N_k(x)} w_j(x) y^{(j)}}{\\sum_{j \\in N_k(x)} w_j(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8184f",
   "metadata": {},
   "source": [
    "In the weighted kNN regression, we can use all training examples instead of just $k$:\n",
    "$$\\hat{y} = \\frac{\\sum_{j \\in n} w_j(x) y^{(j)}}{\\sum_{j \\in n} w_j(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e929c",
   "metadata": {},
   "source": [
    "Weights can be found using a **kernel function**\n",
    "$$w_j(x) = K(x, x^{(j)})$$\n",
    "\n",
    "**Common Kernel Choices:**\n",
    "- Gaussian Kernel:\n",
    "$$K(x, x^{(j)}) = e^{-\\frac{d(x, x^{(j)})}{\\sigma^2}}$$\n",
    "- Bandwidth $\\sigma$ controls smoothness:\n",
    "    - Small $\\sigma$: More localized to fit (risk of overfitting)\n",
    "    - Large $\\sigma$: Smoother fit (risk of undefitting)\n",
    "    \n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/KernelKNN.png\" alt=\"Gaussian Kernel\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0955d",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "- **Inability to Capture Simple Global Structures**  \n",
    "    kNN fails to identify fundamental patterns in the data (e.g., linear, polynomial, or periodic relationships), even when they are obvious.\n",
    "\n",
    "- **Unreliable Answers at edges**  \n",
    "    Failure to extrapolate at edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc1510",
   "metadata": {},
   "source": [
    "#### **Locally Weighted Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc911c0d",
   "metadata": {},
   "source": [
    "For each test sample:\n",
    "- Fits a **local parametric** linear model to nearby training points\n",
    "\n",
    "**Advantages Over kNN Regression**\n",
    "- Avoids piecewise-constant artifacts of kNN\n",
    "- Converges to true function as $n \\rightarrow \\infty$ (under proper $\\sigma$ scheduling)\n",
    "- Superior edge estimation to kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd3d08",
   "metadata": {},
   "source": [
    "**Locally Unweighted Linear Regression**\n",
    "\n",
    "For a neighborhood $N_k(x)$ of $k$ nearest neighbors:\n",
    "$$\\hat{y} = f(x;w) = w_0 + w_1x_1 + \\ldots + w_dx_d$$\n",
    "\n",
    "Optimization:\n",
    "$$J(w) = \\sum_{i \\in N_k(x)} (y^{(i)} - w^Tx^{(i)})^2$$\n",
    "\n",
    "**Locally Weighted Linear Regression**\n",
    "\n",
    "Introduces kernel weights $K(x,x^{(i)})$ to emphasize nearby points:\n",
    "$$J(w) = \\sum_{i \\in N_k(x)} K(x, x^{(i)}) (y^{(i)} - w^Tx^{(i)})^2$$\n",
    "\n",
    "**Global Kernel Extension**\n",
    "\n",
    "Uses all $n$ training samples with distance-based weighting:\n",
    "$$J(w) = \\sum_{i = 1}^n K(x, x^{(i)}) (y^{(i)} - w^Tx^{(i)})^2$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
