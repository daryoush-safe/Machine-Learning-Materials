{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8600f37a",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d6a19",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62be02",
   "metadata": {},
   "source": [
    "**Given:**  \n",
    "Training Set\n",
    "- A dataset $D$ with $N$ labeled instances $D = \\{x^{(i)}, y^{(i)}\\}_{i=1}^N$\n",
    "- $y^{(i)} \\in {1, 2, ..., K}$\n",
    "\n",
    "**Goal:**  \n",
    "Given an input $x$, assign it to one of $K$ classes.\n",
    "\n",
    "**Examples:**\n",
    "- Email Spam Detection\n",
    "- Medical Diagnosis\n",
    "- Handwritten digit recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d62065",
   "metadata": {},
   "source": [
    "**Decision Boundary:**\n",
    "\n",
    "**Definition:** A dividing hyperplane that separates different classes in a feature space, also known as **Decision Surface**.\n",
    "\n",
    "In a $d$-dimensional feature space, the decision boundary for a linear classifier is a hyper plane of dimension $d-1$.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/decisionBoundary.png\" alt=\"Decision Boundary Examples\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e12b9e",
   "metadata": {},
   "source": [
    "**Regression vs Classification:**\n",
    "\n",
    "**Linear Regression:**\n",
    "- Target: Continuous values (real numbers)\n",
    "\n",
    "**Linear Classifier:**\n",
    "- Target: Binary or multi-category labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45815a",
   "metadata": {},
   "source": [
    "## Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525d1ad",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6959683",
   "metadata": {},
   "source": [
    "**Definition:** A discriminant function $f_i(x)$ that assigns a score to an input vector $x$, for each class $C_i(i=1, ..., K)$\n",
    "\n",
    "**How it works:**\n",
    "* **Binary Classification:**\n",
    "    - Two functions $f_1(x)$ and $f_2(x)$ for classes $C_1$ and $C_2$. The class is predicted by comparing these two functions:\n",
    "    $$\n",
    "    \\hat{y} = \n",
    "    \\begin{cases} \n",
    "    C_1 & \\text{if } f_1(x) \\gt f_2(x) \\\\\n",
    "    C_2 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    - Decision boundary: $f(x) = 0$\n",
    "    - For binary classification we can only find a function $f: R^d \\rightarrow R$ where:\n",
    "        - $f_1(x) = f(x)$\n",
    "        - $f_2(x) = -f(x)$\n",
    "<br><br>\n",
    "* **General Case:**\n",
    "    - For $k$-class problems, we compute $f_i(x)$ for every class $i$, and assign $x$ to class with highest score:\n",
    "    $$\\hat{y} = \\argmax_i f_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24842403",
   "metadata": {},
   "source": [
    "### **Linear Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd10634",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6a01d",
   "metadata": {},
   "source": [
    "##### **Core Concepts (definitions, limitations, decision surface)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586564",
   "metadata": {},
   "source": [
    "**Definition:** Decision boundaries are linear in $x$, or linear in some given set of functions of $x$ ($\\phi(x)$).\n",
    "\n",
    "**Linearly separable data:** data points that can be exactly classified by a linear decision surface.\n",
    "\n",
    "**Why Linear Classifier:** *(even they are not optimal)*\n",
    "- Simplicity\n",
    "- Easy to compute\n",
    "- Efficiency\n",
    "- Attractive candidates for initial, trial classifiers\n",
    "- Effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190164b",
   "metadata": {},
   "source": [
    "**Two Category Classification**\n",
    "$$f(x;w) = w^Tx + w_0 = w_0 + w_1x_1 + ... + w_dx_d$$\n",
    "Where:\n",
    "- $x = [x_1, x_2, ..., x_d]$\n",
    "- $w = [w_1, w_2, ..., w_d]$\n",
    "- $w_0$: Bias\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases} \n",
    "C_1 & \\text{if } w^Tx + w_0 \\ge 0 \\\\\n",
    "C_2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Decision Boundary(Surface):** \n",
    "$$w^Tx + w_0 = 0$$\n",
    "Decision Boundary is a $(dâˆ’1)$-dimensional hyperplane in $d$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6e52c",
   "metadata": {},
   "source": [
    "##### **$w$ is orthogonal to every vector lying within the decision surface**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b031a31",
   "metadata": {},
   "source": [
    "Decision Surface:\n",
    "$$w^Tx + w_0 = 0$$\n",
    "For any two points $x_1$ and $x_2$ on the decision surface:\n",
    "$$w^Tx_1 + w_0 = 0 \\quad \\text{and} \\quad w^Tx_2 + w_0 = 0$$\n",
    "Subtracting these equations:\n",
    "$$w^T(x_1 - x_2)  = 0$$\n",
    "Which:\n",
    "$x_1 - x_2$: A vector parallel to the decision surface.\n",
    "\n",
    "**Conclusion:**\n",
    "- $w$ is *orthogonal* to every vector lying within the decision surface\n",
    "- $w$ acts as the *normal vector* to the decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad4c5c",
   "metadata": {},
   "source": [
    "##### **Signed measure of the perpendicular distance $r$ of the point $x$ from the decision surface:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5c106",
   "metadata": {},
   "source": [
    "- Decision surface $H$ is determined by the normal vector $w = [w_1, w_2, ..., w_d]$: $H = w^Tx + w_0$\n",
    "- $w_0$ determine the location of the surface\n",
    "- The normal distance from the origin to the decision surface is $\\frac{w_0}{||w||}$\n",
    "\n",
    "$$x = x_\\perp + r\\frac{w}{||w||}$$\n",
    "Where, $r$ is the scalar distance (signed).\n",
    "\n",
    "Evaluate $H$ at $x$:\n",
    "$$w^Tx + w_0 = w^T(x_\\perp + r\\frac{w}{||w||}) + w_0$$\n",
    "$$ = (w^Tx_\\perp + w_0) + r\\frac{w^Tw}{||w||}$$\n",
    "\n",
    "Since $x_\\perp \\in H$ then $w^Tx_\\perp + w_0 = 0$. So:\n",
    "$$ = r\\frac{||w||^2}{||w||} = r||w||$$\n",
    "\n",
    "As a result:\n",
    "$$w^Tx + w_0 = r||w||$$\n",
    "\n",
    "Solve for r:\n",
    "$$r = \\frac{w^Tx + w_0}{||w||}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/LinearClassifierDistance.png\" alt=\"Linear Classifier Distance\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb7855",
   "metadata": {},
   "source": [
    "Linear boundary geometry POV summary:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/LinearClassifierSummary.png\" alt=\"Linear boundary geometry POV summary\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652b986",
   "metadata": {},
   "source": [
    "##### **Non-Linear Decision Boundary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95aead",
   "metadata": {},
   "source": [
    "- **Feature Transformation ($\\phi(x)$):** Non-linearity is introduced by transforming features into a higher dimensional space.\n",
    "\n",
    "- **Linear in Transformed Space:** The decision boundary becomes linear in the new space, but non-linear in the original space.\n",
    "\n",
    "Same as what we did in *Polynomial Regression* problem.\n",
    "\n",
    "- Example:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/non-linearDecisionBoundary.png\" alt=\"Non-Linear Decision Boundary Example\">\n",
    "</div>\n",
    "\n",
    "$$x_1^2 + x_2^2 = 1$$\n",
    "$$\\phi(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1x_2]$$\n",
    "$$w = [w_0, w_1, ..., w_6] = [-1, 0, 0, 1, 1, 0]$$\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } w^T\\phi(x) \\ge 0 \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f82a6",
   "metadata": {},
   "source": [
    "#### **Cost function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1332a",
   "metadata": {},
   "source": [
    "##### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3fe11f",
   "metadata": {},
   "source": [
    "Finding linear classifier can be formulated as *optimization problem*:\n",
    "\n",
    "**Given:**\n",
    "- Training set $D = \\{x^{(i)},y^{(i)}\\}$\n",
    "- Cost function $J(w)$\n",
    "\n",
    "**Find:**\n",
    "- Optimal $\\hat{f}(x) = f(x;\\hat{w})$ where:\n",
    "$$\\hat{w} = \\argmin_w J(w)$$\n",
    "\n",
    "Unlike Regression problem, we will investigate several cost functions for Classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d639da8",
   "metadata": {},
   "source": [
    "##### **Sum of Squared Error (SSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b97fa",
   "metadata": {},
   "source": [
    "**Formula:**\n",
    "$$J(w) = \\sum_{i=1}^n(w^Tx^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "**Limitations:**\n",
    "- If the model predicts close to the true class but not exactly 0 or 1, SSE still shows positive error, even for correct predicts\n",
    "- SSE penalizes too correct predictions (ones which lie a long way on the correct side of the decision)\n",
    "- Lack of robustness to noise. Small variations can cause significant changes in the cost\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/SSECostFuncForClassification.png\" alt=\"SSE cost function example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c516771",
   "metadata": {},
   "source": [
    "##### **Alternative for SSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588fffe",
   "metadata": {},
   "source": [
    "**Signed Activation Function**\n",
    "\n",
    "make my notes better and prettier\n",
    "**Definition:** Measures how many samples are misclassified by the model, penalizing each error by 4 units\n",
    "\n",
    "**Formula:**\n",
    "$$J(w) = \\sum_{i=1}^n(\\text{sign}(w^Tx^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Sign Function:\n",
    "$$\n",
    "\\text{sign}(z) = \n",
    "\\begin{cases} \n",
    "1 & z \\lt 0 \\\\\n",
    "-1 & z \\ge 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "| **True Label (y)** | **Prediction** | **Calculation**               | **Cost (J)**|  \n",
    "|--------------------|----------------|-------------------------------|-------------|  \n",
    "|     $+1$           |     $+1$       |   $(+1 - (+1))^2 = 0$         | 0           |  \n",
    "|     $-1$           |     $-1$       |   $(-1 - (-1))^2 = 0$         | 0           |  \n",
    "|     $-1$           |     $+1$       |   $(-1 - (+1))^2 = 4$         | 4           |  \n",
    "|     $+1$           |     $-1$       |   $(+1 - (-1))^2 = 4$         | 4           |  \n",
    "\n",
    "\n",
    "**Limitations:**\n",
    "- **Non-Differentiable:**\n",
    "    - The $\\text{sign}(z)$ function has discontinuities at $z=0$, making gradient-based optimization impossible.\n",
    "- **Flat Gradients:**\n",
    "    - The cost landscape has large flat regions.\n",
    "- **Coarse Error Sensitivity:**\n",
    "    - All misclassifications are penalized equally (4 units), regardless of how *close* the prediction was to the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4ce36",
   "metadata": {},
   "source": [
    "**Sigmoid Activation Function**\n",
    "\n",
    "- **Far from decision boundary**: Output approaches **1** (high confidence).\n",
    "- **Close to decision boundary**: Output â‰ˆ **0.5** (uncertain classification).\n",
    "- Solves the **non-differentiable** problem of the sign function by providing smooth gradients.\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{(Output range: 0 to 1)}\n",
    "$$\n",
    "\n",
    "**Limitations:**\n",
    "- **Non-Convex Cost Function:**\n",
    "    - May converge to local minima during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6083bd6",
   "metadata": {},
   "source": [
    "##### **Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988bbf8",
   "metadata": {},
   "source": [
    "- **Core concept:**  \n",
    "The perceptron criterion focuses on misclassified points.\n",
    "\n",
    "- **Formula:**\n",
    "    $$J_p(w) = -\\sum_{i \\in M}y^{(i)}w^Tx^{(i)}$$\n",
    "    Where:\n",
    "    - $y^{(i)} \\in \\{-1,+1\\}$\n",
    "    - $M$: Subset of training data what are misclassified\n",
    "    - $w^T x^{(i)}$: Signed distance from decision boundary\n",
    "\n",
    "- **Decompose $y^{(i)}w^Tx^{(i)}$ term:**\n",
    "    - $y^{(i)}$ Set the sign (direction) of the perceptron.\n",
    "    - $w^T x^{(i)}$: Signed distance from the decision boundary. It causes points farther from the boundary to be penalized more.\n",
    "\n",
    "- **Classification Cases:**\n",
    "    - **Correct Prediction:**\n",
    "        - *Ignored in the sum (since $i \\notin M$)*\n",
    "    - **Incorrect Prediction** ($\\text{sign}(w^T x^{(i)}) \\neq y^{(i)}$)\n",
    "        - $ y^{(i)} w^T x^{(i)} < 0 $ â†’ **Positive penalty** in $ J_p(w) $  \n",
    "        - Penalty scales with distance from boundary (farther = worse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294386a",
   "metadata": {},
   "source": [
    "#### **Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ef548",
   "metadata": {},
   "source": [
    "##### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd936db",
   "metadata": {},
   "source": [
    "**Perceptron Unit:**\n",
    "- **Basic Building Block:**\n",
    "  - A perceptron is the simplest type of artificial neuron used in machine learning.\n",
    "- **Linear Classifier:**\n",
    "  - It maps input features to an output by applying a linear combination and a threshold.\n",
    "- **Binary Decision:**\n",
    "  - Outputs 1 if the weighted sum of inputs exceeds the threshold.\n",
    "- **Components:**\n",
    "  - Inputs, weights, bias & an activation function (step or sigmoid function)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/perceptronUnit.png\" alt=\"Perceptron Unit\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc264c2",
   "metadata": {},
   "source": [
    "**Inspired By Neurons:** Perceptron mimics the basic function of biological neurons in the brain\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/biologicalNeuron.png\" alt=\"Biological Motivation Behind Perceptron\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62ca51",
   "metadata": {},
   "source": [
    "**Single Neuron as a Linear Decision Boundary**\n",
    "\n",
    "The output of a single neuron is:\n",
    "$$y = f(w^Tx+w_0)$$\n",
    "Where:\n",
    "- $x$: Input vector\n",
    "- $w$: weight vector\n",
    "- $w_0$: Bias term\n",
    "- $f$: Activation function (e.g: step, sigmoid)\n",
    "\n",
    "**Linear Separation:** A neuron defines a linear decision boundary:\n",
    "$$w^Tx+w_0 = threshold \\text{ (0 for step, 0.5 for sigmoid)}$$\n",
    "\n",
    "**Decision Rule:**\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "C_1 & \\text{if } w^Tx+w \\ge threshold \\\\\n",
    "C_2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a77153",
   "metadata": {},
   "source": [
    "**Limitations of a Single Perceptron**\n",
    "- **Performs Linear Separation:**\n",
    "    - A single perceptron can handle linearly separable problems such as:\n",
    "        - AND operation\n",
    "        - OR operation\n",
    "\n",
    "- **Fails on Non-Linear Problems:**\n",
    "    - A single perceptron fails to solve non-linear problems like XOR\n",
    "    - Non-Linear problem: Data points cannot be separated by a straight line.\n",
    "    - Handle by using *Multi-Layer Perceptron* (MLP)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/XORProblem.png\" alt=\"XOR Problem\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d91c04",
   "metadata": {},
   "source": [
    "**Multi-Layer Perceptron**\n",
    "\n",
    "- **Adding Layers for More Complexity:**\n",
    "    - MLP consists of multiple layers of neurons that allow us to model more complex functions\n",
    "    - Each layer has new decision boundaries, making possible to separate non-linear data\n",
    "\n",
    "- **Two-Layer Example**\n",
    "    - Input Layer $\\rightarrow$ Hidden Layer $\\rightarrow$ Output Layer\n",
    "    - Hidden layers introduce non-linear transformations through activation functions, enabling the network to model complex decision boundaries.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/MLP.png\" alt=\"2-Layer Perceptron\">\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3d7ba",
   "metadata": {},
   "source": [
    "##### **Perceptron Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ce02",
   "metadata": {},
   "source": [
    "- Binary Classification:\n",
    "    $$y \\in \\{-1, 1\\}$$\n",
    "- Goal:\n",
    "    $$\\forall_i, x^{(i)} \\in C_1 \\rightarrow w^Tx^{(i)} \\gt 0$$\n",
    "    $$\\forall_i, x^{(i)} \\in C_2 \\rightarrow w^Tx^{(i)} \\lt 0$$\n",
    "- Activation function:\n",
    "    $$f(x;w) = \\text{sign}(w^Tx)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613effa",
   "metadata": {},
   "source": [
    "**Perceptron criterion**\n",
    "$$J_p(w) = - \\sum_{i \\in M} w^Tx^{(i)}y^{(i)}$$\n",
    "Where:\n",
    "- $M$: Subset of training data that are misclassified\n",
    "\n",
    "*Which is discussed linear classifier cost function section.*\n",
    "\n",
    "**Goal:** Minimize the loss by correctly classified all points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c2ff",
   "metadata": {},
   "source": [
    "##### **Batch Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37836472",
   "metadata": {},
   "source": [
    "**Definition:** Updates the weight vector using all misclassified points in each iteration.\n",
    "\n",
    "**Gradient Descent:** Adjusting weights in the direction that reduces the loss:\n",
    "    $$w^t+1 = w^t - \\eta \\nabla_w J_p(w^t)$$\n",
    "    $$\\nabla_w J_p(w) = - \\sum_{i \\in M} x^{(i)}y^{(i)}$$\n",
    "\n",
    "Batch Perceptron converges in finite number of steps for linearly separable data.\n",
    "\n",
    "> Initialize $w$  \n",
    "> Repeat:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$w = w + \\eta \\sum_{i \\in M} x^{(i)}y^{(i)}$  \n",
    "> Until:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$\\eta \\sum_{i \\in M} x^{(i)}y^{(i)} < \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c558810a",
   "metadata": {},
   "source": [
    "##### **Single-Sample Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154e245",
   "metadata": {},
   "source": [
    "**Definition:** Updates the weight vector after each individual point.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):**\n",
    "$$J_p(w) = \\sum_{k=1}^KJ_p^{(k)}(w)$$\n",
    "- Using only one misclassified sample at a time:\n",
    "    $$w^t+1 = w^t + \\eta x^{(i)}y^{(i)}$$\n",
    "- Lower computational cost per iteration, maybe faster convergence.\n",
    "- If we predicted wrong:\n",
    "$$\n",
    "w^{t+1} = \n",
    "\\begin{cases} \n",
    "w^t + \\eta x^{(i)} & \\text{if } y^{(i)} \\gt 0 \\\\\n",
    "w^t - \\eta x^{(i)} & \\text{if } y^{(i)} \\lt 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "If training data are linearly separable, the single-sample perceptron is also guaranteed to find a solution in a finite number of steps.\n",
    "\n",
    "> Initialize $w$, $t \\leftarrow 0$  \n",
    "> Repeat:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$t \\leftarrow t + 1$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$i \\leftarrow t \\bmod N$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;if $x^{(i)}$ is misclassified then:  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w = w + \\eta x^{(i)} y^{(i)}$  \n",
    "> Until all patterns are properly classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c95e0e",
   "metadata": {},
   "source": [
    "##### **Pocket Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5374e99",
   "metadata": {},
   "source": [
    "**Limitations:**\n",
    "- The Perceptron stops learning as soon as all training points are correctly classified, even if the decision boundary is suboptimal.\n",
    "- When no linear decision boundary can perfectly separate the classes, the Perceptron fails to converge.\n",
    "\n",
    "For the data that are not linearly separable due to noise:\n",
    "Keeps in its pocket the best ð’˜ encountered up to now.\n",
    "\n",
    "> Initialize w  \n",
    "> for $t = 1, ..., T$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$i \\leftarrow t \\text{mod} N$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;if $x^{(i)}$ is misclassified then  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w^{new} = w + x^{(i)}y^{(i)}$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $E_{train}(w^{new}) \\lt E_{train}(w)$ then  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w = w^{new}  \n",
    "> end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4d4f5",
   "metadata": {},
   "source": [
    "### **Linear Discriminant Algorithm (LDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff3a87",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947207d1",
   "metadata": {},
   "source": [
    "Fisher's Linear Discriminant Analysis \n",
    "\n",
    "LDA, like the Perceptron algorithm, seeks a line (or hyperplane) to separate data, but its approach is different.\n",
    "\n",
    "**How it works?**\n",
    "- Predicts the class of an observation $x$ by first projecting it to the space of discriminant variables and then classifying it in this space\n",
    "- Predicts the class of an observation $x$ by first **projecting it** to the space of new discriminant variables and then classifying it in this space\n",
    "\n",
    "**Goal:**  After projection, the mapped data should be as separable as possible (unlike the Perceptron, which only focuses on correct classification).\n",
    "\n",
    "**Dimensionality Reduction:** LDA can also reduce dimensions by creating a new feature (linear combination of original features) that best preserves class discrimination.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/LDA.png\" alt=\"LDA example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb62db79",
   "metadata": {},
   "source": [
    "**LDA Problem Definition**\n",
    "\n",
    "**Problem:**\n",
    "- $C = 2$ classes\n",
    "- $\\{(x^{i}, y^{(i)})\\}_{i=1}^N$ training samples with $N_1$ samples from the first class ($C_1$)\n",
    "and $N_2$ samples from the second class ($C_2$)\n",
    "\n",
    "**Goal:**\n",
    "- finding the best direction $w$ that we hope to enable accurate classification\n",
    "\n",
    "**Tip:**\n",
    "- The projection of sample $x$ onto a line in direction $w$ is $w^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da2f3f",
   "metadata": {},
   "source": [
    "#### **Measure of Separation in the Projected Direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295f267",
   "metadata": {},
   "source": [
    "##### **large separation between the projected class means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f856c7",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "$$\\hat{w} = \\max_w J(w) = (\\mu_1' - \\mu_2')^2$$\n",
    "$$\\text{s.t.} ||w|| = 1$$\n",
    "\n",
    "Where:\n",
    "$$\\mu_1 = \\frac{\\sum_x^{(i) \\in C_1}}{N_1}, \\quad \\mu_1' = w^T\\mu_1$$\n",
    "$$\\mu_2 = \\frac{\\sum_x^{(i) \\in C_2}}{N_2}, \\quad \\mu_2' = w^T\\mu_2$$\n",
    "\n",
    "**Problem:** It does not consider the variances of the classes in the projected direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4bea7",
   "metadata": {},
   "source": [
    "##### **LDA Criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df8759",
   "metadata": {},
   "source": [
    "**Fisher Idea:**\n",
    "Find a projection direction $w$ that maximizes class separability by:\n",
    "- **Maximizing the distance** between the projected means of the two classes *(between-class separation)*.\n",
    "- **Minimizing the variance** (scatter) within each class after projection *(within-class compactness)*.\n",
    "\n",
    "**Formula:**\n",
    "$$J(w) = \\frac{|\\mu'_1 - \\mu'_2|^2}{{s'_1}^2 + {s'_2}^2}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu'_1, \\mu'_2$: Projected means of classes 1 and 2 onto direction w.\n",
    "- ${s'_1}^2, {s'_2}^2$: Scatter (variance) of projected data within each class. *(Scatter matrices)*\n",
    "\n",
    "Comparison between large separation between means *(left pic)* and LDA criteria *(right pic)*:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/LDACriteria.png\" alt=\"Comparison image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d015751",
   "metadata": {},
   "source": [
    "**Scatter Matrix**\n",
    "\n",
    "Measures how tightly data points are clustered around their class mean.  \n",
    "Scatter Matrix is a better choice rather than variance because it is sensitive to the number of samples per class.\n",
    "\n",
    "The scatters of the original data:\n",
    "$$s_1^2 = \\sum_{x^{(i)} \\in C_1} ||x^{(i)} - \\mu_1||^2$$\n",
    "$$s_2^2 = \\sum_{x^{(i)} \\in C_2} ||x^{(i)} - \\mu_2||^2$$\n",
    "\n",
    "The scatters of projected data:\n",
    "$${s'_1}^2 = \\sum_{x^{(i)} \\in C_1} (w^Tx^{(i)} - w^T\\mu_1)^2$$\n",
    "$${s'_2}^2 = \\sum_{x^{(i)} \\in C_2} (w^Tx^{(i)} - w^T\\mu_2)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2b1d5",
   "metadata": {},
   "source": [
    "- **Objective Function (Fisher's Criterion):**  \n",
    "    Maximize the ratio of between-class separation to within-class scatter:\n",
    "    $$J(w) = \\frac{|\\mu'_1 - \\mu'_2|^2}{{s'_1}^2 + {s'_2}^2}$$\n",
    "    Where:\n",
    "    - $\\mu_k'$: projected mean of class $k$\n",
    "    - $s_k'^2$: scatter of projected class $k$\n",
    "\n",
    "- **Key Components**  \n",
    "    - Between-Class Separation:\n",
    "        $$|\\mu_1' - \\mu_2'|^2 = |w^T\\mu_1 - w^T\\mu_2|^2$$\n",
    "        $$ = w^T(\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^Tw$$\n",
    "        \n",
    "        - Between-class scatter matrix:\n",
    "            $$S_B = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^T$$\n",
    "\n",
    "    - Within-Class Scatter\n",
    "        $${s'_1}^2 = \\sum_{x^{(i)} \\in C_1}(w^Tx^{(i)} - \\mu_1)^2 = w^T\\left(\\sum_{x^{(i)} \\in C_1} \\left(x^{(i)} - \\mu_1\\right) \\left(x^{(i)} - \\mu_1 \\right)^T \\right)w$$\n",
    "\n",
    "        $${s'_2}^2 = \\sum_{x^{(i)} \\in C_2}(w^Tx^{(i)} - \\mu_2)^2 = w^T\\left(\\sum_{x^{(i)} \\in C_2} \\left(x^{(i)} - \\mu_2\\right) \\left(x^{(i)} - \\mu_2 \\right)^T \\right)w$$\n",
    "\n",
    "        $$S_1 = \\sum_{x^{(i)} \\in C_1} {(x^{(i)} - \\mu_1)(x^{(i)} - \\mu_1)^T}$$\n",
    "        $$S_2 = \\sum_{x^{(i)} \\in C_2} {(x^{(i)} - \\mu_2)(x^{(i)} - \\mu_2)^T}$$\n",
    "\n",
    "        - Since ${s'_1}^2 = w^T s_1 w$ and ${s'_2}^2 = w^T s_2 w$ then:\n",
    "            $${s'_1}^2 + {s'_2}^2 = w^T(s_1 + s_2)w$$\n",
    "            $$S_W = S_1 + S_2$$\n",
    "\n",
    "- **Generalized Objective**  \n",
    "    Rewrite $J(w)$ using $S_B$ and $S_W$:\n",
    "    $$J(w) = \\frac{w^TS_Bw}{w^TS_Ww}$$\n",
    "\n",
    "- **Optimization**  \n",
    "    Since:  \n",
    "    $$\\frac{\\partial}{\\partial w} w^TAw = 2Aw$$\n",
    "    Take the derivative and set it to zero:  \n",
    "    $$\\frac{\\partial J(w)}{\\partial w} = \\frac{\\frac{\\partial w^TS_Bw}{\\partial w}w^TS_Ww - \\frac{\\partial w^TS_Ww}{\\partial w}w^TS_Bw}{(w^TS_Ww)^2} = \\frac{(2S_Bw)w^TS_Ww - (2S_Ww)w^TS_Bw}{(w^TS_Ww)^2} = 0$$\n",
    "    So:  \n",
    "    $$(2S_Bw)w^TS_Ww - (2S_Ww)w^TS_Bw = 0$$\n",
    "    $$(S_Bw)w^TS_Ww = (S_Ww)w^TS_Bw$$\n",
    "    Let $w^TS_Ww = \\beta$ and $w^TS_Bw = \\alpha$ (scalars):  \n",
    "    $$S_Bw = \\frac{\\alpha}{\\beta}S_Ww$$\n",
    "    Define $\\frac{\\alpha}{\\beta} = \\lambda$:  \n",
    "    $$S_Bw = \\lambda S_Ww$$\n",
    "\n",
    "- **Eigenvalue Problem:**  \n",
    "    *The core relationship is $Av=\\lambda v$, where A is the matrix, $v$ is the eigenvector, and $\\lambda$ is the (scalar) eigenvalue.*  \n",
    "    If $S_W$ is invertible (full-rank):\n",
    "    $$S_W^{-1}S_Bw = \\lambda w$$\n",
    "\n",
    "    $S_Bw \\propto (\\mu_1 - \\mu_2)$ (since $S_B = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^T$).  \n",
    "    Thus, the optimal $w$ is:\n",
    "    $$w \\propto S_W^{-1}(\\mu_1 - \\mu_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1580e2d",
   "metadata": {},
   "source": [
    "##### **LDA Algorithm Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69454653",
   "metadata": {},
   "source": [
    "- Find $\\mu_1$ and $\\mu_2$ as the mean of class 1 and 2\n",
    "- Find $S_1$ and $S_2$ as scatter matrix of class 1 and 2\n",
    "- $S_W = S_1 + S_2$\n",
    "- $S_B = (\\mu_1 - \\mu_2)(\\mu_1 - \\mu_2)^T$\n",
    "- **Feature Extraction:** $w = S_W^{-1}(\\mu_1 - \\mu_2)$\n",
    "- **Classification:** Using a threshold on $w^Tx$, we can classify $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92036631",
   "metadata": {},
   "source": [
    "### **Multi-Category Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f184113",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b74a29",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "- Solutions to multi-category problems\n",
    "\n",
    "**How to solve:**\n",
    "- Extend the learning algorithm to support multi-class:\n",
    "    - A function $f_i(x) for each class $C_i$ us found$\n",
    "    - $x$ is assigned to $C_i$ if $f_i(x) \\gt f_j(x) \\quad \\forall j \\neq i$\n",
    "    $$\\hat{y} = \\argmax_{i=1, 2, ..., C} f_i(x)$$\n",
    "\n",
    "- Converting the problem to a set of two-class problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a29c54",
   "metadata": {},
   "source": [
    "#### **Converting the problem to a set of two-class problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3ea4a",
   "metadata": {},
   "source": [
    "##### **One-vs-Rest** or **One Against All**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06078b5b",
   "metadata": {},
   "source": [
    "For each class $C_i$, a linear discriminant function that separates samples of $C_i$ from all the other samples is found.\n",
    "- Totally linearly separable\n",
    "\n",
    "**Decision Making:**  \n",
    "Decision process for a new input works as follows:\n",
    "- If the new data point clearly lies on the positive side of **only one classifier**, it is **unambiguously** assigned to that class.\n",
    "- In ambiguous cases, the data point might lie on the positive side of multiple classifiers.  \n",
    "In such situations, the algorithm compares the distance of the point from each decision boundary.  \n",
    "For example, using the signed distance in the perceptron model:  \n",
    "$$f_i(x) = w_i^Tx$$\n",
    "\n",
    "**Limitations:**\n",
    "- Even if the overall classes are linearly separable from each other, there can be no linear boundary that separates one specific class against all the rest. (we will fix this issue using next algorithm)\n",
    "- Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec505e",
   "metadata": {},
   "source": [
    "##### **One-vs-One**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502acd5",
   "metadata": {},
   "source": [
    "$\\binom{C}{2} = \\frac{C(C-1)}{2}$ linear discriminant functions are used, one to separate samples of a pair of classes.\n",
    "- Pairwise linearly separable\n",
    "\n",
    "**Decision Function:**\n",
    "For each classifier that separates class $i$ from class $j$, we define a function:\n",
    "$$f_{ij}(x) = w_{ij}^Tx$$\n",
    "This function returns the signed distance of the input $x$ from the decision boundary between classes $i$ and $j$.\n",
    "\n",
    "**Decision Making:**\n",
    "- First method:\n",
    "    - Compute all $f_{ij}(x)$ for every class pair.\n",
    "    - For each pair $(i, j)$if $f_{ij}(x) \\gt 0$ we count a vote for class $i$, otherwise for class $j$.\n",
    "    - The class with the highest number of votes is chosen as the predicted class for $x$.\n",
    "\n",
    "    **Limitation:**  If two classes receive the same number of votes.\n",
    "\n",
    "- Second method:\n",
    "    - We resolve the previous ambiguity using the following approach:\n",
    "    $$C_i = \\argmax_i \\sum_{j} f_{i,j}(x)$$\n",
    "    - Using this approach helps ypu to select the class with largest sum.\n",
    "    - **Limitation:** This method compares independently trained classifiers, which might not be consistent with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57afd",
   "metadata": {},
   "source": [
    "##### **Ambiguity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7eae92",
   "metadata": {},
   "source": [
    "Converting the multi-class problem to a set of two-class problems can lead to **regions in which the classification is undefined**:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/LDAAmbiguity.png\" alt=\"Ambiguity\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142045d",
   "metadata": {},
   "source": [
    "#### **Linear Machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea77a5",
   "metadata": {},
   "source": [
    "##### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22535f2",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "- Alternative to *One-vs-Rest* and *One-vs-One* methods; each class is represented by its own discriminant function $f_i(x) = w_i^Tx + w_0$ for each class $C_i (i=1, 2, ..., K)$\n",
    "\n",
    "**Decision Rule:**  \n",
    "$x$ is assigned to class $C_i$ if:\n",
    "$$f_i(x) \\gt f_j(x) \\quad \\forall j \\neq i$$\n",
    "Or:\n",
    "$$\\hat{y} = \\argmax_{i=1, 2, .., c} f_i(x)$$\n",
    "\n",
    "**Decision Surfaces (Boundaries):**  \n",
    "Boundary of the region $i$ and $j$ is:\n",
    "$$\\forall x, f_i(x) = f_j(x)$$\n",
    "$$(w_i - w_j)^Tx + (w_{0i} - w_{0j}) = 0$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/LinearMachine.png\" alt=\"Linear Machine example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b15dee",
   "metadata": {},
   "source": [
    "##### **Perceptron Multi-Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4197149",
   "metadata": {},
   "source": [
    "Maintain a weight matrix $w \\in R^{mK}$, where $m$ is the *number of features* and $K$ is the *number of classes*.  \n",
    "Each column $w_k$ of the matrix corresponds to the weight vector for class $k$.\n",
    "$$\\hat{y} = \\argmax_{i=1, 2, ..., c} w_i^Tx$$\n",
    "$$J_p(w) = -\\sum_{i \\in M} (w_{y^{(i)}} - w_{\\hat{y}^{(j)}})^Tx^{(i)}$$\n",
    "Where:\n",
    "- $M$: Subset of training data that are misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a0721",
   "metadata": {},
   "source": [
    "> Initialize $w = [w_1, ..., w_c]$  \n",
    "> Repeat  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;$k \\leftarrow (k+1) \\text{ mod } N$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;if x^{(i)} is misclassified then  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_{\\hat{y}^{(i)}} - \\eta x^{(i)}$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$w_{y^{(i)}} + \\eta x^{(i)}$  \n",
    "> Until all patterns properly classified.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6ffb2",
   "metadata": {},
   "source": [
    "## Probabilistic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6abbead",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda8dae",
   "metadata": {},
   "source": [
    "- **Class Prior:**  \n",
    "    $p(y=C_k) = \\int p(x,y) dy$  \n",
    "    Percentage of samples belonging to class $C_k$\n",
    "    \n",
    "- **Likelihood:**  \n",
    "    $p(x|C_k)$  \n",
    "    Probability density function (PDF) of feature $x$ for class $C_k$\n",
    "\n",
    "- **Prior Probability:**  \n",
    "    $p(C_k)$  \n",
    "    Probability that a randomly selected sample belongs to $C_k$\n",
    "\n",
    "- **Posterior Probability:**  \n",
    "    $p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{p(x)}$  \n",
    "    Probability of class $C_k$ given prior knowledge.\n",
    "\n",
    "- **Evidence:**  \n",
    "    $p(x) = \\sum_{k=1}^K p(x|C_k)p(C_k)$  \n",
    "    PDF of feature vector $x$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10efed",
   "metadata": {},
   "source": [
    "#### **Bayesian Decision Rule**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe4153",
   "metadata": {},
   "source": [
    "**Optimal Classifier:** Choose the class with the **maximum posterior probability**\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "C_1 &  \\text{if } p(C_1 | x) > p(C_2 | x) \\\\\n",
    "C_2 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Classification Error:**\n",
    "$$\n",
    "p(error | x) = \n",
    "\\begin{cases} \n",
    "p(C_2 | x) & \\text{if we decide } C_1 \\\\\n",
    "p(C_1 | x) & \\text{if we decide } C_2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Minimizing error at each point $x$:*\n",
    "$$p(error | x) = min\\{p(C_1 | x), p(C_2 | x)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6af35",
   "metadata": {},
   "source": [
    "The Bayesian decision rule can be simplified:\n",
    "\n",
    "Posterior Comparison:\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "C_1 &  \\text{if } p(C_1 | x) \\gt p(C_2 | x) \\\\\n",
    "C_2 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Bayesian theorem on Posterior Comparison:\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "C_1 &  \\text{if } \\frac{p(x|C_1)p(C_1)}{p(x)} \\gt \\frac{p(x|C_2)p(C_2)}{p(x)} \\\\\n",
    "C_2 & otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Likelihood-Prior Product\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "C_1 &  \\text{if } p(x|C_1)p(C_1) \\gt p(x|C_2)p(C_2) \\\\\n",
    "C_2 & otherwise\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8432d0",
   "metadata": {},
   "source": [
    "#### **Minimizing Misclassification Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d0322",
   "metadata": {},
   "source": [
    "Assume:\n",
    "- **Decision Function:**  \n",
    "    $\\alpha(x)$: Outputs class label $k$ for each $x$\n",
    "\n",
    "- **Decision Regions:**  \n",
    "    $R_k = \\{x|\\alpha(x) = k\\}$  \n",
    "    All $x$ in $R_k$ is assigned to class $C_k$\n",
    "\n",
    "**Total Probability Error:**\n",
    "$$p(error) = E_{x,y}[I(\\alpha(x) \\neq y)]$$\n",
    "$$ = p(x \\in R_1, C_2) + p(x \\in R_2, C_1)$$\n",
    "$$ = \\int_{R_1} p(x, C_2)dx + \\int_{R_2} p(x, C_1)dx$$\n",
    "$$ = \\int_{R_1}p(C_2|x)p(x)dx + \\int_{R_2}p(C_1|x)p(x)dx$$\n",
    "\n",
    "**Optimal Decision Regions:**  \n",
    "To minimize $p(error)$, assign $x$ to the class with the highest posterior:\n",
    "$$\\alpha(x) = \\argmax_k p(C_k|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707ae10",
   "metadata": {},
   "source": [
    "**Bayes Minimum Error Classifier:**\n",
    "\n",
    "Objective:\n",
    "$$min_{\\alpha}\\{E_{x,y}[I(\\alpha(x) \\neq y)]\\}$$\n",
    "\n",
    "Solution (if true probabilities are known):\n",
    "$$\\alpha(x) = \\argmax_y p(y|x)$$\n",
    "\n",
    "*In practice we can estimate $p(y|x)$ based on the set of training samples $D$*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97797e3f",
   "metadata": {},
   "source": [
    "### **Generative Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c084880",
   "metadata": {},
   "source": [
    "Assume:\n",
    "- Assume Gaussian distribution for $p(x|C_1)$ and $p(x|C_2)$\n",
    "- We already know the prior ($\\pi$) $p(C_1)$ and $p(C_2) = 1 - p(C_1)$ *(only for binary classification)*\n",
    "\n",
    "Recall that for samples $D = \\{x^{(1)}, x^{(2)}, ..., x^{(N)}\\}$ with Gaussian distribution, MLE estimates will be:\n",
    "$$\\mu = \\frac{1}{N} \\sum_{i=1}^N x^{(i)}$$\n",
    "$$\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x^{(i)} - \\mu)^2$$\n",
    "\n",
    "So we know prior and likelihood. product of these 2 value give us posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e710158",
   "metadata": {},
   "source": [
    "**Covariance Matrix**\n",
    "\n",
    "**Definition**  \n",
    "The **covariance matrix** ($\\Sigma$) is a square matrix that captures the pairwise covariances between features in a dataset. For a random vector $x = [x_1, x_2, ..., x_n]^T$, it's defined as:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\\n",
    "\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- Diagonal elements $\\sigma_{ii}$ = variance of feature $x_i$\n",
    "- Off-diagonal elements $\\sigma_{ij}$ = covariance between $x_i$ and $x_j$\n",
    "\n",
    "**Captures relationships**:\n",
    "   - $\\sigma_{ij} > 0$: Features increase together\n",
    "   - $\\sigma_{ij} < 0$: One increases when other decreases\n",
    "   - $\\sigma_{ij} = 0$: No linear relationship\n",
    "\n",
    "**Example (2D Case)**  \n",
    "For features $x$ and $y$:\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\text{Var}(x) & \\text{Cov}(x,y) \\\\\n",
    "\\text{Cov}(y,x) & \\text{Var}(y)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99f46e",
   "metadata": {},
   "source": [
    "**Multivariate Gaussian Distribution:**\n",
    "$$p(x|\\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} e^{(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu))}$$\n",
    "Where:\n",
    "- $x$: is a vector in $R^d$ ($d$-dimensional space) representing the random variables.\n",
    "- $\\mu$: is the mean vector.\n",
    "- $\\Sigma$: is the covariance matrix *(which we will discuss about it soon)*\n",
    "- $|\\Sigma|$ is the determinant of $\\Sigma$.\n",
    "\n",
    "So for $p(x|C_k) = p(x|y=k)$ likelihood is:\n",
    "$$p(x|y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} e^{(-\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k))}$$\n",
    "\n",
    "Prior Distribution ($p(x|C_k)$)  \n",
    "$$p(y=1) = \\pi, \\quad p(y=0) = 1 - \\pi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb731aa",
   "metadata": {},
   "source": [
    "**MLE for Multivariate Gaussian**\n",
    "\n",
    "Assume:\n",
    "- Dataset: $\\{x^{(1)}, x^{(2)}, \\dots, x^{(N)}\\}$ where $x^{(i)} \\in R^d$\n",
    "- Samples drawn from a multivariate Gaussian distribution\n",
    "\n",
    "For each class MLE estimates:\n",
    "- **Mean Vector ($\\mu$)**\n",
    "    $$\\mu = \\frac{\\sum_{i-1}^N x^{(i)}}{N}$$\n",
    "\n",
    "- **Covariance Matrix ($\\Sigma$)**\n",
    "    $$\\Sigma = \\frac{1}{N} \\sum_{i=1}^N (x^{(i)} - \\mu)(x^{(i)} - \\mu)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157292e9",
   "metadata": {},
   "source": [
    "**Decision Boundary for Gaussian Bayes Classifier**\n",
    "\n",
    "$$p(C_1 | x) = p(C_2 | x)$$\n",
    "$$\\ln p(C_1 | x) = \\ln p(C_2 | x)$$\n",
    "$$\\ln p(x|C_1) + \\ln p(C_1) - \\ln p(x) = \\ln p(x|C_2) + \\ln p(C_2) - \\ln p(x)$$\n",
    "$$\\ln p(x|C_1) + \\ln p(C_1) = \\ln p(x|C_2) + \\ln p(C_2)$$\n",
    "Where:\n",
    "$$\\ln p(x|C_k) = -\\frac{d}{2} \\ln 2\\pi - \\frac{1}{2} \\ln|\\Sigma_k^{-1}| - \\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k)$$\n",
    "\n",
    "Key Result:\n",
    "- **Quadratic Boundary**: Curved surface when $\\Sigma_1 \\neq \\Sigma_2$\n",
    "- The posterior $p(C_k \\mid x)$ follows a **sigmoidal (logistic) curve**\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/decisionSurfaceGaussianBayes.png\" alt=\"Decision Boundary Example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d0339",
   "metadata": {},
   "source": [
    "**Shared Covariance Matrix**\n",
    "\n",
    "When class shared a single covariance matrix $\\Sigma = \\Sigma_1 = \\Sigma_2 = ... = \\Sigma_k$:\n",
    "$$p(x|y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} e^{(-\\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k))}$$\n",
    "$$p(C_1) = \\pi \\quad p(C_2) = 1 - \\pi$$\n",
    "Where:\n",
    "$$\\pi = \\frac{N_1}{N}$$\n",
    "$$\\mu_1 = \\frac{\\sum_{n=1}^N y^{(n)}x^{(n)}}{N_1}$$\n",
    "$$\\mu_2 = \\frac{\\sum_{n=1}^N (1 - y^{(n)})x^{(n)}}{N_2}$$\n",
    "$$\\Sigma = \\frac{1}{N}\\left(\\sum_{n \\in C_1}(x^{(n)} - \\mu_1)(x^{(n)} - \\mu_1)^T + \\sum_{n \\in C_2}(x^{(n)} - \\mu_2)(x^{(n)} - \\mu_2)^T\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b775ec",
   "metadata": {},
   "source": [
    "**Decision Boundary for Shared Covariance Gaussian**\n",
    "\n",
    "$$p(C_1 | x) = p(C_2 | x)$$\n",
    "$$\\ln p(C_1 | x) = \\ln p(C_2 | x)$$\n",
    "$$\\ln p(x|C_1) + \\ln p(C_1) - \\ln p(x) = \\ln p(x|C_2) + \\ln p(C_2) - \\ln p(x)$$\n",
    "$$\\ln p(x|C_1) + \\ln p(C_1) = \\ln p(x|C_2) + \\ln p(C_2)$$\n",
    "Where:\n",
    "$$\\ln p(x|C_k) = -\\frac{d}{2} \\ln 2\\pi - \\frac{1}{2} \\ln|\\Sigma^{-1}| - \\frac{1}{2}(x - \\mu_k)^T\\Sigma^{-1}(x - \\mu_k)$$\n",
    "\n",
    "\n",
    "$$\\ln P(x|C_k) = \\underbrace{-\\frac{d}{2} \\ln 2\\pi - \\frac{1}{2} \\ln |\\Sigma|}_{\\text{Constant}} - \\frac{1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)$$\n",
    "\n",
    "Simplify the $x$ contained term:\n",
    "$$-\\frac{1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) + \\ln \\pi = -\\frac{1}{2} (x - \\mu_2)^T \\Sigma^{-1} (x - \\mu_2) + \\ln (1 - \\pi)$$\n",
    "\n",
    "Expand $(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)$ term:\n",
    "$$(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) = x^T \\Sigma^{-1} x - 2 \\mu_k^T \\Sigma^{-1} x + \\mu_k^T \\Sigma^{-1} \\mu_k$$\n",
    "\n",
    "Substitute back:\n",
    "$$-\\frac{1}{2} \\left( x^T \\Sigma^{-1} x - 2 \\mu_1^T \\Sigma^{-1} x + \\mu_1^T \\Sigma^{-1} \\mu_1 \\right) + \\ln \\pi = \\\\\n",
    "-\\frac{1}{2} \\left( x^T \\Sigma^{-1} x - 2 \\mu_2^T \\Sigma^{-1} x + \\mu_2^T \\Sigma^{-1} \\mu_2 \\right) + \\ln (1 - \\pi)$$\n",
    "\n",
    "Since $\\Sigma$ is shared, $x^T \\Sigma^{-1} x$ cancels out:\n",
    "$$\\mu_1^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_1^T \\Sigma^{-1} \\mu_1 + \\ln \\pi = \\mu_2^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_2^T \\Sigma^{-1} \\mu_2 + \\ln (1 - \\pi)$$\n",
    "\n",
    "So the problem will change to linear equation for $x$:\n",
    "$$\\underbrace{(\\mu_1 - \\mu_2)^T \\Sigma^{-1}}_{w^T} x + \\underbrace{\\ln \\frac{\\pi}{1 - \\pi} - \\frac{1}{2} (\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_2^T \\Sigma^{-1} \\mu_2)}_{w_0} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834eb10e",
   "metadata": {},
   "source": [
    "**Naive Bayes Classifier**\n",
    "\n",
    "**Generative Method Issue**\n",
    "- **High number of parameters:**\n",
    "    - Mean vectors:  \n",
    "    $\\mu_1 \\in \\mathbb{R}^d$ (first class), $\\mu_2 \\in \\mathbb{R}^d$ (second class)\n",
    "    Total: $2d$ parameters\n",
    "\n",
    "    - Covariance matrix:  \n",
    "    Symmetric $\\Sigma \\in \\mathbb{R}^{dÃ—d}$ with $\\frac{d(d+1)}{2}$ unique parameters\n",
    "\n",
    "**Assumption:**\n",
    "- Conditional independency of features:\n",
    "    $$p(x|C_k) = p(x_1|C_k)p(x_2|C_k)...p(x_d|C_k)$$\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "- Forces the covariance matrix $\\Sigma$ to be diagonal\n",
    "- Ignores correlations between features\n",
    "- Decision boundaries become axis-aligned\n",
    "\n",
    "**Naive Bayes Decision Rule:**\n",
    "$$y = \\argmax_{k=1,2,...,K} p(C_k | x) = \\argmax_{k=1,2,...,K} p(C_k)\\prod_{i=1}^Np(x_i | C_k)$$\n",
    "\n",
    "For binary features:\n",
    "- Original: $2^d - 1$ parameters per class\n",
    "- Naive Bayes: Only $d$ parameters per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74114704",
   "metadata": {},
   "source": [
    "### **Discriminative Classifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6493d2",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f05202",
   "metadata": {},
   "source": [
    "**Generative Approach:** *(left graph)*\n",
    "- **Inference Stage:**\n",
    "    - **Gaol:** Model the joint distribution $p(x,y)$\n",
    "    - **Steps:**\n",
    "        - **Likelihood:** Estimate $p(x | C_k)$ using: \n",
    "            - Gaussian (univariate)\n",
    "            - Multivariate Gaussian (for multi-dimensional features)\n",
    "        - **Prior:** Estimate $p(C_k)$ using:\n",
    "            - Bernoulli (binary classes)\n",
    "            - Multinomial (multi-class)\n",
    "        - **Posterior:** Apply Bayes- Theorem:\n",
    "            $$p(C_k | x) = \\frac{p(x|C_k)p(C_k)}{p(x)}$$\n",
    "- **Decision Stage:**\n",
    "    - After learning model, optimal class for new input is:\n",
    "    $$\\argmax_k p(C_k|x)$$\n",
    "\n",
    "- **Discriminative Approach:** *(right graph)*\n",
    "    - Directly estimate $p(C_k | x)$ for each class $C_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867779d2",
   "metadata": {},
   "source": [
    "**Two-Class Problem:**\n",
    "\n",
    "Assume:\n",
    "$$\\alpha(x) = w^T(x) + w_0$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/discriminativeVsGenerative.png\" alt=\"Comparison Generative and Discriminative Approaches\">\n",
    "</div>\n",
    "\n",
    "$p(C_k | x)$ can be written as a *Sigmoid (logistic) function*:\n",
    "$$p(C_1 | x) = \\frac{1}{1+e^{(-\\alpha(x))}} = \\sigma(w^Tx+w_0)$$\n",
    "$$p(C_0 | x) = 1 - p(C_1 | x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7c8aa",
   "metadata": {},
   "source": [
    "**Multi-Class Problem:**\n",
    "\n",
    "Assume:\n",
    "$$\\alpha(x) = w^T(x) + w_0$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/MultiDiscVsGen.png\" alt=\"Comparison Multi-Class Generative and Discriminative Approaches\">\n",
    "</div>\n",
    "\n",
    "$p(C_k | x)$ can be written as a *soft-max function*:\n",
    "$$p(C_k | x) = \\frac{e^{\\alpha_k(x)}}{\\sum_{j=1}^K e^{(\\alpha_j(x))}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672c38a",
   "metadata": {},
   "source": [
    "#### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea476e",
   "metadata": {},
   "source": [
    "##### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7570d78",
   "metadata": {},
   "source": [
    "**Sigmoid (Logistic) Function:**\n",
    "\n",
    "Activation Function:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/sigmoid.png\" alt=\"sigmoid function\">\n",
    "</div>\n",
    "\n",
    "**Key Points:**\n",
    "- It is a good candidate for activation function\n",
    "- It gives us a number between 0 and 1 smoothly\n",
    "- It is differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06076585",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "\n",
    "$$p(y = 1 | x, w) = f(x;w)$$\n",
    "$$p(y = 0 | x, w) = 1 - f(x;w)$$\n",
    "\n",
    "Where:\n",
    "$$f(x;w) = \\sigma(w^Tx)$$\n",
    "$$0 \\le f(x;w) \\le 1$$\n",
    "$$\\sigma(w^Tx) = \\frac{1}{1+e^{(-w^Tx)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f2240",
   "metadata": {},
   "source": [
    "##### **Decision Surface (Boundary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d679b",
   "metadata": {},
   "source": [
    "**Recall:**\n",
    "- Definition: A dividing hyperplane that separates different classes in a feature space.\n",
    "- In a $d$-dimensional feature space, the decision boundary for a linear classifier is a hyper plane of dimension $d-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1979b6",
   "metadata": {},
   "source": [
    "- Decision surface $f(x;w) = \\text{constant}$\n",
    "    $$f(x;w) = \\sigma(w^Tx) = \\frac{1}{1 + e ^{-(w^Tx)}} = 0.5$$\n",
    "\n",
    "- Decision surfaces are **linear functions** of $x$:\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } f(x;w) \\ge 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3b9b1",
   "metadata": {},
   "source": [
    "##### **Maximum Likelihood Estimation (MLE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27051c87",
   "metadata": {},
   "source": [
    "**Maximum Log Likelihood:**\n",
    "$$\\hat{w} = \\argmax_w \\log \\left(\\prod_{i=1}^N p(y^{(i)} | w, x^{(i)})\\right)$$\n",
    "\n",
    "**Bernoulli Model**  \n",
    "- For binary classification:\n",
    "$$\n",
    "p(y^{(i)} | w, x^{(i)}) = \n",
    "\\begin{cases} \n",
    "f(x^{(i)};w) & \\text{if } y^{(i)} = 1 \\\\\n",
    "1 - f(x^{(i)};w) & \\text{if } y^{(i)} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "- Concept Form:\n",
    "$$p(y^{(i)} | w, x^{(i)}) = f(x^{(i)};w)^{y^{(i)}} (1 - f(x^{(i)};w))^{(1 - y^{(i)})}$$\n",
    "\n",
    "**Substitute In MLE formula:**\n",
    "$$\\log \\left(p(y^{(i)} | w, x^{(i)})\\right) = \\sum_{i=1}^N \\left[y^{(i)}\\log \\left(f(x^{(i)};w)\\right) + (1 - y^{(i)}) \\log\\left(1 - f(x^{(i)};w)\\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b230d2",
   "metadata": {},
   "source": [
    "##### **Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abf9d2",
   "metadata": {},
   "source": [
    "**Cost Function: Negative Likelihood**\n",
    "\n",
    "To convert maximization to minimization:\n",
    "$$J(w) = -\\sum_{i=1}^N \\log \\left(p(y^{(i)} | w, x^{(i)}) \\right)$$\n",
    "$$ = -\\sum_{i=1}^N y^{(i)}\\log \\left(f(x^{(i)};w)\\right) + (1 - y^{(i)}) \\log \\left(1 - f(x^{(i)};w)\\right)$$\n",
    "\n",
    "So:\n",
    "$$\\hat{w} = \\argmin_w J(w)$$\n",
    "\n",
    "**Key Properties:**\n",
    "- No Closed form solution for $\\nabla_wJ(w) = 0$  \n",
    "- However $J(w)$ is **convex** and has global minimum.\n",
    "- Solution Method: Use iterative optimization (e.g., gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de4554",
   "metadata": {},
   "source": [
    "##### **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681bc3e",
   "metadata": {},
   "source": [
    "Recall:\n",
    "$$w^{t+1} = w^t - \\eta \\nabla_wJ(w^t)$$\n",
    "\n",
    "Where:\n",
    "$$\\nabla_wJ(w^t) = \\sum_{i=1}^N \\left(f(x^{(i)};w) - y^{(i)}\\right)x^{(i)}$$\n",
    "\n",
    "Recall gradient descent of SSE using in linear regression:\n",
    "$$\\nabla_wJ(w^t) = \\sum_{i=1}^N \\left(w^Tx^{(i)} - y^{(i)}\\right)x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335ef12",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "Cost Function:\n",
    "$$J(w) = -\\sum_{i=1}^N y^{(i)}\\log \\left(f(x^{(i)};w)\\right) + (1 - y^{(i)}) \\log \\left(1 - f(x^{(i)};w)\\right)$$\n",
    "\n",
    "Where:\n",
    "- $f(x;w) = \\sigma(w^Tx^{(i)})$ is the sigmoid function.\n",
    "\n",
    "Derivative of a single term $(x^{(i)}, y^{(i)})$:\n",
    "$$\\frac{\\partial}{\\partial w}\\left[y^{(i)}\\log \\left(\\sigma(w^Tx^{(i)})\\right) + (1 - y^{(i)})\\log \\left(1 - \\sigma(w^Tx^{(i)})\\right) \\right]$$\n",
    "\n",
    "Using Chain Rule:\n",
    "$$\\frac{\\partial}{\\partial w}\\sigma(w^Tx^{(i)}) = \\sigma(w^Tx^{(i)})(1 - \\sigma(w^Tx^{(i)}))x^{(i)}$$\n",
    "\n",
    "So Break the cost function into two parts\n",
    "- if $y^{(i)} = 1$:\n",
    "$$\\frac{\\partial}{\\partial w} \\log(\\sigma(w^Tx^{i})) = \\frac{1}{\\sigma(w^Tx^{(i)})}\\sigma(w^Tx^{(i)})(1 - \\sigma(w^Tx^{(i)}))x^{(i)} = (1 - \\sigma(w^Tx^{(i)}))x^{(i)}$$\n",
    "- if $y^{(i)} = 0$:\n",
    "$$\\frac{\\partial}{\\partial w} \\log(1 - \\sigma(w^Tx^{i})) = \\frac{-1}{1 - \\sigma(w^Tx^{(i)})}\\sigma(w^Tx^{(i)})(1 - \\sigma(w^Tx^{(i)}))x^{(i)} = -(\\sigma(w^Tx^{(i)}))x^{(i)}$$\n",
    "\n",
    "Combine Case (The gradient for one sample simplifies to):\n",
    "$$\\left(\\sigma(w^Tx^{(i)}) - y^{(i)}\\right)x^{(i)} = \\left(f(x^{(i)};w) - y^{(i)}\\right)x^{(i)}$$\n",
    "\n",
    "Sum over all $N$ samples (Full Gradient):\n",
    "$$\\nabla_wJ(w^t) = \\sum_{i=1}^N \\left(w^Tx^{(i)} - y^{(i)}\\right)x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58343979",
   "metadata": {},
   "source": [
    "**Loss Function**\n",
    "\n",
    "**Definition:** Single overall measure of loss incurred for taking our decisions over entire dataset.\n",
    "\n",
    "**Formula:**\n",
    "$$Loss(y,f(x;w)) = -y \\log \\left(f(x;w)\\right) - (1 - y) \\log \\left(1 - f(x;w) \\right)$$\n",
    "Since in binary classification either $y = 1$ or $y = 0$ then:\n",
    "$$\n",
    "Loss(y,f(x;w))\n",
    "\\begin{cases} \n",
    "\\log \\left(f(x;w)\\right) & \\text{if } y = 1 \\\\\n",
    "\\log \\left(1 - f(x;w)\\right) & \\text{if } y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Key Properties:**\n",
    "- **Penalty for Overconfidence:**\n",
    "    - It heavily penalizes confident but incorrect predictions (e.g., predicting a value close to 1 when the true label is 0). This encourages the model to be cautious unless itâ€™s highly certain.\n",
    "- **Encourages Confidence Near Decision Boundary:**\n",
    "    - Even correct predictions incur a loss if they are close to the decision threshold (e.g., predicting 0.51 when the true label is 1), pushing the model to be more confident and create better class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d7f8f",
   "metadata": {},
   "source": [
    "##### **Multi-Class Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e1360",
   "metadata": {},
   "source": [
    "- **Definition:** A problem where we have K classes and every sample only belongs to one class *(for simplicity)*\n",
    "\n",
    "- For each class $k$, $f_k(x;W)$ predicts the probability of $y=k$:\n",
    "    $$p(y=k|x,W)$$\n",
    "    Where:\n",
    "    - $W$ denotes a matrix of $w_i$'s, which each $w_i$ is a weight vector dedicated for class label $i$.\n",
    "    - $f_k(x;W) = \\sigma_k(x;w)$\n",
    "\n",
    "- On a new input $x$, pick the class that maximizes $f_k(x;W)$:\n",
    "$$\\alpha(x) = \\argmax_{k=1,2,...,K}f_k(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7064eb1",
   "metadata": {},
   "source": [
    "**Problem Setup**\n",
    "\n",
    "**Assumptions:**\n",
    "- $K \\gt 2$\n",
    "- $y \\in \\{1, 2, ..., K\\}$:\n",
    "\n",
    "**Normalized exponential (Softmax)**\n",
    "$$f_k(x,W) = p(y=k | x) = \\frac{e^{w_k^Tx}}{\\sum_{j = 1}^K e^{w_j^Tx}}$$\n",
    "\n",
    "**Decision Behavior:**  \n",
    "$$\\text{If } {w_k^Tx} \\gg {w_j^Tx} \\quad \\forall j \\neq k \\Rightarrow\n",
    "\\begin{cases}\n",
    "p(C_k|x) \\approx 1 \\\\\n",
    "p(C_j|x) \\approx 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Recall *Bayes' theorem* formulation:\n",
    "$$p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{\\sum_{j=1}^K p(x|C_j)p(C_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5813f02",
   "metadata": {},
   "source": [
    "**Softmax Function**\n",
    "\n",
    "Softmax function is good candidate because:\n",
    "- Smoothly highlights the maximum probability\n",
    "- Differentiable.\n",
    "- Handle negative values because of exponential function\n",
    "- Normalization:\n",
    "$$\\sum_{k=1}^K \\frac{e^{w_k^Tx}}{\\sum_{j=1}^K e^{w_j^Tx}} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4f421",
   "metadata": {},
   "source": [
    "**Recall Multinomial Distribution:**\n",
    "\n",
    "Parameter Definition:\n",
    "$$\\theta = [\\theta_1, \\theta_2, ..., \\theta_K]$$\n",
    "Where:\n",
    "$$\\theta_k \\in [0, 1] \\quad \\text{and} \\quad \\sum_{k=1}^{K} \\theta_k = 1$$\n",
    "$$\\theta_k = p(x_k = 1)$$\n",
    "\n",
    "Likelihood:\n",
    "$$P(x|\\theta) = \\prod_{k=1}^K \\theta_k^{x_k} = \\theta_j \\quad \\text{(when $x_j = 1$)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565e78b",
   "metadata": {},
   "source": [
    "Set **cost function** as **negative of log likelihood**.\n",
    "\n",
    "We need $\\hat{W} = \\argmin_W J(W)$\n",
    "\n",
    "$$J(W) = -\\log\\prod_{i=1}^Np(y^{(i)} | x^{(i)}, W)$$\n",
    "$$ = -\\log \\prod_{i=1}^N \\prod_{k=1}^K f_k(x^{(i)};W)^{{y_k}^{(i)}}$$\n",
    "$$ = -\\log \\sum_{i=1}^N \\sum_{k=1}^K {{y_k}^{(i)}}\\log \\left(f_k(x^{(i)};W)\\right)$$\n",
    "\n",
    "There is no closed-form solution for $\\hat{W}$.  \n",
    "Use iterative optimization instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38e076",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "$$w_j^{t+1} = w_j^{t} - \\eta \\nabla_w(W^t)$$\n",
    "Where:\n",
    "$$\\nabla_{w_j}(W) = \\sum_{i=1}^N (f_j(x^{(i)};w) - y_j^{(i)})x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba8ecb",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4baba",
   "metadata": {},
   "source": [
    "**Logistic Regression (LR)**\n",
    "\n",
    "- **Binary Classification:**\n",
    "    - **Type:** Linear classifier with probabilistic outputs\n",
    "    - **Assumption:** Bernoulli-distributed $P(y|x)$ with mean $\\sigma(w^Tx) = \\frac{1}{1+e^{-w^Tx}}$\n",
    "    - **Optimization:**\n",
    "        - MLE-derived cost: $J(w) = -\\sum [y_i\\log\\sigma(w^Tx_i) + (1-y_i)\\log(1-\\sigma(w^Tx_i))]$\n",
    "        - No closed-form solution for its optimization problem\n",
    "        - Convex: Global optimum via gradient ascent\n",
    "\n",
    "- **Multi-Class Extension**\n",
    "    - **Assumption:** Multinomial Distribution for $K$ classes with:\n",
    "        - Probability vector: $\\theta = [\\theta_1,...,\\theta_K]$ where $\\sum_k \\theta_k=1$\n",
    "        - One-hot labels: $y \\in {0,1}^K$ (single 1 per sample)\n",
    "    - **Softmax Regression** (Generalizes logistic regression):\n",
    "    $$p(y=k | x) = \\frac{e^{w_k^Tx}}{\\sum_{j = 1}^K e^{w_j^Tx}}$$\n",
    "    - **Optimization:**\n",
    "        - MLE-derived cost\n",
    "        - No closed-form solution for its optimization problem\n",
    "        - Convex: Global optimum via gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518340b9",
   "metadata": {},
   "source": [
    "**Discriminative vs. Generative: Number of Parameters**\n",
    "\n",
    "for $d$-dimensional feature space\n",
    "\n",
    "- **Logistic Regression:** $d+1$ parameters\n",
    "    - $w = (w_0, w_1, ..., w_d)$\n",
    "\n",
    "- **Generative Approach:** Gaussian class-conditional with shared covariance matrix:\n",
    "    - $2d$ parameters for means\n",
    "    - $\\frac{d(d+1)}{2}$ parameters for shared covariance matrix\n",
    "    - one parameter for class prior $p(C_k)$\n",
    "\n",
    "LR is **more robust**, **less sensitive** to incorrect modeling assumptions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
