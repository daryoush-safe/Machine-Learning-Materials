{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041cf194",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6edc8",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ceade2",
   "metadata": {},
   "source": [
    "**Margin:**\n",
    "- The **smallest distance** between the **decision boundary** and any **training sample**.\n",
    "- A **larger margin** improves **generalization** (better performance on unseen data).\n",
    "\n",
    "**Support Vector Machine (SVM) Objectives:**\n",
    "- Finds the maximum-margin solution for better generalization.\n",
    "- The optimal hyperplane is the one farthest from all training samples.\n",
    "- The best hyperplane has equal distances to the nearest samples of both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d130824",
   "metadata": {},
   "source": [
    "**Finding $w$ with large margin**\n",
    "\n",
    "**Assumptions**\n",
    "- Separate the bias term $w_0$ from weights $w = [w_1, w_2, ..., w_d]$:\n",
    "$$w_Tx + w_0 = 0$$\n",
    "\n",
    "- Normalize $w$ and $w_0$\n",
    "    - Let $x^{(n)}$ be the nearest point to the plane\n",
    "    - Fix the minimum distance of nearest point to decision boundary:\n",
    "    $$|w^Tx^{(n)}| = 1$$\n",
    "\n",
    "**Distance Derivation**\n",
    "- Recall (from earlier proof):\n",
    "    $$\\text{Distance from point to hyperplane} =  \\frac{|w^Tx^{(n)} + w_0|}{||w||}$$\n",
    "- Under our assumption, the nearest point's distance becomes:\n",
    "    $$\\frac{|w^Tx^{(n)} + w_0|}{||w||} = \\frac{1}{||w||}$$\n",
    "\n",
    "**Optimization problem**\n",
    "- For the entire margin (both sides), we maximize:\n",
    "    $$\\max_{w,w_0} \\frac{2}{||w||}$$\n",
    "- Equivalently, we minimize:\n",
    "    $$\\min_{w,w_0} \\frac{1}{2}||w||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744e764",
   "metadata": {},
   "source": [
    "### **Hard-Margin SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56854a05",
   "metadata": {},
   "source": [
    "#### **Problem Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb9277",
   "metadata": {},
   "source": [
    "**Key Assumption:**  \n",
    "The classes are linearly separable.\n",
    "\n",
    "**Objective:**  \n",
    "Find the hyperplane with the maximum margin.\n",
    "\n",
    "**Optimization Problem**  \n",
    "We seek:\n",
    "$$\\max_{w,w_0} \\frac{2}{||w||}$$\n",
    "$$\\text{s.t. } \\min_{n=1,...,N}|w^Tx^{(n)} + w_0| = 1$$\n",
    "\n",
    "For correct classification, the following must hold:\n",
    "  $$\n",
    "  \\begin{cases} \n",
    "  w^Tx^{(n)} + w_0 \\ge +1 & \\forall y^{(n)} = +1 \\\\\n",
    "  w^Tx^{(n)} + w_0 \\le -1 & \\forall y^{(n)} = -1\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  Equivalently:\n",
    "  $$|w^Tx^{(n)} + w_0| = y^{(n)}(w^Tx^{(n)} + w_0)$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/hardMarginProblemOverview.png\" alt=\"Problem Overview\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f527d59",
   "metadata": {},
   "source": [
    "We can equivalently optimize:\n",
    "$$\\min_{w,w_0}\\frac{1}{2}||w||^2$$\n",
    "$$\\text{s.t. } y^{(n)}(w^Tx^{(n)} + w_0) \\ge 1 \\quad n = 1, 2, ..., N$$\n",
    "\n",
    "**Properties:**\n",
    "- It is a convex **Quadratic Programming (QP)** problem.\n",
    "- There are computationally efficient packages to solve it.\n",
    "- It has a global minimum (if any).\n",
    "\n",
    "**QP Standard Form:**  \n",
    "- A general QP problem is defined as:\n",
    "    $$\\min_x \\frac{1}{2}x^TQx + c^Tx$$\n",
    "    $$Ax \\le b, \\quad Ex = d$$\n",
    "- Mapping SVM to QP:\n",
    "    - $Q = I$\n",
    "    - $c = 0$\n",
    "    - $x = w$\n",
    "\n",
    "    As a result we got:\n",
    "    $$\\min_w \\frac{1}{2}w^Tw$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb39051",
   "metadata": {},
   "source": [
    "#### **Dual Optimization Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4198a66",
   "metadata": {},
   "source": [
    "The *dual* problem is equivalent to the original *primal* problem. The dual problem:\n",
    "- Is often **easier** to solve\n",
    "- Gives us **further insights** into the optimal hyperplane\n",
    "- Enables the **kernel trick** (for nonlinear classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e6cb3",
   "metadata": {},
   "source": [
    "**Lagrangian Multiplier Overview**\n",
    "\n",
    "**Given:**\n",
    "$$p^* = \\min_x f(x)$$\n",
    "$$\\text{s.t. } g_i(x) \\le 0 \\quad i=1,...,m$$\n",
    "$$\\text{s.t. } h_i(x) = 0 \\quad i=1,...,p$$\n",
    "\n",
    "**Lagrangian Function:**\n",
    "$$L(x, \\alpha, \\lambda) = f(x) + \\sum_{i=1}^m \\alpha_i g_i(x) + \\sum_{i=1}^p \\lambda_i h_i(x)$$\n",
    "Where \n",
    "- $\\alpha$, $\\lambda$ are Lagrangian multipliers\n",
    "- $\\alpha = [\\alpha_1, ..., \\alpha_m]$\n",
    "- $\\lambda = [\\lambda_1, ..., \\lambda_p]$\n",
    "\n",
    "**Behavior of the Lagrangian:**\n",
    "- If $g_i(x) \\gt 0$ and $\\alpha_i \\gg 0$, $L \\rightarrow +\\infty$\n",
    "- if $h_i(x) \\neq 0$ and $\\lambda_i$ with the same sign as $h_i(x)$ and $|\\lambda_i| \\gg 0$, $L \\rightarrow +\\infty$\n",
    "- Only when all constraints are satisfied does $L = f(x)$\n",
    "\n",
    "$$\n",
    "\\max_{\\{\\alpha_i\\ge0\\}, \\{\\lambda_i\\}} L(x, \\alpha, \\lambda) = \n",
    "\\begin{cases} \n",
    "\\infty & \\forall g_i(x)\\gt 0 \\\\\n",
    "\\infty & \\forall h_i(x)\\neq 0 \\\\\n",
    "f(x) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Primal Problem:**\n",
    "$$p^* = \\min_x \\max_{\\{\\alpha_i\\ge0\\}, \\{\\lambda_i\\}} L(x, \\alpha, \\lambda)$$\n",
    "\n",
    "**Key Insight:**\n",
    "- Instead of constrained minimization of $f(x)$, we now consider unconstrained optimization of $L(x, \\alpha, \\lambda)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e843df",
   "metadata": {},
   "source": [
    "**Dual Problem Remarks**\n",
    "\n",
    "- **Primal problem**:\n",
    "    $$p^* = \\min_x \\max_{\\{\\alpha_i\\ge0\\}, \\{\\lambda_i\\}} L(x, \\alpha, \\lambda)$$   \n",
    "- **Dual problem**\n",
    "    $$d^* = \\max_{\\{\\alpha_i\\ge0\\}, \\{\\lambda_i\\}} \\min_x L(x, \\alpha, \\lambda)$$\n",
    "\n",
    "- **Weak Duality:**  \n",
    "    For any function $h(x, y)$\n",
    "    $$\\max_x \\min_y h(x, y) \\le \\min_y \\max_x h(x, y)$$\n",
    "\n",
    "    Obtained by swapping the order of min and max: $d^* \\le p^*$\n",
    "\n",
    "- **Strong Duality:**\n",
    "    - If the original problem is convex (i.e, $f$ and $g$ are convex, $h$ is affine), we have **strong duality**\n",
    "        $$d^* = p^*$$\n",
    "    - The dual solution equals the primal solution.\n",
    "    - This holds for SVM since $\\frac{1}{2} ||w||^2$ is convex and constraints are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4175ad",
   "metadata": {},
   "source": [
    "**Primal Optimization Problem:**\n",
    "$$\\min_{w, w_0} \\frac{1}{2}||w||^2$$\n",
    "$$\\text{s.t. } y^{(i)}(w^T x^{(i)} + w_0) \\ge 1 \\quad i = 1, ..., N$$\n",
    "\n",
    "**Lagrangian Formulation:**  \n",
    "- For constrained optimization problems of the form:\n",
    "    $$p^* = \\min_x f(x)$$\n",
    "    $$\\text{s.t. } g_i(x) \\le 0 \\quad i=1,...,m$$\n",
    "- The Lagrangian function is:\n",
    "    $$L(x, \\alpha, \\lambda) = f(x) + \\sum_{i=1}^m \\alpha_i g_i(x)$$\n",
    "\n",
    "**Applying to SVM:**  \n",
    "- rewriting constraints:\n",
    "    $$1 - y^{(i)}(w^T x^{(i)} + w_0) \\le 0 \\quad i = 1, ..., N$$\n",
    "- Primal Lagrangian:\n",
    "    $$\\min_{w,w_0} \\max_{\\{\\alpha_n \\ge 0\\}} \\{\\frac{1}{2}||w||^2 + \\sum_{n=1}^N \\alpha_n(1 - y^{(n)}(w^T x^{(n)} + w_0))\\}$$\n",
    "\n",
    "**Dual Problem Formulation:**  \n",
    "- Switching min/max order:\n",
    "    $$\\max_{\\{\\alpha_n \\ge 0\\}} \\min_{w,w_0} \\{\\frac{1}{2}||w||^2 + \\sum_{n=1}^N \\alpha_n(1 - y^{(n)}(w^T x^{(n)} + w_0))\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef911e6",
   "metadata": {},
   "source": [
    "**Dual Problem Formulation:**\n",
    "$$\\max_{\\{\\alpha_n \\ge 0\\}} \\min_{w,w_0} L(w, w_0, \\alpha)$$\n",
    "$$L(w, w_0, \\alpha) = \\frac{1}{2}||w||^2 + \\sum_{n=1}^N \\alpha_n(1 - y^{(n)}(w^T x^{(n)} + w_0))$$\n",
    "\n",
    "**Optimality Conditions:**\n",
    "- Gradient w.r.t. $w$:\n",
    "    $$\\nabla_w L(w, w_0, \\alpha) = 0 \\implies w - \\sum_{n=1}^N \\alpha_n y^{(n)} x^{(n)} = 0$$\n",
    "    $$w = \\sum_{n=1}^N \\alpha_n y^{(n)} x^{(n)}$$\n",
    "\n",
    "- Derivate w.r.t. $w_0$:\n",
    "    $$\\frac{\\partial L(w, w_0, \\alpha)}{\\partial w_0} = 0 \\implies -\\sum_{n=1}^N \\alpha_n y^{(n)} = 0$$\n",
    "\n",
    "No constraints on $w_0$, instead, a global constraint on $\\alpha$ is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa2d87",
   "metadata": {},
   "source": [
    "Original Lagrangian function:\n",
    "    $$L(w, w_0, \\alpha) = \\frac{1}{2} w^Tw + \\sum_{n=1}^N a_n(1-y^{(n)}(w^Tx^{(n)} + w_0))$$\n",
    "    $$ = \\frac{1}{2} w^Tw + \\sum_{n=1}^N a_n - \\sum_{n=1}^N a_ny^{(n)}w^Tx^{(n)} - \\sum_{n=1}^N a_n y^{(n)}w_0$$\n",
    "\n",
    "Since $\\sum_{n=1}^N \\alpha_n y^{(n)} = 0$ Then:\n",
    "    $$\\sum_{n=1}^N a_n y^{(n)}w_0 = 0$$\n",
    "\n",
    "Substitute optimal $w$:\n",
    "    $$\\frac{1}{2} w^Tw = \\frac{1}{2} \\sum_{n=1}^N a_ny^{(n)}w^Tx^{(n)} \\implies L(w, w_0, \\alpha) = \\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N a_ny^{(n)}w^Tx^{(n)}$$\n",
    "\n",
    "Substitute optimal $w^T$:\n",
    "    $$L(w, w_0, \\alpha) = \\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {x^{(n)}}^T x^{(m)}$$\n",
    "\n",
    "Subject to:\n",
    "    $$\\sum_{n=1}^N \\alpha_n y^{(n)} = 0$$\n",
    "    $$\\alpha_n \\ge 0 \\quad n = 1, ..., N$$\n",
    "\n",
    "**Key Result:**\n",
    "- Final Dual Optimization:\n",
    "    $$\\max_{\\alpha} \\left\\{\\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {x^{(n)}}^T x^{(m)}\\right\\}$$\n",
    "    $$\\text{s.t. } \\sum_{n=1}^N \\alpha_n y^{(n)} = 0, \\quad \\alpha_n \\ge 0 \\quad \\forall n$$\n",
    "- It is convex QP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b10d7f",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "The dual SVM optimization problem can be expressed as a QP:\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2} \\alpha^T \n",
    "\\begin{bmatrix}\n",
    "y^{(1)} y^{(1)} x^{(1)^T} x^{(1)} & \\cdots & y^{(1)} y^{(N)} x^{(1)^T} x^{(N)} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(N)} y^{(1)} x^{(N)^T} x^{(1)} & \\cdots & y^{(N)} y^{(N)} x^{(N)^T} x^{(N)}\n",
    "\\end{bmatrix}\n",
    "\\alpha - 1^T \\alpha\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "- $ -\\alpha \\leq 0$  \n",
    "   (Equivalent to $\\alpha_i \\geq 0 \\ \\forall i$)\n",
    "\n",
    "- $ \\mathbf{y}^T \\alpha = 0 $  \n",
    "   (Equivalent to $\\sum_{n=1}^N \\alpha_n y^{(n)} = 0$)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Connection to Standard QP Form:**  \n",
    "This matches the general QP formulation:\n",
    "$$\n",
    "\\min_{x} \\frac{1}{2}x^T Q x + \\mathbf{c}^T x\n",
    "$$\n",
    "where:\n",
    "- $Q$ is the kernel matrix\n",
    "- $\\mathbf{c} = -\\mathbf{1}$\n",
    "- $x = \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd7cb0",
   "metadata": {},
   "source": [
    "#### **Karush-Kuhn-Tucker (KKT) Conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0e085",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "\n",
    "**Optimization Problem:**\n",
    "    $$p^* = \\min_x f(x)$$\n",
    "    $$\\text{s.t. } g_i(x) \\le 0 \\quad i=1,...,m$$\n",
    "\n",
    "**Lagrangian Function:**\n",
    "    $$L(x, \\alpha, \\lambda) = f(x) + \\sum_{i=1}^m \\alpha_i g_i(x)$$\n",
    "\n",
    "The Optimal $x^*$ and $\\alpha^*$ satisfies **KKT conditions:**\n",
    "- **Stationary:**\n",
    "    $$\\nabla_w L(x, \\alpha)|_{x^*, \\alpha^*} = 0$$\n",
    "- **Primal Feasibility:**\n",
    "    $$g_i(x^*) \\le 0 \\quad \\forall i$$\n",
    "- **Dual Feasibility:**\n",
    "    $$\\alpha_i^* \\ge 0 \\quad \\forall i$$\n",
    "- **Complementary Slackness:**\n",
    "    $$\\alpha_i^* g_i(x^*) = 0 \\quad \\forall i$$\n",
    "    Means either the constraint $g_i(x^*) = 0$ or the Lagrange multiplier $\\alpha_i^* = 0$\n",
    "    - **Active Constraints:**  \n",
    "        If $\\alpha_i^* \\gt 0$, then $g_i(x^*) = 0$ (the $i$-th constraint is tight at the solution).\n",
    "\n",
    "    - **Inactive Constraints:**  \n",
    "        If $g_i(x^*) \\lt 0$, then $\\alpha_i^* = 0$ (the constraint has no influence on the solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c7672",
   "metadata": {},
   "source": [
    "**SVM via KKT**\n",
    "\n",
    "For the solution $(w^*, w_0^*, \\alpha^*)$, the following must hold:\n",
    "- **Stationary:**\n",
    "    $$\\nabla_w L(w^*, w_0^*, \\alpha^*)|_{w^*, w_0^*, \\alpha^*} = 0$$\n",
    "    $$\\frac{\\partial L(w^*, w_0^*, \\alpha^*)}{\\partial w_0} = 0$$\n",
    "\n",
    "- **Primal Feasibility:**\n",
    "    $$y^{(n)}({w^*}^T x^{(n)} + w_0^*) \\ge 1 \\quad \\forall n$$\n",
    "\n",
    "- **Dual Feasibility:**\n",
    "    $$\\alpha_n^* \\ge 0 \\quad \\forall n$$\n",
    "\n",
    "- **Complementary Slackness:**\n",
    "    $${\\alpha_i}^*(1 - y^{(n)}({w^*}^T x^{(n)} + w_0^*)) = 0 \\quad \\forall n$$\n",
    "    - **Active Constraint** ($\\alpha_n^* > 0$):  \n",
    "        - $y^{(n)}({w^*}^T x^{(n)} + w_0^*) = 1$\n",
    "        - $x^{(n)}$ lies exactly on the margin (**Support Vector**).  \n",
    "\n",
    "    - **Inactive Constraint** ($\\alpha_n^* = 0$):  \n",
    "        - $y^{(n)}({w^*}^T x^{(n)} + w_0^*)  \\ge 1$\n",
    "        - $x^{(n)}$ has no impact on $w^*$.  \n",
    "        - A sample with $\\alpha_n^* = 0$ can also lie on one of the margin hyperplanes\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/supportVector.png\" alt=\"Support Vector\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c163d",
   "metadata": {},
   "source": [
    "**Support Vectors:**\n",
    "\n",
    "- Data points that are closest to the hyperplane that separates different classes.\n",
    "- Lie exactly on the margin ($y^{(n)}(w^T x^{(n)} + w_0) = 1$).\n",
    "- SV = $\\{x^{(n)} | \\alpha_n \\gt 0\\}$\n",
    "- Sparse solution: Typically few SVs compared to total samples.\n",
    "- The direction of hyper-plane can be found only based on support vectors:\n",
    "$$w = \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} x^{(n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22bcc3",
   "metadata": {},
   "source": [
    "#### **Constructing the Hyperplane**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263f457",
   "metadata": {},
   "source": [
    "After finding $\\alpha$ by QP, **compute $w$:**\n",
    "$$w = \\sum_{n=1}^N \\alpha_n y^{(n)} x^{(n)} = \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} x^{(n)}$$\n",
    "\n",
    "**Key Properties:**\n",
    "- Number of dual variables equals training set size.\n",
    "- Sparse solution: Typically few SVs ($\\alpha \\gt 0$) compared to total samples.\n",
    "\n",
    "**Compute $w_0$:**  \n",
    "Each of the samples that has $\\alpha_s \\gt 0$ is on the margin, thus we solve for $ð‘¤_0$ using any of SVs:\n",
    "$$|w^Tx^{(s)} + w_0| = 1$$\n",
    "$$(w^Tx^{(s)} + w_0) = y^{(s)}$$\n",
    "$$w_0 = y^{(s)} - w^Tx^{(s)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdee49",
   "metadata": {},
   "source": [
    "Classification of a new sample $x$:\n",
    "$$\\hat{y} = \\text{sign}(w_0 + w^Tx)$$\n",
    "$$\\hat{y} = \\text{sign} \\left(w_0 + \\left(\\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} x^{(n)} \\right)^T x \\right)$$\n",
    "\n",
    "$$\\hat{y} = \\text{sign} \\left(\\underbrace{y^{(s)} - \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} {x^{(n)}}^T x^{(s)}}_{w_0^*} + \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} {x^{(n)}}^T x \\right)$$\n",
    "\n",
    "**Key Insights:**\n",
    "- $\\alpha_n \\gt 0$: Support vectors are sufficient to predict labels of new samples.\n",
    "- The classifier is based on the expansion in terms of dot products of $x$ with support vectors.\n",
    "- Later, we will see how replacing the dot product with more complex kernel functions enables the creation of more complex classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de7b95",
   "metadata": {},
   "source": [
    "#### **Dual Problem Key Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af2990",
   "metadata": {},
   "source": [
    "**Dual Optimization Problem:**\n",
    "$$\\max_{\\alpha} \\left\\{\\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {x^{(n)}}^T x^{(m)}\\right\\}$$\n",
    "$$\\text{s.t. } \\sum_{n=1}^N \\alpha_n y^{(n)} = 0, \\quad \\alpha_n \\ge 0 \\quad \\forall n$$\n",
    "\n",
    "**Key Properties:**\n",
    "- The Lagrange multiplier vector $\\alpha$ has length $N$ (one entry per training sample)\n",
    "- Typically, most $\\alpha_n = 0$ (only support vectors have $\\alpha_n > 0$)\n",
    "- Simpler Constraints: Only requires $\\alpha_n \\geq 0$ and one linear equality\n",
    "- Number of parameters ($N$) is independent of feature space dimension (enables kernel trick)\n",
    "- Sparse solution (few support vectors) reduces prediction complexity\n",
    "- $\\alpha$ values reveal support vectors and margin properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea5d7f",
   "metadata": {},
   "source": [
    "**Most Important Property:**  \n",
    "    We can work in high-dimensional (even infinite) feature spaces without explicitly computing $\\phi(x)$, as long as we can compute the kernel $K(x,x')$:\n",
    "    $$\\max_{\\alpha} \\left\\{\\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {\\phi(x^{(n)})}^T \\phi(x^{(m)})\\right\\}$$\n",
    "    $$\\text{s.t. } \\sum_{n=1}^N \\alpha_n y^{(n)} = 0, \\quad \\alpha_n \\ge 0 \\quad \\forall n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b39cf",
   "metadata": {},
   "source": [
    "### **Soft-Margin SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a2782",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756546b6",
   "metadata": {},
   "source": [
    "Must find a solution even though the classes are not exactly linearly separable\n",
    "\n",
    "Extend the hard-margin SVM to allow classification error:\n",
    "- Overlapping classes that can be approximately separated by a linear boundary\n",
    "- Noise in the linearly separable classes\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/softMarginCauses.png\" alt=\"Why Soft Margin\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312328c8",
   "metadata": {},
   "source": [
    "- Minimizing the number of misclassified points\n",
    "    - NP-complete\n",
    "- Soft-Margin:\n",
    "    - Maximizing margin\n",
    "    - Try to minimize the distance between misclassified points and their correct margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072e36e",
   "metadata": {},
   "source": [
    "#### **Problem Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392edda",
   "metadata": {},
   "source": [
    "**Recall: Hard-Margin**\n",
    "$$\\min_{w,w_0}\\frac{1}{2}||w||^2$$\n",
    "$$\\text{s.t. } y^{(n)}(w^Tx^{(n)} + w_0) \\ge 1 \\quad n = 1, 2, ..., N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60447f",
   "metadata": {},
   "source": [
    "SVM with **slack variables**:  \n",
    "Allows samples to fall within the margin, but penalizes them\n",
    "$$\\min_{w,w_0}\\frac{1}{2}||w||^2 + C\\sum_{n=1}^N\\xi_n$$\n",
    "$$\\text{s.t. } y^{(n)}(w^Tx^{(n)} + w_0) \\ge 1 - \\xi_n \\quad n = 1, 2, ..., N$$\n",
    "$$\\xi_n \\ge 0$$\n",
    "\n",
    "Where:\n",
    "- Margin violation amount $\\xi_n$: **Slack variable**\n",
    "- Total violation: $\\sum_{n=1}^N\\xi_n$\n",
    "$$ \\begin{cases}\n",
    "\\xi_n = 0 & \\text{$x{(n)}$ is linearly separable} \\\\\n",
    "0 \\lt \\xi_n \\lt 1 & \\text{$x{(n)}$ is correctly classified but inside margin} \\\\\n",
    "\\xi_n \\gt 1 & \\text{$x{(n)}$ is misclassified}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c568180",
   "metadata": {},
   "source": [
    "**Parameter $C$**\n",
    "\n",
    "$C$ is a tradeoff parameter:\n",
    "- Small $C$ allows margin constraints to be easily ignored\n",
    "    - Large margin\n",
    "- Large $C$ makes constraints hard to ignore\n",
    "    - Narrow margin\n",
    "\n",
    "$C \\rightarrow \\infty$ enforces all constraints: **Hard-Margin**\n",
    "\n",
    "$C$ can be determined using a technique like *cross validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772f39b",
   "metadata": {},
   "source": [
    "**Key Results:**\n",
    "\n",
    "Linear penalty (hinge loss) for a sample if it is misclassified or lied in the margin:\n",
    "- Tries to maintain $\\xi_n$ small while maximizing the margin.\n",
    "- Always finds a solution (as opposed to hard-margin SVM)\n",
    "- More robust to the outliers\n",
    "\n",
    "Soft margin problem is still a convex QP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220c1fd",
   "metadata": {},
   "source": [
    "#### **Cost Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e5cf6",
   "metadata": {},
   "source": [
    "**Soft Margin Formula:**\n",
    "$$\\min_{w,w_0}\\frac{1}{2}||w||^2 + C\\sum_{n=1}^N\\xi_n$$\n",
    "$$\\text{s.t. } y^{(n)}(w^Tx^{(n)} + w_0) \\ge 1 - \\xi_n \\quad n = 1, 2, ..., N$$\n",
    "$$\\xi_n \\ge 0$$\n",
    "\n",
    "Slack variable constraints:\n",
    "$$\\xi_n \\ge 1 - y^{(n)}(w^Tx^{(n)} + w_0)$$\n",
    "$$\\xi_n \\ge 0$$\n",
    "So:\n",
    "$$\\min_{\\xi_n} = \\max(0, 1 - y^{(n)}(w^Tx^{(n)} + w_0))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d917f",
   "metadata": {},
   "source": [
    "Unconstrained optimization problem:\n",
    "$$\\min_{w,w_0} \\left(\\underbrace{\\frac{1}{2}||w||^2}_{\\text{regularization}} + C\\sum_{n=1}^N \\underbrace{\\max(0, 1 - y^{(n)}(w^Tx^{(n)} + w_0))}_{\\text{Hinge Loss}} \\right)$$\n",
    "\n",
    "*Hinge Loss* vs. *0-1 Loss*:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/hingeLoss.png\" alt=\"Hinge Loss\">\n",
    "</div>\n",
    "\n",
    "**Key Results:**\n",
    "- Smaller loss for points where correctly classified but inside margin\n",
    "- Larger loss for misclassified points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6e392",
   "metadata": {},
   "source": [
    "#### **Dual Optimization Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20885dca",
   "metadata": {},
   "source": [
    "**Recall:**\n",
    "\n",
    "- **Soft Margin Formula:**\n",
    "    $$\\min_{w,w_0}\\frac{1}{2}||w||^2 + C\\sum_{n=1}^N\\xi_n$$\n",
    "    $$\\text{s.t. } 1 - \\xi_n - y^{(n)}(w^Tx^{(n)} + w_0) \\le 0 \\quad n = 1, 2, ..., N$$\n",
    "    $$-\\xi_n \\le 0$$\n",
    "\n",
    "- **Lagrange Formulation**\n",
    "    $$p^* = \\min_x f(x)$$\n",
    "    $$\\text{s.t. } g_i(x) \\le 0 \\quad i=1,...,m$$\n",
    "    Lagrangian function:\n",
    "    $$L(x, \\alpha, \\lambda) = f(x) + \\sum_{i=1}^m \\alpha_i g_i(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef82a8f",
   "metadata": {},
   "source": [
    "**Soft Margin Lagrangian Formulation:**\n",
    "$$L(w, w_0, \\xi, \\alpha, \\beta) = \\frac{1}{2}||w||^2 + C\\sum_{n=1}^N\\xi_n + \\sum_{n=1}^N \\alpha_n(1 - \\xi_n - y^{(n)}(w^Tx^{(n)} + w_0)) - \\sum_{n=1}^N \\beta_n \\xi_n$$\n",
    "Subject to:\n",
    "$$\\alpha_n \\ge 0$$\n",
    "$$\\beta_n \\ge 0$$\n",
    "\n",
    "Expand the formulation:\n",
    "$$L(w, w_0, \\xi, \\alpha, \\beta) = \\frac{1}{2}||w||^2 + C\\sum_{n=1}^N\\xi_n + \\sum_{n=1}^N \\alpha_n - \\sum_{n=1}^N \\alpha_n \\xi_n - \\sum_{n=1}^N \\alpha_n y^{(n)} w^Tx^{(n)} - \\sum_{n=1}^N \\alpha_n y^{(n)} w_0 - \\sum_{n=1}^N \\beta_n \\xi_n$$\n",
    "\n",
    "- Minimize w.r.t. $w, w_0, \\xi$\n",
    "- Maximize w.r.t. $a_n \\ge 0$ and $\\beta_n \\ge 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e669e85",
   "metadata": {},
   "source": [
    "**Optimality Conditions:**\n",
    "- Gradient w.r.t. $w$:\n",
    "    $$\\nabla_w L(w, w_0, \\xi, \\alpha, \\beta) = 0 \\implies w - \\sum_{n=1}^N \\alpha_n y^{(n)} x^{(n)} = 0$$\n",
    "    $$w = \\sum_{n=1}^N \\alpha_n y^{(n)} x^{(n)}$$\n",
    "\n",
    "- Derivate w.r.t. $w_0$:\n",
    "    $$\\frac{\\partial L(w, w_0, \\xi, \\alpha, \\beta)}{\\partial w_0} = 0 \\implies -\\sum_{n=1}^N \\alpha_n y^{(n)} = 0$$\n",
    "\n",
    "- Derivate w.r.t. $\\xi$\n",
    "    $$\\frac{\\partial L(w, w_0, \\xi, \\alpha, \\beta)}{\\partial \\xi} = 0 \\implies \\sum_{n=1}^N C - \\alpha_n - \\beta_n = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79b207",
   "metadata": {},
   "source": [
    "**Substitute in lagrange function:**\n",
    "$$L(w, w_0, \\xi, \\alpha, \\beta) = \\frac{1}{2}||w||^2  + \\sum_{n=1}^N (C + \\alpha_n - \\beta_n) \\xi_n + \\sum_{n=1}^N \\alpha_n(1 - y^{(n)}(w^Tx^{(n)} + w_0))$$\n",
    "\n",
    "**Key Properties:**\n",
    "- Because of $(C + \\alpha_n - \\beta_n) = 0$, the lagrange function is the same as hard margin problem.\n",
    "- Since there is no $\\beta$ and $xi$ in the function change last constraint to:\n",
    "    $$C - \\alpha_n = \\beta_n \\implies C - \\alpha_n \\ge 0$$\n",
    "- Because of\n",
    "    $$ \\begin{cases}\n",
    "    \\alpha_n \\le C \\\\\n",
    "    \\alpha_n \\ge 0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    parameter $0 \\le \\alpha_n \\ge C$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3ce15",
   "metadata": {},
   "source": [
    "**Dual optimization Problem:**\n",
    "    $$\\max_{\\alpha} \\left\\{\\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {x^{(n)}}^T x^{(m)}\\right\\}$$\n",
    "    $$\\text{s.t. } \\sum_{n=1}^N \\alpha_n y^{(n)} = 0$$\n",
    "    $$0 \\le \\alpha_n \\le C \\quad n = 1, ..., N$$\n",
    "\n",
    "After solving the above QP, $w$ is find as:\n",
    "$$w = \\sum_{n=1}^N \\alpha_n y^{(n)} x^{(n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26667e4",
   "metadata": {},
   "source": [
    "#### **Support Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6ca65",
   "metadata": {},
   "source": [
    "**Dual optimization Problem:**\n",
    "    $$\\max_{\\alpha} \\left\\{\\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {x^{(n)}}^T x^{(m)}\\right\\}$$\n",
    "    $$\\text{s.t. } \\sum_{n=1}^N \\alpha_n y^{(n)} = 0$$\n",
    "    $$0 \\le \\alpha_n \\le C \\quad n = 1, ..., N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48166b34",
   "metadata": {},
   "source": [
    "**Support Vectors:** $\\alpha_n \\gt 0$\n",
    "- If $0 \\lt \\alpha_n \\lt C$ (Margin support vector):\n",
    "    - **SVs on the margin**\n",
    "    $$y^{(n)}(w^Tx^{(n)} + w_0) = 1$$\n",
    "    $$\\xi_n = 0$$\n",
    "\n",
    "- If $\\alpha_n = C$ (Non-margin support vector):\n",
    "    - **SVs on or over the margin**\n",
    "    $$y^{(n)}(w^Tx^{(n)} + w_0) \\lt 1$$\n",
    "    $$\\xi_n \\gt 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dc01f",
   "metadata": {},
   "source": [
    "#### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7b65d",
   "metadata": {},
   "source": [
    "- **Hard margin**: maximizing **margin**\n",
    "\n",
    "- **Soft margin**: handling noisy data and overlapping classes\n",
    "    - Slack variables in the problem\n",
    "\n",
    "- **Dual problems** of hard-margin and soft-margin SVM using **Lagrange function**\n",
    "    - Classifier decision in terms of **support vectors**\n",
    "\n",
    "- Dual problems lead us to **non-linear** SVM method easily by **kernel substitution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f96463",
   "metadata": {},
   "source": [
    "### **Non-Linear SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498c01c",
   "metadata": {},
   "source": [
    "#### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3169fd8",
   "metadata": {},
   "source": [
    "**Not Linearly Separable Data:**\n",
    "- Noisy data or overlapping classes *(discussed in soft margin)*\n",
    "- Non-linear decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a1db2",
   "metadata": {},
   "source": [
    "Assume a transformation $\\phi: R^d \\rightarrow R^m$ on the feature space:\n",
    "$$x \\rightarrow \\phi(x)$$\n",
    "Where:\n",
    "- $\\phi(x) = [\\phi_1(x), ..., \\phi_m(x)]$: set of basis functions\n",
    "- $\\phi_i(x): R^d \\rightarrow R$\n",
    "\n",
    "The goal is to find a hyper-plane in the transformed feature space:\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/nonLinearSVM.png\" alt=\"non-linear svm\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a8c2a",
   "metadata": {},
   "source": [
    "#### **Primal Problem vs. Dual Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672993cc",
   "metadata": {},
   "source": [
    "##### **Primal Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf169e",
   "metadata": {},
   "source": [
    "**Optimization Problem:**\n",
    "$$\\min_{w_0, w}\\frac{1}{2}||w||^2 + C \\sum_{n=1}^N \\xi_n$$\n",
    "$$\\text{s.t.} \\quad y^{(n)}(w^T x^{(n)} + w_0) \\ge 1 - \\xi_n \\quad n = 1, ..., N$$\n",
    "$$\\xi_n \\ge 0$$\n",
    "\n",
    "**Key Properties:**\n",
    "- $w \\in R^m$: the weights that must be found\n",
    "- If $m \\gg d$ (very high dimensional feature space) then there are many more parameters to learn\n",
    "- Length of weights set will grows with feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7263878",
   "metadata": {},
   "source": [
    "##### **Dual Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2c213",
   "metadata": {},
   "source": [
    "**Optimization Problem:**\n",
    "$$\\max_{\\alpha} \\left\\{\\sum_{n=1}^N a_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N a_n^T a_m y^{(n)} y^{(m)} {\\phi(x^{(n)})}^T \\phi(x^{(m)})\\right\\}$$\n",
    "$$\\text{s.t.} \\quad \\sum_{n=1}^N \\alpha_n y^{(n)} = 0$$\n",
    "$$0 \\le \\alpha_n \\le C \\quad n = 1, ..., N$$\n",
    "\n",
    "**Key Properties:**\n",
    "- We have inner products ${\\phi(x^{(n)})}^T \\phi(x^{(m)})$, only $\\alpha = [\\alpha_1, ..., \\alpha_N]$ needs to learn.\n",
    "    - No need to learn $m$ parameters as opposed to the *primal solution*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfd32a",
   "metadata": {},
   "source": [
    "**Classifying a New Data**\n",
    "$$\\hat{y} = \\text{sign}(w_0 + w^T\\phi(x))$$\n",
    "Where:\n",
    "$$w = \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} \\phi(x^{(n)})$$\n",
    "$$w_0 = y^{(s)} - w^T \\phi(x^{(s)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cceb25",
   "metadata": {},
   "source": [
    "#### **Kernel SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79af881",
   "metadata": {},
   "source": [
    "##### **Core Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03604ffc",
   "metadata": {},
   "source": [
    "Learns **linear decision boundary** in a **high dimension** space **without** explicitly working on the **mapped data**.\n",
    "\n",
    "Let ${\\phi(x)}^T \\phi(x') = K(x, x')$ (Kernel)\n",
    "\n",
    "**Optimization Problem:**\n",
    "$$\n",
    "\\max_{\\alpha} \\left\\{ \\sum_{n=1}^{N} \\alpha_n - \\frac{1}{2} \\sum_{n=1}^{N} \\sum_{m=1}^{N} \\alpha_n \\alpha_m y^{(n)} y^{(m)} K(x^{(n)}, x^{(m)}) \\right\\}\n",
    "$$\n",
    "\n",
    "Constraints:\n",
    "- $$\\sum_{n=1}^{N} \\alpha_n y^{(n)} = 0$$\n",
    "- $$0 \\leq \\alpha_n \\leq C \\quad \\forall n \\in \\{1,...,N\\}$$\n",
    "\n",
    "**Kernel Matrix (Q)**\n",
    "$$\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)} y^{(1)} K(x^{(1)}, x^{(1)}) & \\cdots & y^{(1)} y^{(N)} K(x^{(1)}, x^{(N)}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(N)} y^{(1)} K(x^{(N)}, x^{(1)}) & \\cdots & y^{(N)} y^{(N)} K(x^{(N)}, x^{(N)}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c65d4",
   "metadata": {},
   "source": [
    "Example: $x = [x_1, x_2]$ and second-order $\\phi$:\n",
    "- with transforming $x$ and $x'$:\n",
    "    $$\\phi(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1x_2]$$\n",
    "    $$\\phi(x') = [1, x_1', x_2', x_1'^2, x_2'^2, x_1'x_2']$$\n",
    "    Kernel is:\n",
    "    $$K(x, x') = 1 + x_1 x_1' + x_2 x_2' + x_1^2 x_1'^2 + x_2^2 x_2'^2 + x_1 x_1' x_2 x_2'$$\n",
    "\n",
    "- without transforming $x$ and $x'$:\n",
    "    - Consider $K(x, x') = (1 + x^Tx')^2$:\n",
    "    $$K(x, x') = (1 + x^Tx')^2 = (1 + x_1 x_1' + x_2 x_2')^2 = 1 + 2x_1 x_1' + 2x_2 x_2' + x_1^2 x_1'^2 + x_2^2 x_2'^2 + 2 x_1 x_1' x_2 x_2'$$\n",
    "\n",
    "    This is an inner product in:\n",
    "    $$\\phi(x) = [1, \\sqrt{2} x_1, \\sqrt{2} x_2, x_1^2, x_2^2, \\sqrt{2}x_1x_2]$$\n",
    "    $$\\phi(x') = [1, \\sqrt{2} x'_1, \\sqrt{2} x_2', x_1'^2, x_2'^2, \\sqrt{2}x_1'x_2']$$\n",
    "\n",
    "**Results (Polynomial Kernel: Degree 2):**\n",
    "- We instead use $k(x, x')$ = $(1 + x^Tx')^2$ that corresponds to:  \n",
    "    $d$-dimensional feature space $x = [x_1, ..., x_d]^T$\n",
    "    $$\\phi(x) = [1, \\sqrt{2} x_1, ..., \\sqrt{2} x_d, x_1^2, ..., x_d^2, \\sqrt{2}x_1x_2, ..., \\sqrt{2}x_1x_d, \\sqrt{2}x_2x_3, ..., \\sqrt{2}x_{d-1}x_d]^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf84028",
   "metadata": {},
   "source": [
    "**Why Kernel?**\n",
    "\n",
    "Given:\n",
    "- $d$: Dimensionality of the input.\n",
    "- $M$: Order of polynomials.\n",
    "- $K$: Kernel function\n",
    "\n",
    "$$\n",
    "\\text{Complexity order} = \n",
    "\\begin{cases}\n",
    "O(d^M) &  \\text{not using kernel} \\\\\n",
    "O(d) &  \\text{using kernel}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Example:** Consider the second-order polynomial transform:\n",
    "$$\\phi(x) = [1, x_1, ..., x_d, x_1^2, ..., x_d^2, x_1x_2, ..., x_1x_d, x_2x_3, ..., x_{d-1}x_d]^T \\quad (m = 1 + d + d^2)$$\n",
    "- Without using Kernel:\n",
    "    $$\\phi(x)^T\\phi(x') = 1 + \\sum_{i=1}^d x_i x_i' + \\sum_{i=1}^d \\sum_{j=1}^d x_i x_j x_i' x_j' \\quad O(m)$$\n",
    "\n",
    "- With Kernel:\n",
    "    $$\\phi(x)^T\\phi(x') = 1 + (x^Tx') + (x^Tx')^2 \\quad O(d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6f714",
   "metadata": {},
   "source": [
    "**Classifying a New Data**\n",
    "\n",
    "**Primal**\n",
    "$$\\hat{y} = \\text{sign} (w_0 + w^T \\phi(x))$$\n",
    "Where:\n",
    "- $$w = \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} \\phi(x^{(n)})$$\n",
    "- $$w_0 = y^{(s)} - w^T \\phi(x^{(s)})$$\n",
    "<br><br>\n",
    "**Covert to Dual:**\n",
    "$$\\hat{y} = \\text{sign} (w_0 + \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} k(x^{(n)}, x))$$\n",
    "Where:\n",
    "- $$w_0 = y^{(s)} - \\sum_{\\alpha_n \\gt 0} \\alpha_n y^{(n)} k(x^{(n)}, x^{(s)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed012e",
   "metadata": {},
   "source": [
    "**Common Kernel Functions**\n",
    "\n",
    "- **Linear:** $k(x, x') = x^Tx'$\n",
    "\n",
    "- **Polynomial:** $k(x, x') = (1 + x^Tx')^M$\n",
    "\n",
    "- **Gaussian:** $k(x, x') = e^{(-\\frac{||x - x'||^2}{\\gamma})}$\n",
    "\n",
    "- **Sigmoid:** $k(x, x') = tanh(ax^Tx' + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e470ef",
   "metadata": {},
   "source": [
    "##### **Polynomial Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd0e4c",
   "metadata": {},
   "source": [
    "Earlier we showed for polynomial with degree 2, we have:\n",
    "$$K(x, x') = (1 + x^Tx')^2 = (1 + x_1 x_1' + x_2 x_2')^2$$\n",
    "\n",
    "This can similarly be generalized to:\n",
    "- $d$-dimension $x$\n",
    "- $\\phi$ s are polynomials of order $M$:\n",
    "$$K(x, x') = (1 + x^Tx')^M = (1 + x_1 x_1' + x_2 x_2', ..., x_d x_d')^M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd1e07",
   "metadata": {},
   "source": [
    "**Example:** SVM boundary for polynomial kernel\n",
    "\n",
    "Primal problem:\n",
    "    $$w_0 + w^T \\phi(x) = 0$$\n",
    "Dual problem:\n",
    "    $$w_0 + \\sum_{\\alpha_i \\gt 0} \\alpha_i y^{(i)} \\phi(x^{(i)})^T \\phi(x)$$\n",
    "Since $K(x^{(i)}, x) = \\phi(x^{(i)})^T \\phi(x)$, then:\n",
    "    $$w_0 + \\sum_{\\alpha_i \\gt 0} \\alpha_i y^{(i)} K(x^{(i)}, x)$$\n",
    "For polynomial kernel $K(x, x') = (1 + x^Tx')^M$, then:\n",
    "    $$w_0 + \\sum_{\\alpha_i \\gt 0} \\alpha_i y^{(i)} (1 + x^Tx')^M$$\n",
    "\n",
    "**Result:** Boundary is a polynomial of order $M$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61465bfc",
   "metadata": {},
   "source": [
    "##### **Gaussian (RBF) Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec946a",
   "metadata": {},
   "source": [
    "The RBF kernel corresponds to an inner product in a space spanned by **infinitely** many basis functions (via **Taylor expansion**).\n",
    "\n",
    "$$K(x, x') = e^{(-\\frac{||x - x'||^2}{\\gamma})}$$\n",
    "Take one dimensional case with $\\gamma = 1$:\n",
    "$$K(x, x') = e^{||x - x'||^2} = - e^{-x^2} e^{-x'^2} e^{2xx'}$$\n",
    "\n",
    "Taylor Expansion of cross-term ($e^{2xx'}$):\n",
    "$$\\sum_{k=1}^{\\infty} \\frac{2^k x^k x'^k}{k!}$$\n",
    "\n",
    "So:\n",
    "$$\\phi(x) = e^{-x^2} [x^0, \\frac{\\sqrt{2^0}}{0!}x, \\frac{\\sqrt{2^1}}{1!}x^2, ...]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e83d7",
   "metadata": {},
   "source": [
    "**Decision Boundary:**\n",
    "$$w_0 + \\sum_{\\alpha_i \\gt 0} \\alpha_i y^{(i)} e^{(-\\frac{||x - x^{(i)}||^2}{\\sigma})} = 0$$\n",
    "\n",
    "Where:\n",
    "$$w_0 = y^{(s)} - \\sum_{\\alpha_i \\gt 0} \\alpha_i y^{(i)} e^{(-\\frac{||x^{(i)} - x^{(s)}||^2}{\\sigma})}$$\n",
    "\n",
    "Since Gaussian kernel operates in an **infinite-dimensional** feature space, can classify any arbitrary training set with **no training error**:\n",
    "- Training error becomes zero when $\\sigma \\rightarrow 0$\n",
    "- All samples become support vector (overfitting)\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"images/RBF.png\" alt=\"RBF\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ca30c",
   "metadata": {},
   "source": [
    "#### **Kernel Trick**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866e061",
   "metadata": {},
   "source": [
    "##### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ee3ba",
   "metadata": {},
   "source": [
    "**Kernel Trick:**\n",
    "- A technique to extend linear algorithms to non-linear versions by implicitly mapping data to high-dimensional spaces.\n",
    "- Replace the inner product of $x$ and $x'$ in the transformed space with the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc56469",
   "metadata": {},
   "source": [
    "Use when **input vectors** appear **only** as **dot products** in the algorithm.\n",
    "\n",
    "Advantages:\n",
    "- Solving the problem without explicitly mapping the data\n",
    "- Explicit mapping is expensive if $\\phi(x)$ is very high dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc2988",
   "metadata": {},
   "source": [
    "##### **Valid Kernels: Construction & Verification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca1efd",
   "metadata": {},
   "source": [
    "For constructing kernel function directly, we need to ensure it is a **valid kernel**.\n",
    "\n",
    "**Valid Kernel:** Kernel matrix, is as an inner product matrix in some space.\n",
    "\n",
    "**Example:**\n",
    "$$k(x, x') = (1 + x^Tx')^2$$\n",
    "Corresponding Mapping for $x = [x_1, x_2]^T$:\n",
    "$$\\phi(x) = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]^T$$\n",
    "\n",
    "We need a way to test whether a kernel is valid without having to construct $\\phi(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53d416",
   "metadata": {},
   "source": [
    "**Necessary & sufficient conditions**\n",
    "\n",
    "**Gram Matrix**  \n",
    "For any dataset $\\{x^{(1)}, ..., x^{(N)}\\}$ The Gram matrix $K_{N \\times N}: K_{ij} = k(x^{(i)}, x^{(j)})$ must be:\n",
    "\n",
    "$$\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "k(x^{(1)}, x^{(1)}) & \\cdots & k(x^{(1)}, x^{(N)}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(x^{(N)}, x^{(1)}) & \\cdots & k(x^{(N)}, x^{(N)}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Mercer Theorem:**  \n",
    "A function $K$ is a valid kernel iff:\n",
    "- It is **symmetric**.\n",
    "- For all finite datasets, $K$ is **Positive Semi-Definite (PSD)**.\n",
    "\n",
    "**Mercer Conditions:**\n",
    "- **Symmetry:**\n",
    "    $$k(x, x') = \\phi(x)^T\\phi(x') = \\phi(x')^T\\phi(x) = k(x', x)$$\n",
    "- **Positive Semi-Definiteness:**\n",
    "    - The Gram matrix â€‹$K$ must satisfy:\n",
    "    $$c^T K c \\ge 0 \\quad \\forall c \\in R^N$$\n",
    "    - If the Gram matrix $K$ can be factored as $K = BB^T$, then $K$ is positive semi-definite (PSD).\n",
    "        - Proof:  \n",
    "        for any real vector $v \\neq 0$\n",
    "        $$v^T K v = v^T (B B^T) v = (B^T v)^T (B^T v) = ||B^T v||^2 \\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dfd825",
   "metadata": {},
   "source": [
    "##### **Example: Minimum Distance Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6af951",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "Assign a point $x$ to class $C_1$ if its Euclidean distance to $\\mu_1$ (mean of $C_1$) is smaller than to $\\mu_2$ (mean of $C_2$):\n",
    "$$||x - \\mu_1|| \\lt ||x - \\mu_2|| \\implies x \\in C_1$$\n",
    "Where:\n",
    "$$\\mu_1 = \\frac{\\sum_{y^{(i)} = 1} x^{(i)}}{N_1}, \\quad \\mu_2 = \\frac{\\sum_{y^{(i)} = 2} x^{(i)}}{N_2}$$\n",
    "\n",
    "**Kernel-Based Solution:**\n",
    "$$(x - \\mu_1)^T(x - \\mu_1) \\lt (x - \\mu_2)^T(x - \\mu_2)$$\n",
    "$$x^Tx - 2x^T\\mu_1 + \\mu_1^T\\mu_1 \\lt x^Tx - 2x^T\\mu_2 + \\mu_2^T\\mu_2$$\n",
    "\n",
    "Substitute Class Means:\n",
    "$$-2 \\frac{\\sum_{y^{(n)} = 1} x^T x^{(n)}}{N_1} + \\frac{\\sum_{y^{(n)} = 1} \\sum_{y^{(m)} = 1} {x^{(n)}}^T x^{(m)}}{{N_1}^2} \\lt -2 \\frac{\\sum_{y^{(n)} = 2} x^T x^{(n)}}{N_2} + \\frac{\\sum_{y^{(n)} = 2} \\sum_{y^{(m)} = 2} {x^{(n)}}^T x^{(m)}}{{N_2}^2}$$\n",
    "\n",
    "Replace inner products with kernel function:\n",
    "$$-2 \\frac{\\sum_{y^{(n)} = 1} K(x, x^{(n)})}{N_1} + \\frac{\\sum_{y^{(n)} = 1} \\sum_{y^{(m)} = 1} K(x^{(n)}, x^{(m)})}{{N_1}^2} \\lt -2 \\frac{\\sum_{y^{(n)} = 2} K(x, x^{(n)})}{N_2} + \\frac{\\sum_{y^{(n)} = 2} \\sum_{y^{(m)} = 2} K(x^{(n)}, x^{(m)})}{{N_2}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e46cd8",
   "metadata": {},
   "source": [
    "**Applications:**\n",
    "\n",
    "- We know all pairwise distances (include distance of points from center of mass of a set)\n",
    "- Many *dimensionality reduction*, *clustering*, and *classification methods* rely on **pairwise distances**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec8abb",
   "metadata": {},
   "source": [
    "##### **Example: Kernel Ridge Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff2bcf",
   "metadata": {},
   "source": [
    "**Ridge Regression Optimization Problem:**\n",
    "$$\\min_w \\left(\\sum_{n=1}^N (w^T x^{(n)} - y^{(n)})^2 + \\lambda w^T w \\right)$$\n",
    "Equivalently:\n",
    "$$\\nabla_w J(w) = 0 \\implies \\sum_{n=1}^N 2 x^{(n)} (w^T x^{(n)} - y^{(n)}) + 2 \\lambda w = 0$$\n",
    "$$w = \\sum_{n=1}^N -\\frac{1}{\\lambda} (w^Tx^{(n)} - y^{(n)}) x^{(n)}$$\n",
    "Let $\\alpha_n = -\\frac{1}{\\lambda} (w^Tx^{(n)} - y^{(n)})$, we have:\n",
    "$$w = \\sum_{n=1}^N \\alpha_n x^{(n)}$$\n",
    "After transform to new space:\n",
    "$$w = \\sum_{n=1}^N \\alpha_n \\phi(x^{(n)}) = \\Phi^T \\alpha$$\n",
    "\n",
    "Replace $w$ in primal problem. Dual representation:\n",
    "$$J(w) = \\alpha^T \\phi \\phi^T \\phi \\phi^T \\alpha - 2 \\alpha^T \\phi \\phi^T y + y^T y + \\lambda \\alpha^T \\phi \\phi^T \\alpha$$\n",
    "Replace inner products with kernel function:\n",
    "$$J(\\alpha) = \\alpha^T K K \\alpha - 2 \\alpha^T K y + y^T y + \\lambda \\alpha^T K \\alpha$$\n",
    "$$\\nabla_{\\alpha}J(\\alpha) = 0 \\implies \\alpha = (K + \\lambda I_N)^{-1}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe31ce4",
   "metadata": {},
   "source": [
    "**Prediction for new $x$**\n",
    "\n",
    "For a new input $x$, the predicted output $f(x)$ is computed as:\n",
    "$$f(x) = w^T\\phi(x)$$\n",
    "\n",
    "Weight Vector in Dual Form:\n",
    "$$w = \\Phi^T \\alpha = \\sum_{n=1}^N \\alpha_n \\phi(x^{(n)})$$\n",
    "With the design matrix:\n",
    "$$\n",
    "\\Phi = \\begin{bmatrix}\n",
    "\\phi(x^{(1)})^T \\\\ \n",
    "\\vdots \\\\ \n",
    "\\phi(x^{(N)})^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Kernel Trick Application:\n",
    "$$f(x) = \\sum_{n=1}^N \\alpha_n K(x^{(n)}, x) = K(x^{(n)}, x)^T \\alpha$$\n",
    "$$\n",
    "f(x) = \n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "K(x^{(1)}, x) \\\\\n",
    "\\vdots \\\\\n",
    "K(x^{(N)}, x)\n",
    "\\end{bmatrix}^T\n",
    "}_{\\text{Kernel vector}}\n",
    "\\underbrace{\n",
    "(K + \\lambda I_N)^{-1} y\n",
    "}_{\\text{Trained coefficients}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc7ab06",
   "metadata": {},
   "source": [
    "**Kernel vs. Non-Kernel Solutions**\n",
    "\n",
    "**With kernel:**\n",
    "- **Matrix Size:**  \n",
    "    Operates on an $N \\times N$ kernel matrix $K$, where $K_{ij} = k(x^{(i)}, x^{(j)})$ for $N$ samples.\n",
    "\n",
    "- **Solution Form:**\n",
    "    $$\\alpha = (K + \\lambda I_N)^{-1}y$$\n",
    "    Here, $\\lambda I_N$ ensures numerical stability (ridge regularization).\n",
    "\n",
    "- **When to Use:**  \n",
    "    - High-dimensional data ($d \\gg N$), as $N \\times N$ is smaller than $d \\times d$.  \n",
    "    - Non-linear patterns, since kernels implicitly map to high-D spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dd235",
   "metadata": {},
   "source": [
    "**Without Kernel:**\n",
    "- **Matrix Size:**  \n",
    "    Involves a $d \\times d$ matrix $XX^T$ for $d$-dimensional features.        \n",
    "- **Solution Form:**\n",
    "    $$\\alpha = (XX^T + \\lambda I_d)^{-1}y$$\n",
    "- **When to Use:**\n",
    "    - Low-dimensional data ($d \\ll N$), where $d \\times d$ is tractable.\n",
    "    - Linear problems, as no kernel mapping is needed.\n",
    "\n",
    "In classification (e.g., SVM), many $\\alpha_i=0$ (support vectors only).  \n",
    "In regression usually $d \\ll N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cb142",
   "metadata": {},
   "source": [
    "##### **Kernel for Structured Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9dd60",
   "metadata": {},
   "source": [
    "Kernels can operate on **any data type** (graphs, strings, trees, etc.) as long as we define a valid similarity measure $K(x, xâ€²)$ that is:\n",
    "- **Symmetric**\n",
    "- **Positive Semi-Definite (PSD)**\n",
    "\n",
    "Use kernel-based version of classical learning algorithms for recognition of **structured data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7a239",
   "metadata": {},
   "source": [
    "**Why Inner Products (kernel) $\\equiv$ Similarity:**\n",
    "\n",
    "For vectors: $K(x, x') = x^T x' = \\|x\\| \\|x'\\| \\cos(\\theta)$  \n",
    "(Cosine similarity normalized by magnitudes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed717f1",
   "metadata": {},
   "source": [
    "**Example: Kernel Function for Objects**\n",
    "\n",
    "**Strings** (Sets): The inner product of the feature vectors for two strings can be defined as\n",
    "$$k(A, B) = 2^{|A \\cap B|}$$\n",
    "Sum over all common subsequences weighted according to their frequency of occurrence and lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295169e",
   "metadata": {},
   "source": [
    "##### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b27673",
   "metadata": {},
   "source": [
    "**Key Properties of Kernel Methods**\n",
    "\n",
    "- **Implicit Feature Space Operations:**\n",
    "    - Operating in the mapped space **without ever computing** the coordinates of the data in that space\n",
    "    \n",
    "    - Mathematically:\n",
    "        $$f(x)=\\sum_{n=1â€‹}^N \\alpha_n â€‹K(x^{(n)} ,x)$$\n",
    "        where $K(x^{(i)}, x) = \\langle \\phi(x^{(i)}), \\phi(x) \\rangle$ avoids explicit $\\phi(x)$ calculation.\n",
    "\n",
    "- **Structured Data Kernels:**\n",
    "    - Besides vectors, kernels can be defined for non-vectorial (structured) data like (graphs, strings, etc.)\n",
    "\n",
    "- **Geometry Through Dot Products:**\n",
    "    - Cosine similarity $\\propto K(x, x')$ when $|\\phi(x)| = 1$.\n",
    "\n",
    "- **Efficient Inner Products:**\n",
    "    - Inner products in high-D spaces can be computed efficiently via kernels:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
