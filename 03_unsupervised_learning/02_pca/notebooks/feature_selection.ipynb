{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac94554e",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f60c4",
   "metadata": {},
   "source": [
    "**Avoiding Overfitting**\n",
    "\n",
    "To prevent overfitting, several techniques are employed:\n",
    "- Structural Risk Minimization\n",
    "- Regularization\n",
    "- Cross-Validation\n",
    "    - Model Selection\n",
    "- **Feature Selection**\n",
    "- **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedbc67",
   "metadata": {},
   "source": [
    "**Feature Selection vs. Feature Extraction**\n",
    "\n",
    "- Feature **Selection:**\n",
    "    - Select a **subset** from a original set\n",
    "    - Filter methods, wrapper methods, embedded methods\n",
    "\n",
    "- Feature **Extraction:**\n",
    "    - Applies a **transformation** to project features into a lower-dimensional space\n",
    "    - PCA (Principal Component Analysis), LDA (Linear Discriminant Analysis), etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7c945",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34778549",
   "metadata": {},
   "source": [
    "**Challenges in Real-World Data**\n",
    "\n",
    "Datasets frequently contain:\n",
    "- **Irrelevant features:**  \n",
    "    Variables that provide no predictive signal (e.g., noise).\n",
    "\n",
    "- **Redundant features:**  \n",
    "    Highly correlated or duplicate measurements.\n",
    "\n",
    "- **High-dimensionality:**  \n",
    "    Number of features $d$ is very large (perhaps $d \\gg n$), common in:\n",
    "    - Text classification (e.g., bag-of-words representations).\n",
    "    - Genomics (e.g., gene expression micro-arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e911d0",
   "metadata": {},
   "source": [
    "**Benefits of Feature Selection**\n",
    "\n",
    "**Feature Selection** is way to find **more accurate**, **faster**, and **easier to understand** classifiers.\n",
    "\n",
    "**Effects:**\n",
    "- **Improve Model Performance**\n",
    "    - Reduces overfitting by eliminating noise\n",
    "    - Mitigates the *curse of dimensionality*\n",
    "    - Fewer features improve the sample-to-parameter ratio, enhancing generalization\n",
    "\n",
    "- **Efficiency:**\n",
    "    - Lowers memory usage, training time, and inference cost\n",
    "    \n",
    "- **Interpretability:**\n",
    "    - Simplifies model explanations\n",
    "    - Critical in domains like healthcare or finance\n",
    "\n",
    "- **Noise Reduction:**\n",
    "    - Directly improves test accuracy by removing non-informative features\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/noise_feature.png\" alt=\"Noise Feature Example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c891b47",
   "metadata": {},
   "source": [
    "**Supervised Feature Selection:**\n",
    "\n",
    "Given:\n",
    "- A feature matrix $X \\in R^{N \\times d}$ *(where $N$ is number of samples, $d$ is number of features)*\n",
    "- A label vector $Y \\in R^N$.\n",
    "\n",
    "Feature selection outputs a subset of indices $\\{i_1, i_2, \\ldots, i_{d'}\\}$ (with $d' \\ll d$) representing the most discriminative features.\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} \n",
    "x_1^{(1)} & \\cdots & x_d^{(1)} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "x_1^{(N)} & \\cdots & x_d^{(N)} \n",
    "\\end{bmatrix}, \n",
    "\\quad \n",
    "Y = \\begin{bmatrix} \n",
    "y^{(1)} \\\\ \n",
    "\\vdots \\\\ \n",
    "y^{(N)} \n",
    "\\end{bmatrix} \\quad \\xrightarrow{\\text{Feature Selection}} \\quad \\{i_1, i_2, \\ldots, i_{d'}\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573bb6c",
   "metadata": {},
   "source": [
    "# Filter: Univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3ffdb",
   "metadata": {},
   "source": [
    "**Univariate Method:**  \n",
    "Consider one variable *(feature)* at a time\n",
    "\n",
    "**Filter Method:**  \n",
    "Ranks features or feature subsets independent of the classifier as a preprocessing step\n",
    "\n",
    "**Univariate Filter Method:**\n",
    "- Score the each feature $k$ based on the $k$-th column of the data matrix and the label vector\n",
    "    - Score: relevance of the feature to predict labels: Can the feature discriminate the patterns of different classes?\n",
    "\n",
    "- Rank features according to their score values and select the ones with the highest scores.\n",
    "    - Use *cross-validation* to select among the possible values of the $k$\n",
    "\n",
    "**Advantage:**\n",
    "- Computational & statistical scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc8c51",
   "metadata": {},
   "source": [
    "## Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82763070",
   "metadata": {},
   "source": [
    "### **Pearson Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5b294",
   "metadata": {},
   "source": [
    "**Formula:**\n",
    "$$\n",
    "R(k) = \\frac{\\text{cov}(X_k, Y)}{\\sqrt{\\text{var}(X_k)} \\sqrt{\\text{var}(Y)}} \\approx \\frac{\\sum_{i=1}^N (x_k^{(i)} - \\bar{x_k})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_k^{(i)} - \\bar{x_k})^2} \\sqrt{\\sum_{i=1}^N (y^{(i)} - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "**Goal:** Find maximum $R(k)$ between $k$-th attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcac16b",
   "metadata": {},
   "source": [
    "**Limitation:**  \n",
    "Covariance can detect linear dependence and it cannot perform as we expect at non-linear dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa96e7",
   "metadata": {},
   "source": [
    "### **Mutual Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b773d8",
   "metadata": {},
   "source": [
    "**Recall: Mutual Information**\n",
    "$$\n",
    "MI(X, Y) = H(Y) - H(Y | X) = \\sum_{i} \\sum_{j} p(x_i, y_j) \\left(\\log{\\frac{p(x_i, y_j)}{p(x_i)p(y_j)}} \\right)\n",
    "$$\n",
    "Where:\n",
    "- Marginal Entropy:\n",
    "    $$\n",
    "    H(Y) = -\\sum_{j} p(y_j) \\log{p(y_j)}\n",
    "    $$\n",
    "\n",
    "- Conditional Entropy:\n",
    "    $$\n",
    "    H(Y | X) = -\\sum_{i} \\sum_{j} p(x_i, y_j) \\log{p(y_j | x_i)}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    = \\sum_{i} \\sum_{j} p(x_i, y_j) \\log{\\frac{p(x_i)}{p(x_i, y_j)}}\n",
    "    $$\n",
    "\n",
    "Equivalently:\n",
    "$$\n",
    "MI(X, Y) = E_{X,Y} \\left[ \\log{\\frac{P(X, Y)}{P(X)P(Y)}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd94993",
   "metadata": {},
   "source": [
    "Score of $X_k$ based on Mutual Information with $Y$:\n",
    "$$\n",
    "I(k) = MI(X_k, Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d6dbf",
   "metadata": {},
   "source": [
    "### **Feature Irrelevance with KL Divergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786a24e",
   "metadata": {},
   "source": [
    "#### **Key Definitions:**\n",
    "- **$X_k$**: The $ k $-th feature component in input vectors (e.g., $[x_1, x_2, \\dots, x_d]$).\n",
    "- **$Y$**: Binary labels ($Y \\in \\{1, -1\\}$).\n",
    "\n",
    "#### **Irrelevance Condition:**\n",
    "A feature $X_k$ is **irrelevant** for predicting $Y$ if:  \n",
    "$$\n",
    "P(X_k | Y = 1) = P(X_k | Y = -1).\n",
    "$$  \n",
    "This means $X_k$'s distribution does not depend on $Y$, providing no predictive information, as the distribution of $X_k$ does not change with $Y$.\n",
    "\n",
    "#### **Measuring Relevance with KL Divergence:**\n",
    "- The Kullback-Leibler (KL) divergence $D(P || Q)$ measures the \"distance\" between two probability distributions $P$ and $Q$.  \n",
    "Mathematically, it is defined as:\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_{x \\in X} P(x) \\log{\\frac{P(x)}{Q(x)}}\n",
    "$$\n",
    "- To quantify relevance, use the **symmetric KL divergence**:\n",
    "$$\n",
    "d(k) = D(P(X_k | Y{=}1) \\| P(X_k | Y{=}{-1})) + D(P(X_k | Y{=}{-1}) \\| P(X_k | Y{=}1))\n",
    "$$\n",
    "- **$ d(k) = 0 $**: $X_k$ is irrelevant (distributions are identical).  \n",
    "- **$ d(k) > 0 $**: $X_k$ is relevant (distributions differ).\n",
    "\n",
    "#### **Density Context:**\n",
    "- \"Density\" refers to the conditional probability distributions:\n",
    "  - $ P(X_k | Y = 1) $ (density under class 1).\n",
    "  - $ P(X_k | Y = -1) $ (density under class -1).\n",
    "\n",
    "#### **Summary:**\n",
    "- **Relevance**: $ d(k) > 0 $ implies $X_k$ is useful for prediction.  \n",
    "- **Irrelevance**: $ d(k) = 0 $ implies $X_k$ can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589f367",
   "metadata": {},
   "source": [
    "## Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e0cd6",
   "metadata": {},
   "source": [
    "**Redundant Subsets:**\n",
    "- Completely same attributes cannot recognized via univariate filter\n",
    "- We cannot say specific relation between redundancy and correlation\n",
    "\n",
    "**Complementary Attributes:**\n",
    "- What if two low correlated attributes actually complete each other and sth like that\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/complementary_attributes.png\" alt=\"complementary attributes example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79652fa",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433e500",
   "metadata": {},
   "source": [
    "Search in the space of all possible combinations of features.\n",
    "- all feature subsets: For $d$ features, $2^d$ possible subsets.\n",
    "- high computational and statistical complexity\n",
    "\n",
    "**Multivariate Feature Selection:**\n",
    "- **Wrapper** Method:\n",
    "    - Uses a classifier to evaluate the score of features or feature subsets.\n",
    "    - Training $2^d$ classifiers is infeasible for large $d$.\n",
    "    - Most wrapper algorithms use a heuristic search.\n",
    "\n",
    "- **Filter** Method:\n",
    "    - Use an evaluation function\n",
    "    - Cheaper to compute than the performance of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e8f3c",
   "metadata": {},
   "source": [
    "## General Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d966c0d",
   "metadata": {},
   "source": [
    "**Input:**\n",
    "- OriginalFeatureSet\n",
    "- EvaluationFunction\n",
    "- StoppingCriterion\n",
    "\n",
    "**Output:**\n",
    "- BestFeatureSubset\n",
    "\n",
    "> bestSubset $\\leftarrow$ OriginalFeatureSet  \n",
    "> bestScore $\\leftarrow$ $-\\infty$  \n",
    "> repeat  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;candidateSubset $\\leftarrow$ GenerateNextSubset(OriginalFeatureSet)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;score $\\leftarrow$ EvaluateSubset(candidateSubset, EvaluationFunction)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;if score > bestScore then  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bestScore $\\leftarrow$ score  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bestSubset $\\leftarrow$ candidateSubset  \n",
    "> until StoppingCriterionMet(bestSubset, bestScore, StoppingCriterion)  \n",
    "> Validate(bestSubset)  \n",
    "> return bestSubset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5607a41",
   "metadata": {},
   "source": [
    "Where:\n",
    "- **Subset Generation:**  \n",
    "    select a candidate feature subset for evaluation\n",
    "    \n",
    "- **Subset Evaluation:**  \n",
    "    compute the score (relevancy value) of the subset\n",
    "    \n",
    "- **Stopping criterion:**  \n",
    "    when stopping the search in the space of feature subsets\n",
    "    \n",
    "- **Validation:**  \n",
    "    verify that the selected subset is valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646eb67",
   "metadata": {},
   "source": [
    "**Stopping Criteria**\n",
    "\n",
    "- Predefined **number of features** is selected\n",
    "- Predefined **number of iterations** is reached\n",
    "- Addition (or deletion) of any feature does **not result in a better subset**\n",
    "- An **optimal subset** (according to the evaluation criterion) is obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19e417",
   "metadata": {},
   "source": [
    "## Filters vs. Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75944e0",
   "metadata": {},
   "source": [
    "### **Filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64b7a0",
   "metadata": {},
   "source": [
    "**Evaluation criteria:**\n",
    "- **Euclidean distance:**\n",
    "    - **Class separability:** Features supporting instances of the same class to be closer in terms of distance than those from different classes\n",
    "\n",
    "- **Information Gain:**\n",
    "    - Formula:\n",
    "        $$\\text{Gain} (S, A) = H_S(Y) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H_{S_v}(Y)$$\n",
    "\n",
    "    - Select $S_{IG}$:\n",
    "        $$S_{IG} = \\argmax_{i} \\text{Gain}(S_i, A)$$\n",
    "\n",
    "- **Dependency (correlation coefficient):**\n",
    "    - Good feature subsets contain features highly correlated with the class, yet uncorrelated with each other\n",
    "\n",
    "- **Consistency (min-features bias):**\n",
    "    - Selects features that guarantee no inconsistency in data\n",
    "        - *inconsistent instances have the same feature vector but different class labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9b9f3",
   "metadata": {},
   "source": [
    "**Key Properties**\n",
    "- **Fast execution:**  \n",
    "    Evaluation function computation is faster than a classifier training\n",
    "\n",
    "- **Generality:**  \n",
    "    Evaluate intrinsic properties of the data, rather than their interactions with a particular classifier (“good” for a larger family of classifiers)\n",
    "\n",
    "- **Tendency to select large subsets:**  \n",
    "    Their objective functions are generally monotonic (so tending to select the full feature set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cf6be",
   "metadata": {},
   "source": [
    "### **Wrappers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cf9f4",
   "metadata": {},
   "source": [
    "**How It Works?**  \n",
    "For each feature subset, train classifier on training data and assess its performance using evaluation techniques like cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21647912",
   "metadata": {},
   "source": [
    "**Key Properties**\n",
    "\n",
    "- **Slow execution:**  \n",
    "    Must train a classifier for each feature subset (or several trainings if cross-validation is used)\n",
    "\n",
    "- **Lack of generality:**  \n",
    "    The solution lacks generality since it is tied to the bias of the classifier used in the evaluation function.\n",
    "\n",
    "- **Ability to generalize:**  \n",
    "    Since they typically use cross-validation measures to evaluate classification accuracy, they have a mechanism to avoid overfitting.\n",
    "\n",
    "- **Accuracy:**  \n",
    "    Generally achieve better recognition rates than filters since they find a proper feature set for the intended classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6019f3",
   "metadata": {},
   "source": [
    "## Subset Selection or Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da5aa3",
   "metadata": {},
   "source": [
    "### **Search Direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946c4eb",
   "metadata": {},
   "source": [
    "- **Forward**\n",
    "- **Backward**\n",
    "- **Random**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e9a7c",
   "metadata": {},
   "source": [
    "### **Search Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e5b6c",
   "metadata": {},
   "source": [
    "#### **Exhaustive - Complete**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728aaa24",
   "metadata": {},
   "source": [
    "Examine all combinations of feature subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d6946",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "- Optimal subset is achievable\n",
    "- Too expensive if $d$ is large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9fb93",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Branch & Bound\n",
    "- Best first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66abda10",
   "metadata": {},
   "source": [
    "#### **Heuristic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775f369",
   "metadata": {},
   "source": [
    "Selection is directed under certain guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726582e",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "- Incremental generation of subsets\n",
    "- Smaller search space and thus faster search\n",
    "- May miss out feature sets of high importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182fa54e",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Sequential forward selection\n",
    "- Sequential backward elimination\n",
    "- Plus-l Minus-r Selection\n",
    "- Bidirectional Search\n",
    "- Sequential floating Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd389f8",
   "metadata": {},
   "source": [
    "#### **Non-deterministic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebbfdc",
   "metadata": {},
   "source": [
    "No predefined way to select feature candidate (i.e., probabilistic approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a296",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "- Optimal subset depends on the number of trials\n",
    "- Need more user-defined parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e64a0",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Simulated annealing\n",
    "- Genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5b2c3",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edc5f9",
   "metadata": {},
   "source": [
    "**Feature Selection Categorization:**\n",
    "- **Univariate method:**  \n",
    "    Considers one variable (feature) at a time.\n",
    "    \n",
    "- **Multivariate method:**  \n",
    "    Considers subsets of features together.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f8334",
   "metadata": {},
   "source": [
    "**Another Feature Selection Categorization:**\n",
    "- **Filter method:**  \n",
    "    Ranks features or feature subsets independent of the classifier as a preprocessing step.\n",
    "\n",
    "- **Wrapper method:**  \n",
    "    Uses a classifier to evaluate the score of features or feature subsets.\n",
    "\n",
    "- **Embedded method:**  \n",
    "    Feature selection is done during the training of a classifier  \n",
    "    *E.g., Adding a regularization term $||w||_1$ or $||w||_2$ in the cost function of linear classifiers*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
