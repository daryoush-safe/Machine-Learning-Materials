{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac94554e",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f60c4",
   "metadata": {},
   "source": [
    "**Avoiding Overfitting**\n",
    "\n",
    "To prevent overfitting, several techniques are employed:\n",
    "- Structural Risk Minimization\n",
    "- Regularization\n",
    "- Cross-Validation\n",
    "    - Model Selection\n",
    "- **Feature Selection**\n",
    "- **Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedbc67",
   "metadata": {},
   "source": [
    "**Feature Selection vs. Feature Extraction**\n",
    "\n",
    "- Feature **Selection:**\n",
    "    - Select a **subset** from a original set\n",
    "    - Filter methods, wrapper methods, embedded methods\n",
    "\n",
    "- Feature **Extraction:**\n",
    "    - Applies a **transformation** to project features into a lower-dimensional space\n",
    "    - PCA (Principal Component Analysis), LDA (Linear Discriminant Analysis), etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7c945",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34778549",
   "metadata": {},
   "source": [
    "**Challenges in Real-World Data**\n",
    "\n",
    "Datasets frequently contain:\n",
    "- **Irrelevant features:**  \n",
    "    Variables that provide no predictive signal (e.g., noise).\n",
    "\n",
    "- **Redundant features:**  \n",
    "    Highly correlated or duplicate measurements.\n",
    "\n",
    "- **High-dimensionality:**  \n",
    "    Number of features $d$ is very large (perhaps $d \\gg n$), common in:\n",
    "    - Text classification (e.g., bag-of-words representations).\n",
    "    - Genomics (e.g., gene expression micro-arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e911d0",
   "metadata": {},
   "source": [
    "**Benefits of Feature Selection**\n",
    "\n",
    "**Feature Selection** is way to find **more accurate**, **faster**, and **easier to understand** classifiers.\n",
    "\n",
    "**Effects:**\n",
    "- **Improve Model Performance**\n",
    "    - Reduces overfitting by eliminating noise\n",
    "    - Mitigates the *curse of dimensionality*\n",
    "    - Fewer features improve the sample-to-parameter ratio, enhancing generalization\n",
    "\n",
    "- **Efficiency:**\n",
    "    - Lowers memory usage, training time, and inference cost\n",
    "    \n",
    "- **Interpretability:**\n",
    "    - Simplifies model explanations\n",
    "    - Critical in domains like healthcare or finance\n",
    "\n",
    "- **Noise Reduction:**\n",
    "    - Directly improves test accuracy by removing non-informative features\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/noise_feature.png\" alt=\"Noise Feature Example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c891b47",
   "metadata": {},
   "source": [
    "**Supervised Feature Selection:**\n",
    "\n",
    "Given:\n",
    "- A feature matrix $X \\in R^{N \\times d}$ *(where $N$ is number of samples, $d$ is number of features)*\n",
    "- A label vector $Y \\in R^N$.\n",
    "\n",
    "Feature selection outputs a subset of indices $\\{i_1, i_2, \\ldots, i_{d'}\\}$ (with $d' \\ll d$) representing the most discriminative features.\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} \n",
    "x_1^{(1)} & \\cdots & x_d^{(1)} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "x_1^{(N)} & \\cdots & x_d^{(N)} \n",
    "\\end{bmatrix}, \n",
    "\\quad \n",
    "Y = \\begin{bmatrix} \n",
    "y^{(1)} \\\\ \n",
    "\\vdots \\\\ \n",
    "y^{(N)} \n",
    "\\end{bmatrix} \\quad \\xrightarrow{\\text{Feature Selection}} \\quad \\{i_1, i_2, \\ldots, i_{d'}\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573bb6c",
   "metadata": {},
   "source": [
    "# Filter: Univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d5c3d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3ffdb",
   "metadata": {},
   "source": [
    "**Univariate Method:**  \n",
    "Evaluates each feature independently by considering its individual relationship with the target variable.\n",
    "\n",
    "**Filter Method:**  \n",
    "Ranks features or subsets *without* involving a classifier (preprocessing step).\n",
    "\n",
    "**Advantage:**  \n",
    "Computationally efficient and statistically scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1e5e1",
   "metadata": {},
   "source": [
    "**Univariate Filter Method:**\n",
    "- Score the each feature $k$ based on the $k$-th column of the data matrix and the label vector\n",
    "    - Score: relevance of the feature to predict labels: Can the feature discriminate the patterns of different classes?\n",
    "\n",
    "- Rank features according to their score values and select the ones with the highest scores.\n",
    "    - Use *cross-validation* to select among the possible values of the $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee18add",
   "metadata": {},
   "source": [
    "**General Procedure**\n",
    "\n",
    "- **Scoring:**  \n",
    "    For each feature $X_k$, compute a score quantifying its relevance to predict labels $Y$ (e.g., correlation, mutual information).  \n",
    "    *Key question:* How well does $X_k$ discriminate between classes?\n",
    "\n",
    "- **Ranking:**  \n",
    "    Sort features by their scores and select the top-$k$ highest-scoring features.  \n",
    "    *Optimization:* Use cross-validation to determine the optimal $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc8c51",
   "metadata": {},
   "source": [
    "## Scoring Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82763070",
   "metadata": {},
   "source": [
    "### **Pearson Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5b294",
   "metadata": {},
   "source": [
    "**Formula:**\n",
    "$$\n",
    "R(k) = \\frac{\\text{cov}(X_k, Y)}{\\sqrt{\\text{var}(X_k)} \\sqrt{\\text{var}(Y)}} \\approx \\frac{\\sum_{i=1}^N (x_k^{(i)} - \\bar{x_k})(y^{(i)} - \\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_k^{(i)} - \\bar{x_k})^2} \\sqrt{\\sum_{i=1}^N (y^{(i)} - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "**Goal:**  \n",
    "Find maximum $R(k)$ between $k$-th attribute to select features with strong linear dependence on $Y$\n",
    "\n",
    "**Limitation:**  \n",
    "Fails to capture nonlinear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa96e7",
   "metadata": {},
   "source": [
    "### **Mutual Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b773d8",
   "metadata": {},
   "source": [
    "**Recall: Mutual Information**\n",
    "$$\n",
    "I(k) = MI(X, Y) = H(Y) - H(Y | X) = \\sum_{i} \\sum_{j} p(x_i, y_j) \\left(\\log{\\frac{p(x_i, y_j)}{p(x_i)p(y_j)}} \\right)\n",
    "$$\n",
    "Where:\n",
    "- Marginal Entropy:\n",
    "    $$\n",
    "    H(Y) = -\\sum_{j} p(y_j) \\log{p(y_j)}\n",
    "    $$\n",
    "\n",
    "- Conditional Entropy:\n",
    "    $$\n",
    "    H(Y | X) = \\sum_{i} \\sum_{j} p(x_i, y_j) \\log{\\frac{p(x_i)}{p(x_i, y_j)}}\n",
    "    $$\n",
    "\n",
    "Equivalently:\n",
    "$$\n",
    "MI(X, Y) = E_{X,Y} \\left[ \\log{\\frac{P(X, Y)}{P(X)P(Y)}} \\right]\n",
    "$$\n",
    "\n",
    "**Advantage:** Captures both linear and nonlinear dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d6dbf",
   "metadata": {},
   "source": [
    "### **KL Divergence for Irrelevance Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786a24e",
   "metadata": {},
   "source": [
    "#### **Key Definitions:**\n",
    "- **$X_k$**: The $ k $-th feature component in input vectors (e.g., $[x_1, x_2, \\dots, x_d]$).\n",
    "- **$Y$**: Binary labels ($Y \\in \\{1, -1\\}$).\n",
    "\n",
    "#### **Irrelevance Condition:**\n",
    "A feature $X_k$ is **irrelevant** for predicting $Y$ if:  \n",
    "$$\n",
    "P(X_k | Y = 1) = P(X_k | Y = -1).\n",
    "$$  \n",
    "This means $X_k$'s distribution does not depend on $Y$, providing no predictive information, as the distribution of $X_k$ does not change with $Y$.\n",
    "\n",
    "#### **Measuring Relevance with KL Divergence:**\n",
    "- The Kullback-Leibler (KL) divergence $D(P || Q)$ measures the \"distance\" between two probability distributions $P$ and $Q$.  \n",
    "Mathematically, it is defined as:\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_{x \\in X} P(x) \\log{\\frac{P(x)}{Q(x)}}\n",
    "$$\n",
    "- To quantify relevance, use the **symmetric KL divergence**:\n",
    "$$\n",
    "d(k) = D(P(X_k | Y{=}1) \\| P(X_k | Y{=}{-1})) + D(P(X_k | Y{=}{-1}) \\| P(X_k | Y{=}1))\n",
    "$$\n",
    "- **$ d(k) = 0 $**: $X_k$ is irrelevant (distributions are identical).  \n",
    "- **$ d(k) > 0 $**: $X_k$ is relevant (distributions differ).\n",
    "\n",
    "#### **Density Context:**\n",
    "- \"Density\" refers to the conditional probability distributions:\n",
    "  - $ P(X_k | Y = 1) $ (density under class 1).\n",
    "  - $ P(X_k | Y = -1) $ (density under class -1).\n",
    "\n",
    "#### **Summary:**\n",
    "- **Relevance**: $ d(k) > 0 $ implies $X_k$ is useful for prediction.  \n",
    "- **Irrelevance**: $ d(k) = 0 $ implies $X_k$ can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589f367",
   "metadata": {},
   "source": [
    "## Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e0cd6",
   "metadata": {},
   "source": [
    "**Redundant Features:**\n",
    "- **Problem:**\n",
    "    - Fails to detect redundant features (variables providing identical/similar information).\n",
    "    - Example: If two features $X_1$ and $X_2$ are perfectly correlated, univariate filters will rank both highly despite redundancy\n",
    "- **Key Challenge:**\n",
    "    - Correlation alone cannot distinguish between relevance and redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fca760",
   "metadata": {},
   "source": [
    "**Complementary Features:**\n",
    "- **Problem:**\n",
    "    - Ignores *feature interactions*: Pairs of weakly correlated features may jointly improve prediction.\n",
    "\n",
    "- **Key Challenge:**\n",
    "    - Univariate methods cannot evaluate synergistic effects between features.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/complementary_attributes.png\" alt=\"complementary attributes example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79652fa",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433e500",
   "metadata": {},
   "source": [
    "Search in the space of all possible combinations of features.\n",
    "\n",
    "**Core Challenges: Feature Subset Search**\n",
    "- **Search Space:** For $d$ features, $2^d$ possible subsets.\n",
    "\n",
    "- **Complexity:**\n",
    "    - *Computational:* Evaluating all subsets is infeasible for large $d$\n",
    "    - *Statistical:* Risk of overfitting when testing many combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ca2ef",
   "metadata": {},
   "source": [
    "**Multivariate Feature Selection:**\n",
    "- **Wrapper Method:**\n",
    "    - Uses a classifier to evaluate the score of features or feature subsets.\n",
    "    - Accurate for specific models\n",
    "    - Training $2^d$ classifiers is infeasible for large $d$.\n",
    "    - Most wrapper algorithms use a heuristic search.\n",
    "\n",
    "- **Filter Method:**\n",
    "    - Evaluates subsets via statistical measures\n",
    "    - Cheaper to compute than the performance of the classifier\n",
    "    - May ignore model-specific interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e8f3c",
   "metadata": {},
   "source": [
    "## General Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d966c0d",
   "metadata": {},
   "source": [
    "**Input:**\n",
    "- OriginalFeatureSet\n",
    "- EvaluationFunction\n",
    "- StoppingCriterion\n",
    "\n",
    "**Output:**\n",
    "- BestFeatureSubset\n",
    "\n",
    "> bestSubset $\\leftarrow$ OriginalFeatureSet  \n",
    "> bestScore $\\leftarrow$ $-\\infty$  \n",
    "> repeat  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;candidateSubset $\\leftarrow$ GenerateNextSubset(OriginalFeatureSet)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;score $\\leftarrow$ EvaluateSubset(candidateSubset, EvaluationFunction)  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;if score > bestScore then  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bestScore $\\leftarrow$ score  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bestSubset $\\leftarrow$ candidateSubset  \n",
    "> until StoppingCriterionMet(bestSubset, bestScore, StoppingCriterion)  \n",
    "> Validate(bestSubset)  \n",
    "> return bestSubset  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5607a41",
   "metadata": {},
   "source": [
    "Where:\n",
    "- **Subset Generation:**  \n",
    "    - Select a candidate feature subset for evaluation\n",
    "    - *(We will discuss about this later)*\n",
    "    \n",
    "- **Subset Evaluation:**  \n",
    "    - **Wrapper:** Train a model on the subset; use its performance (e.g., accuracy) as the score.\n",
    "    - **Filter:** Compute relevance scores (e.g., multivariate mutual information).\n",
    "    \n",
    "- **Stopping criterion:**  \n",
    "    - When stopping the search in the space of feature subsets\n",
    "    \n",
    "- **Validation:**  \n",
    "    - Verify selected subset on holdout data or via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646eb67",
   "metadata": {},
   "source": [
    "**Stopping Criteria**\n",
    "\n",
    "- Predefined **number of features** is selected\n",
    "- Predefined **number of iterations** is reached\n",
    "- Addition (or deletion) of any feature does **not result in a better subset**\n",
    "- An **optimal subset** (according to the evaluation criterion) is obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19e417",
   "metadata": {},
   "source": [
    "## Filters vs. Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75944e0",
   "metadata": {},
   "source": [
    "### **Filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64b7a0",
   "metadata": {},
   "source": [
    "**Evaluation criteria:**\n",
    "- **Euclidean distance:**\n",
    "    - **Class separability:** Maximize intra-class similarity, inter-class dissimilarity.\n",
    "\n",
    "- **Information Gain:**\n",
    "    - Select features maximizing entropy reduction: $S_{IG}$\n",
    "        $$S_{IG} = \\argmax_{i} \\text{Gain}(S_i, A)$$\n",
    "\n",
    "    - Where:\n",
    "        $$\\text{Gain} (S, A) = H_S(Y) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H_{S_v}(Y)$$\n",
    "\n",
    "- **Dependency (correlation coefficient):**\n",
    "    - Good feature subsets contain features highly correlated with the class, yet uncorrelated with each other\n",
    "\n",
    "- **Consistency (min-features bias):**\n",
    "    - Selects features that guarantee no inconsistency in data\n",
    "        - *inconsistent instances have the same feature vector but different class labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9b9f3",
   "metadata": {},
   "source": [
    "**Key Properties**\n",
    "- **Fast execution:**  \n",
    "    Computationally efficient (no classifier training).\n",
    "\n",
    "- **Generality:**  \n",
    "    Evaluate intrinsic properties of the data, rather than their interactions with a particular classifier (“good” for a larger family of classifiers)\n",
    "\n",
    "- **Tendency to select large subsets:**  \n",
    "    Their objective functions are generally monotonic (so tending to select the full feature set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cf6be",
   "metadata": {},
   "source": [
    "### **Wrappers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cf9f4",
   "metadata": {},
   "source": [
    "**How It Works?**  \n",
    "For each feature subset, train classifier on training data and assess its performance using evaluation techniques like cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21647912",
   "metadata": {},
   "source": [
    "**Key Properties**\n",
    "\n",
    "- **Slow execution:**  \n",
    "    Must train a classifier for each feature subset (or several trainings if cross-validation is used)\n",
    "\n",
    "- **Lack of generality:**  \n",
    "    The solution lacks generality since it is tied to the bias of the classifier used in the evaluation function.\n",
    "\n",
    "- **Ability to generalize:**  \n",
    "    Since they typically use cross-validation measures to evaluate classification accuracy, they have a mechanism to avoid overfitting.\n",
    "\n",
    "- **Accuracy:**  \n",
    "    Generally achieve better recognition rates than filters since they find a proper feature set for the intended classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6019f3",
   "metadata": {},
   "source": [
    "## Subset Selection or Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da5aa3",
   "metadata": {},
   "source": [
    "### **Search Direction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5946c4eb",
   "metadata": {},
   "source": [
    "- **Forward**\n",
    "- **Backward**\n",
    "- **Random**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e9a7c",
   "metadata": {},
   "source": [
    "### **Search Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e5b6c",
   "metadata": {},
   "source": [
    "#### **Exhaustive - Complete**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728aaa24",
   "metadata": {},
   "source": [
    "**Approach:**  \n",
    "Evaluate all possible feature subsets (total combinations: $2^d$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d6946",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "- Optimal subset is achievable\n",
    "- Too expensive if $d$ is large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9fb93",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Branch & Bound\n",
    "- Best first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66abda10",
   "metadata": {},
   "source": [
    "#### **Heuristic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775f369",
   "metadata": {},
   "source": [
    "**Approach:**  \n",
    "Guided incremental selection/elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726582e",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "- Incremental generation of subsets\n",
    "- Smaller search space and thus faster search\n",
    "- May miss optimal subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182fa54e",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Sequential forward selection\n",
    "- Sequential backward elimination\n",
    "- Plus-l Minus-r Selection\n",
    "- Bidirectional Search\n",
    "- Sequential floating Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd389f8",
   "metadata": {},
   "source": [
    "#### **Non-deterministic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebbfdc",
   "metadata": {},
   "source": [
    "**Approach:**  \n",
    "No predefined way to select feature candidate (i.e., probabilistic approach)  \n",
    "Avoids local optima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a296",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "- Optimal subset depends on the number of trials\n",
    "- Need more user-defined parameters\n",
    "- Handles more complex interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e64a0",
   "metadata": {},
   "source": [
    "**Methods:**\n",
    "- Simulated annealing\n",
    "- Genetic algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5b2c3",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edc5f9",
   "metadata": {},
   "source": [
    "**Feature Selection Categorization:**\n",
    "- **Univariate method:**  \n",
    "    Considers one variable (feature) at a time.\n",
    "    \n",
    "- **Multivariate method:**  \n",
    "    Considers subsets of features together.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f8334",
   "metadata": {},
   "source": [
    "**Another Feature Selection Categorization:**\n",
    "- **Filter method:**  \n",
    "    Ranks features or feature subsets independent of the classifier as a preprocessing step.\n",
    "\n",
    "- **Wrapper method:**  \n",
    "    Uses a classifier to evaluate the score of features or feature subsets.\n",
    "\n",
    "- **Embedded method:**  \n",
    "    Feature selection is done during the training of a classifier  \n",
    "    *E.g., Adding a regularization term $||w||_1$ or $||w||_2$ in the cost function of linear classifiers*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
