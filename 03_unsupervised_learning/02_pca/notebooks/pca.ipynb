{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e310c3b",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b2c00",
   "metadata": {},
   "source": [
    "**Feature Selection vs. Feature Extraction**\n",
    "\n",
    "- Feature **Selection:**\n",
    "    - Select a **subset** from a original set\n",
    "    - Filter methods, wrapper methods, embedded methods\n",
    "\n",
    "- Feature **Extraction:**\n",
    "    - Applies a **transformation** to project features into a lower-dimensional space\n",
    "    - PCA (Principal Component Analysis), LDA (Linear Discriminant Analysis), etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451383d4",
   "metadata": {},
   "source": [
    "**Feature Extraction**\n",
    "\n",
    "**Definition:**  \n",
    "    Transforms data from a high-dimensional space $R^d$ to a lower-dimensional space ($R^{d'}$, where $d' \\lt d$) while preserving maximal information.\n",
    "\n",
    "**Feature Extraction algorithms:**\n",
    "- **Unsupervised:**\n",
    "    - **Goal:**\n",
    "        - Minimize the information loss (reconstruction error)\n",
    "    - **Methods:**\n",
    "        - **PCA** (Principal Component Analysis)\n",
    "        - **ICA** (Independent Component Analysis)\n",
    "        - **SVD** (SingularValue Decomposition)\n",
    "        - MDS (Multi Dimensional Scaling)\n",
    "        - CCA (Canonical Correlation Analysis)\n",
    "\n",
    "- **Supervised:**\n",
    "    - **Goal:**\n",
    "        - Maximize class separability in the projected space.\n",
    "    - **Methods:**\n",
    "        - **LDA**   (Linear DiscriminantAnalysis)\n",
    "        *Also known as Fisher’s Discriminant Analysis (FDA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18673d4d",
   "metadata": {},
   "source": [
    "**Key Benefits:**\n",
    "- **Visualization:**  \n",
    "    Projection of high-dimensional data onto lower-dimensional space\n",
    "\n",
    "- **Data Compression:**  \n",
    "    Efficient storage, communication, or and retrieval.\n",
    "\n",
    "- **Helps Avoid Overfitting:**\n",
    "    - Eliminates redundant/noisy features\n",
    "    - Improves model generalization by reducing features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39644331",
   "metadata": {},
   "source": [
    "**Linear Transformation:**\n",
    "\n",
    "Projects original data $x \\in R^d$ to $x' \\in R^{d'}$ via:\n",
    "$$x' = A^Tx$$\n",
    "\n",
    "```math\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix} \n",
    "x_1' \\\\ \\vdots \\\\ x_d' \n",
    "\\end{bmatrix} \n",
    "&= \n",
    "\\begin{bmatrix} \n",
    "a_{11} & \\cdots & a_{d1} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "a_{1d'} & \\cdots & a_{d'd} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "x_1 \\\\ \\vdots \\\\ x_d \n",
    "\\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $A^T \\in R^{d' \\times d}$ Projection Matrix\n",
    "\n",
    "- $a_j = \\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{d1} \\end{bmatrix}$\n",
    "\n",
    "- $x \\in R^d$: Original features vector\n",
    "\n",
    "- $x' \\in R^{d'}$: Reduced features vector ($d' \\ll d$)\n",
    "\n",
    "Each new dimension in the transformed input matrix is constructed via a linear transformation of the original data:\n",
    "$$x_j' = a_j' x, \\quad \\forall j = 1, \\cdots d'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d1ce9",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93f2ef",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739ea37",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**  \n",
    "*(Also known as Karhonen-Loeve (KL) transform)*\n",
    "\n",
    "**Goal:**  \n",
    "    Reduce the dimensionality of the data while preserving as much of its variation as possible.\n",
    "\n",
    "**Equivalently:**  \n",
    "    Find the orthogonal projection that minimizes the squared reconstruction error of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148a0d9",
   "metadata": {},
   "source": [
    "**Assumption:**  \n",
    "- The data is **mean-centered**:\n",
    "    $$\\mu_x = \\frac{1}{N} \\sum_{i=1}^N X_i = 0_{d \\times 1}$$\n",
    "\n",
    "**Core Idea:**  \n",
    "PCA projects the data onto a **lower-dimensional linear subspace** such that:\n",
    "- **Interpretation 1:**\n",
    "    - The variance of the projected data is maximized.\n",
    "- **Interpretation 2:**\n",
    "    - The sum of squared distances from the data points to the subspace is minimized.\n",
    "\n",
    "These two interpretations are **equivalent** because:\n",
    "- Maximizing the variance of projections (red) $\\iff$ Minimizing reconstruction error (blue).\n",
    "\n",
    "    <div style=\"text-align:center\">\n",
    "    <img src=\"../assets/pythagoras.png\" alt=\"Pythagoras Example\">\n",
    "    </div>\n",
    "    \n",
    "    By the **Pythagoras theorem**:\n",
    "    $$||\\text{red}||^2 + ||\\text{blue}||^2 = ||\\text{green}||^2$$\n",
    "\n",
    "    Since the total distance (green) is fixed (due to mean-centering), maximizing the projection variance (red) necessarily minimizes the reconstruction error (blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e857f18",
   "metadata": {},
   "source": [
    "**Principal Components (PCs):**\n",
    "\n",
    "A set of **orthonormal** vectors ordered by the fraction of total information (variance) they capture:\n",
    "- The first PC maximizes the variance of the projected data.\n",
    "- Subsequent PCs (orthogonal to prior PCs) capture the next highest variance.\n",
    "\n",
    "**Mathematically:**  \n",
    "PCA performs an orthogonal projection that **maximizes the variance** of the projected data.  \n",
    "The **PCs** are the **eigenvectors** of the data’s covariance matrix, **ranked by their corresponding eigenvalues**  \n",
    "*(which indicate explained variance).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953706d",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe156269",
   "metadata": {},
   "source": [
    "**PCA: Steps**\n",
    "\n",
    "**Input:**\n",
    "- $X \\in R^{N \\times d}$ (data matrix with N data points and d dimensions)\n",
    "\n",
    "**Output:**\n",
    "- $X' \\in R^{N \\times k}$ (transformed data with reduced dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63ccc1",
   "metadata": {},
   "source": [
    "> Compute the mean of each feature: $\\mu = \\frac{1}{N} \\sum_{i=1}^N x^{(i)}$  \n",
    "> Subtract the mean from each data point (center the data): $\\tilde{X} \\leftarrow X - \\mu$  \n",
    "> Compute the covariance matrix: $C = \\frac{1}{N} \\tilde{X}^{T} \\tilde{X}$  \n",
    "> Compute the eigenvalues $[\\lambda_1, \\lambda_2, \\cdots,\\lambda_d]$ and eigenvectors $[v_1, v_2, \\cdots, v_d]$ of $C$  \n",
    "> Select the top $d'$ eigenvectors corresponding to the largest eigenvalues: $A \\leftarrow [v_1, v_2, \\cdots, v_k]$  \n",
    "> Transform the data into the new subspace: $X' \\leftarrow \\tilde{X}A$  \n",
    "\n",
    "Where:\n",
    "- $v_i$ is the $i$-th PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f03d66",
   "metadata": {},
   "source": [
    "**Two Key Interpretations of PCA**\n",
    "\n",
    "- **Maximum Variance Subspace:**\n",
    "    - PCA identifies orthogonal vectors $a$ that maximize the variance of projected data:\n",
    "    $$\\max_{a} \\frac{1}{N} \\sum_{n=1}^N (a^T x^{(n)})^2$$\n",
    "\n",
    "- **Minimum Reconstruction Error:**\n",
    "    - PCA finds vectors $a$ that minimize the mean squared error (MSE) when reconstructing the original data from its projections:\n",
    "    $$\\max_{a} \\frac{1}{N} \\sum_{n=1}^N ||x^{(n)} - (a^Tx^{(n)})a||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e32ca0",
   "metadata": {},
   "source": [
    "## Maximum Variance Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a304b",
   "metadata": {},
   "source": [
    "### **The Principal Component as Eigenvector**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3dac36",
   "metadata": {},
   "source": [
    "**Goal:**  \n",
    "We demonstrate that the first principal component (PC) corresponds to the eigenvector of the covariance matrix associated with its largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd6238",
   "metadata": {},
   "source": [
    "**Key Definitions**\n",
    "\n",
    "**Mean vector:**\n",
    "$$\n",
    "\\mu_x = \n",
    "\\begin{bmatrix}\n",
    "\\mu_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mu_d\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "E(x_1) \\\\\n",
    "\\vdots \\\\\n",
    "E(x_d)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Covariance matrix:**\n",
    "$$\n",
    "\\Sigma = E[(x - \\mu_x)(x - \\mu_x)^T]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e77bfc",
   "metadata": {},
   "source": [
    "**Sample Estimates:**\n",
    "\n",
    "Given data points $\\{x^{(i)}\\}_{i=1}^N$:\n",
    "\n",
    "1. Sample mean:\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N} \\sum_{i=1}^N x^{(i)}\n",
    "$$\n",
    "\n",
    "2. Mean-centered data matrix:\n",
    "$$\n",
    "\\tilde{X} = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{x}^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "\\tilde{x}^{(N)}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)} - \\hat{\\mu} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(N)} - \\hat{\\mu}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Sample covariance matrix:\n",
    "$$\n",
    "\\hat{\\Sigma} = \\frac{1}{N} \\sum_{i=1}^N (x^{(i)} - \\hat{\\mu})(x^{(i)} - \\hat{\\mu})^T = \\frac{1}{N} \\tilde{X}^T \\tilde{X}\n",
    "$$\n",
    "\n",
    "*Note:* All subsequent analysis assumes mean-centered data ($x \\equiv \\tilde{x}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1a2e5",
   "metadata": {},
   "source": [
    "**Projection Geometry**\n",
    "\n",
    "For a unit vector $a$ ($|a| = 1$), the projection of $x$ onto $a$ is:\n",
    "$$\n",
    "\\|x\\| \\cos \\theta = \\|x\\| \\frac{a^T x}{\\|x\\| \\|a\\|} = a^T x\n",
    "$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../assets/pythagoras_max_variance.png\" alt=\"Visual example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9145f69",
   "metadata": {},
   "source": [
    "**Variance Maximization Derivation**\n",
    "\n",
    "The variance of the projected data onto the direction $a$ is:\n",
    "$$\\text{var}(x') = \\text{var}(a^Tx) = \\frac{1}{N} \\sum_{n=1}^N (a^Tx^{(n)})^2$$\n",
    "$$= \\frac{1}{N} ||a^T X||^2 = \\frac{1}{N} (a^T X X^T a) = a^T (\\frac{1}{N} X^T X) a$$\n",
    "\n",
    "Let $R_x = \\frac{1}{N}X^TX$ (sample covariance matrix) and $\\|a\\| = 1$. We solve:\n",
    "\n",
    "$$\\argmax_{a} a^T R_x a$$\n",
    "$$\\text{s.t.} \\quad \\|a\\| = a^T a = 1 \\implies 1 - a^T a = 0$$\n",
    "\n",
    "Lagrangian Formulation:\n",
    "$$L(a, \\lambda) = a^T R_x a + \\lambda (1- a^Ta)$$\n",
    "\n",
    "Taking derivatives:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial a} \\left( a^T R_x a + \\lambda (1 - a^Ta) \\right) = 2 R_x a - 2 \\lambda a = 0\n",
    "$$\n",
    "This simplifies to:\n",
    "$$R_x a = \\lambda a$$\n",
    "\n",
    "**Key Result:**\n",
    "- $a$ is the eigenvector of sample covariance matrix $R_x = \\frac{1}{N} X^T X$ with eigenvalue $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1dd62",
   "metadata": {},
   "source": [
    "### **Optimal Principal Components: Eigenvalue Perspective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70881fd",
   "metadata": {},
   "source": [
    "For transformed data $x' = A^Tx$, the covariance matrix becomes:\n",
    "$$R_{x'} = E[x' {x'}^T] = E[A^T x x^T A] = A^T E[x x^T] A = A^T R_x A$$\n",
    "\n",
    "When $A = [a_1, \\cdots, a_d]$ where $a_1, \\cdots, a_d$ are orthonormal eigenvectors of $R_x$:\n",
    "$$R_x A = \\Lambda A = A \\Lambda \\implies R_x = A \\Lambda A^T$$\n",
    "*where $\\Lambda$ is the diagonal eigenvalue matrix.*\n",
    "\n",
    "Substitute into $R_{x'}$:\n",
    "$$R_{x'} = A^T R_x A = A^T (A \\Lambda A^T) A = \\Lambda$$\n",
    "This yields the critical property:\n",
    "$$E[{x'}_i{x'}_j] = 0, \\quad \\forall i \\neq j \\quad (i, j = 1 \\cdots d)$$\n",
    "\n",
    "**Key Insights:**\n",
    "- The principal component transformation completely decorrelate the features, eliminating all redundancy in the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac3320",
   "metadata": {},
   "source": [
    "## Minimum Reconstruction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14e938",
   "metadata": {},
   "source": [
    "**Mean Squared Error Approximation**\n",
    "\n",
    "Incorporating top $d'$ eigenvectors (corresponding to the largest eigenvalues) in $A = [a_1, \\ldots, a_{d'}]$ ($d' < d$).\n",
    "\n",
    "**Key Property:**\n",
    "- Minimizes the Mean Squared Error (MSE) between original data $x$ and its reconstruction $\\hat{x} = Ax'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020cd3a",
   "metadata": {},
   "source": [
    "**Eigenvalues as Variance Explanations**\n",
    "\n",
    "The $j$-th largest eigenvalue of $R_x$ is the variance on the $j$-th PC:\n",
    "\n",
    "$$\n",
    "\\text{var}({x'}_j) = E[{x'}_j {x'}_j]  = E[x_j'^2] = E[{a_j}^T x x^T a_j] = {a_j}^T E[x x^T] a_j\n",
    "$$\n",
    "\n",
    "Since $R_x a_j = \\lambda_j a_j$:\n",
    "$$\n",
    "\\text{var}({x'}_j) = {a_j}^T R_x a_j = {a_j}^T \\lambda_j a_j = \\lambda_j\n",
    "$$\n",
    "\n",
    "*Note:* Eigenvalues are ordered $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef425c",
   "metadata": {},
   "source": [
    "**Reconstruction Error Derivation**\n",
    "\n",
    "The expected reconstruction error for a $d'$-dimensional projection:\n",
    "$$\n",
    "J(A) = E[\\|x - \\hat{x}\\|^2] = E[\\|x - Ax'\\|^2]\n",
    "$$\n",
    "\n",
    "This can be expanded as:\n",
    "\n",
    "$$\n",
    "= E \\left[ \\left\\| \\sum_{j=d'+1}^{d} x_j'a_j \\right\\|^2 \\right]\n",
    "$$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$$\n",
    "= E \\left[ \\sum_{j=d'+1}^{d} \\sum_{k=d'+1}^{d} x_j'a_j^T a_k x_k' \\right] = E \\left[ \\sum_{j=d'+1}^{d} x_j'^2 \\right]\n",
    "$$\n",
    "\n",
    "Finally, this equals the sum of the remaining eigenvalues:\n",
    "\n",
    "$$\n",
    "= \\sum_{j=d'+1}^{d} E \\left[ x_j'^2 \\right] = \\sum_{j=d'+1}^{d} \\lambda_j\n",
    "$$\n",
    "\n",
    "**Key Result:**  \n",
    "- To minimize reconstruction error $J(A)$:\n",
    "    - Retain PCs with largest eigenvalues (maximum variance)\n",
    "    - Discard PCs with smallest eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5022d3",
   "metadata": {},
   "source": [
    "# PCA on Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3baebc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7813ee36",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d1b8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "635598e2",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2fbd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
